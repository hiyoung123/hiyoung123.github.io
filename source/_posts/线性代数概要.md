---
title: 线性代数概要
toc: true
mathjax: true
tags:
  - 线性代数
categories:
  - 线性代数
excerpt: 机器学习深度学习等领域需要的线性代数知识总结。
abbrlink: 58c9c51f
date: 2020-01-15 15:43:27
---

## Basic Concepts and Notation

线性代数提供了一种紧凑表示和操作线性方程式集合的方法。如下面的方程组：

<center>$
\begin {aligned}
4x_1 - 5x_2 &= -13 \\\ -2x_1 + 3x_2 &= 9
\end {aligned}
$</center></br>

除了传统的解方程组求解之外，还可以使用更简洁的矩阵方法表示：

<center>$Ax = b$</center></br>
其中：

<center>$A = \left[ \begin{matrix}4 &-5 \\ -2 &3 \end{matrix} \right], b = \left[\begin{matrix} -13\\ 9 \end{matrix} \right]. $</center></br>
从上面的例子可以看出矩阵形式更加简洁。下面来认识一下常用的基本符号：

### 基本符号

* 使用 $A \in R^{m \times n}$ 表示一个 m 行 n 列的矩阵，其中矩阵 A 中的每个元素都是实数。

* 使用 $x \in R^n$ 表示包含 n 个元素的向量。传统意义上，一个 n 维向量通常认为是一个 n 行 1 列的矩阵，叫做列向量。如果想表示行向量 - 一个 1 行 n 列的矩阵，那么可以写作 $x^T$，叫做 x 的转置。

  <center>$x = \left[ \begin{matrix}x_1\\x_2\\ \vdots \\ x_n \end{matrix} \right]$</center></br>

* 使用 $x_i$ 表示 x 向量的第 i 个元素。

* 使用 $a_{ij}$（或者 $A_{ij},A_{i,j}$）表示矩阵 A 中的第 i 行第 j 列的元素。

  <center>$A = \left[\begin{matrix}a_{11}\ &a_{12}\ &\dots \ &a_{1n} \\ a_{21} \ &a_{22} \ &\dots \ &a_{2n} \\ \vdots \ &\vdots \ &\ddots \ &\vdots \\ a_{m1} \ &a_{m2} \ &\dots &a_{mn} \end{matrix}\right]$</center></br>

需要注意的是，这些符号并不是一成不变的，具体使用还是要在具体的应用当中去抉择。



## Matrix Multiplication

两个矩阵相乘，结果还是一个矩阵。例如矩阵 $A \in R^{m \times n}$ 和矩阵 $B \in R^{n \times p}$相乘：

<center>$C = AB \in R^{m\times p}$</center></br>
其中：

<center>$C_{ij} = \sum^n_{k=1} A_{ik}B_{ik}$</center></br>
需要注意的是两个矩阵相乘的前提条件：前一个矩阵 A 的列数必须等于后一个矩阵 B 的行数。

下面从几个特殊的例子来研究一下矩阵乘法。

### 向量与向量

两个向量 $x,y \in R^n$ 的乘积 $x^Ty$ 也叫作内积、点积或者点乘。符号表示为：

<center>$x^Ty\in R = \left[ \begin{matrix} x_1\ x_2\ \dots \ x_n \end{matrix}\right] \left[ \begin{matrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{matrix}\right] = \sum^n_{i=1}x_i y_i$</center></br>
内积是矩阵乘法的特例，并且它总是符合交换律：$x^Ty = y^Tx$。

另外一种两个向量相乘的例子，向量 $x \in R^m$ 和向量 $y \in R^n$，维度大小不是必须一致的。这个两个向量的乘积 $xy^T\in R^{m\times n}$ 称为外积或者叫做叉乘。相乘后得到的矩阵每个元素表示为：$(xy^T)_{ij} = x_i y_j$：

<center>$xy^T \in R^{m \times n} = \left[\begin{matrix}x_1 \\ x_2 \\ \vdots \\ x_m \end{matrix}\right] \left[\begin{matrix}y_1 \ y_2 \ \dots \ y_n \end{matrix} \right] = \left[\begin{matrix}x_1y_1  &x_1y_2 &\dots  &x_1y_n \\ x_2y_1 &x_2y_2 &\dots &x_2y_n \\ \vdots &\vdots &\ddots &\vdots \\ x_my_1 &x_my_2 &\dots &x_my_n \end{matrix} \right]$</center></br>
外积一个有用的例子是，定义一个 n 维且每个元素都是 1 的单位向量 $I\in R^n$ ，并且定义矩阵 $A\in R^{m \times n}$ 每一列都是相同的向量 $x \in R^m$。这样可以用如下式子来表示 A：

<center>$A = \left[\begin{matrix}\mid &\mid &\mid \\ x &x &x \\ \mid &\mid &\mid \end{matrix} \right] = \left[\begin{matrix}x_1 &x_1 &\dots &x_1 \\ x_2 &x_2 &\dots &x_2 \\ \vdots & \vdots &\ddots &\vdots \\ x_m &x_m &\dots &x_m \end{matrix} \right] = \left[\begin{matrix}x_1 \\ x_2 \\ \vdots \\x_m \end{matrix} \right]\left[\begin{matrix}1 \ 1 \dots \ 1 \end{matrix} \right] = xI^T$</center></br>


### 矩阵与向量

矩阵 $A\in R^{m\times n}$ 和向量 $x\in R^n$ 相乘，它们的乘积是 $y = Ax \in R^m$ 。矩阵与向量相乘有几种不同的形式，下面一个个来说：

首先按行写 A ，然后 Ax 可以表示为：

<center>$y = Ax = \left[\begin{matrix}— &a_1^T &— \\ — &a_2^T &— \\ &\vdots \\ — &a_m^T &—\end{matrix} \right]x = \left[\begin{matrix}a_1^Tx\\a_2^Tx\\ \vdots \\ a_m^Tx \end{matrix} \right]$</center></br>
换句话说，y 的第 i 个元素等于矩阵 A 的第 i 行和 x 的内积 $y_i = a^T_ix$。

如果我们按列去写 A，那么公式可以写成如下形式：

<center>$y = Ax = \left[\begin{matrix}\mid &\mid & &\mid \\ a_1 &a_2 &\dots &a_n \\ \mid &\mid & &\mid \end{matrix} \right]\left[\begin{matrix}x_1 \\ x_2 \\ \vdots \\ x_n \end{matrix} \right] = [a_1]x_1 + [a_2]x_2 + \dots + [a_n]x_n$</center></br>
自己可以举个小例子验证一下，按行按列去计算，两种方式的结果是一样的。从上可以看出，y 是 矩阵 A 的列向量的线性组合，线性组合的系数由 x 决定。

上面讨论的两种乘法都是向量在右边，下面讨论一下向量在左边的情况。

首先是按列写矩阵 A：

<center>$y^T = x^TA = x^T\left[\begin{matrix}\mid &\mid & &\mid \\ a_1 &a_2 &\dots &a_n \\ \mid &\mid & &\mid \end{matrix} \right] = \left[\begin{matrix}x^Ta_1 \ x^Ta_2 \ \dots \ x^Ta_n\end{matrix} \right]$</center></br>
这表明 $y^T$ 的第 i 项等于 x 和矩阵 A 的第 i 列的内积。

下面看一下按行写矩阵 A：

<center>$y^T=x^TA=\left[x_1 \ x_2 \ \dots \ x_n \right]\left[\begin{matrix}— &a_1&— \\ — &a^T_2 &— \\ \ &\vdots & \\ — &a_m^T &—\end{matrix} \right]=x_1[—\ a^T_1\ —]+x_2[— \ a_2^T \ —]+\dots + x_n[— \ a_n^T \ —]$</center></br>
可以看出 $y^T$ 是矩阵 A 行向量的线性组合，线性组合的系数由 x 决定。



### 矩阵与矩阵

有了上面的基础，我们就知道矩阵和矩阵相乘 $C = AB$ 有四种不同的表现形式，当然这些都是等价的。

首先把矩阵乘法看做是向量－向量的集合乘法，那么这样的情况有两种方式，一是矩阵 A 是行向量集合，矩阵 B 是列向量集合；另外一种是矩阵 A 是列向量集合，矩阵 B 是行向量集合。下面是两种表现形式的公式：

<center>$C = AB = \left[\begin{matrix}— &a_1^T &— \\— &a_2^T &— \\ \ &\vdots & \\ — &a_m^T & — \end{matrix} \right]\left[\begin{matrix}\mid &\mid & &\mid \\ b_1 &b_2 &\dots &b_p \\ \mid &\mid & &\mid  \end{matrix} \right] = \left[\begin{matrix}a_1^Tb_1 &a_1^Tb_2 &\dots &a_1^Tb_p \\ a^T_2b_1 &a_2^T b_2 &\dots &a_2^Tb_p \\ \vdots &\vdots &\ddots  &\vdots \\ a_m^Tb_1 &a_m^Tb_2 &\dots &a_m^Tb_p \end{matrix} \right]$</center></br>
从第一种形式可以看出，矩阵 C 的第 i 行第 j 列的元素等于矩阵 A 的第 i 行和矩阵 B 的第 j 列向量的内积。其中 $A\in R^{m\times n},B\in R^{n\times p},a_i\in R^n,b_j\in R^n$，所以内积计算是成立的。

第二种形式的表达式：

<center>$C = AB = \left[\begin{matrix}\mid &\mid & &\mid \\ a_1 &a_2 &\dots &a_n \\ \mid &\mid & &\mid  \end{matrix} \right]\left[\begin{matrix}— &b^T_1 &— \\ — &b_2^T &— \\ \ &\vdots &\\ — &b^T_n &— \end{matrix} \right]=\sum^n_{i=1}a_ib^T_i$</center></br>
很明显，矩阵 C 等于 矩阵 A 的行向量和矩阵 B 的列向量外积之和。

其次也可以把矩阵和矩阵相乘看做成矩阵和向量之间的乘法：

<center>$C = AB = A\left[ \begin{matrix}\mid &\mid & &\mid \\ b_1 &b_2 &\dots &b_p \\ \mid &\mid & &\mid  \end{matrix}\right] = \left[\begin{matrix}\mid &\mid & &\mid \\ Ab_1 &Ab_2 &\dots &Ab_p \\ \mid &\mid & &\mid \end{matrix} \right]$</center></br>
其中 $c_i = Ab_i$。

<center>$C = AB = \left[\begin{matrix}— &a^T_1 &— \\— &a_2^T &— \\ \ &\vdots \ \\ — &a_m^T &—\end{matrix}\right]B = \left[\begin{matrix}— &a^T_1B &— \\— &a_2^TB &— \\ \ &\vdots \ \\ — &a_m^TB &—\end{matrix}\right]$</center></br>
其中 $c^T_i = a^T_iB$。

矩阵乘法的基本性质：

* 结合律：$(AB)C = A(BC)$；
* 分配律：$A(B+C) = AB + AC$；
* 不可交换：$AB \neq BA$

总结一下，其实只要记住矩阵乘法存在的前提就是前一个矩阵的列数必须等于后一个矩阵的行数，最后相乘的结果矩阵行列数分别为前一个矩阵的行数和后一个矩阵的列数，记住这一点就可以解决大多数矩阵乘法。



## Operations and Properties

### 单位矩阵和对角矩阵

### 转置

### 对称矩阵

### 矩阵的迹

### 正则化

### 线性无关与秩

### 逆矩阵

### 正交矩阵

### 矩阵的值域和零空间

### 行列式

### 二次型和半正定矩阵

### 特征值和特征向量

### 对称矩阵的特征值和特征向量

## Matrix Calculus

### 矩阵梯度

### 海森矩阵

### 二次函数和线性函数的梯度和海森矩阵

### 行列式的梯度

### 特征值优化
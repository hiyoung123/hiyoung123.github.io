---
title: 凸优化概述
toc: true
mathjax: true
cover: >-
  https://cdn.jsdelivr.net/gh/hiyoung123/images/feature/undraw_Options_re_9vxh.svg
tags:
  - 凸优化
categories:
  - 凸优化
excerpt: 机器学习深度学习等领域需要的凸优化知识总结。
abbrlink: b0f73140
date: 2021-01-17 23:03:40
---

## Introduction

机器学习中出现了许多情况，我们都想优化某些函数的值。也就是说，给定一个函数 $f:R^n \rightarrow R$，我们想要最小化（或者最大化）$f(x)$。我们现在已经知道一些优化问题的示例：最小二乘，逻辑回归和支持向量机都被看作是优化问题。

事实证明，在一般情况下，找到函数的全局最优值可能是一项非常艰难的任务。但是对于称为凸优化的一类特殊的优化问题，我们可以在许多情况下有效地找到全局解。在这里，”有效地“ 具有实践和理论上的含义：这意味着我们可以在合理的时间内解决许多实际问题。也意味着从理论上讲，我们可以及时解决问题，并且只需要多项式级别的问题大小。

## Convex Sets

如果对于任何 $x,y \in C$ 和 $\theta \in R$ 且 $0\leq \theta \leq 1$，符合下面的式子，那么集合 $C$ 就是凸的：

<center>$\theta x + (1-\theta)y \in C$</center></br>

直观地讲，这意味着如果我们在 $C$ 中采用任意两个元素，并在这两个元素之间绘制一条线段，则该线段上的每个点也都属于 $C$。下图展示了一个凸集和非凸集。

![](https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_conv_001.png)

点 $\theta x + (1-\theta)y$ 称为点 $x,y$ 的凸组合。

### 例子

* 全部的 $R^n$。显然，给定任何 $x,y \in R^n$ ，有 $\theta x + (1-\theta)y \in R^n$。

* 非负象限 $R^n_{+}$。非负象限包含了所有 $R^n$ 中的向量，并且元素都是非负的：$R^n_{+} = \{x: x_i \geq 0,∀_i =1,\cdots,n \}$。为了表明这是一个凸集，只需注意给定任何 $x,y \in R^n_+$ 和 $0\leq \theta \leq 1$，有

  <center>$(\theta x + (1-\theta)y)_i= \theta x_i +(1-\theta)y_i \geq 0 \ ∀_i$</center></br>

* 范式球（what？）：让 $||\cdot||$ 是关于 $R^n$ 的一些范式（如欧几里得范式，$||x||_2 = \sqrt{\sum_{i=1}^n x_i^2}$，集合 ${x: ||x||\leq 1}$ 是一个凸集。为了证明这一点，假设 $x,y \in R^n$ 且 $||x|| \leq 1, ||y|| \leq 1,0\leq \theta \leq 1$，然后有：

  <center>$||\theta x+ (1-\theta)y||\leq ||\theta x|| + ||(1-\theta)y|| = \theta||x|| + (1-\theta )||y|| \leq 1$</center></br>

  在这里使用了三角形不等式和范式的正其次性。

* 仿射子空间和正多面体。给定矩阵 $A \in R^{m \times n}$ 和向量 $b \in R^m$，则仿射子空间是集合 $\{x \in R^n: Ax=b \}$（如果 $b$ 不在 $A$ 里面，那么值是空）。同样的，多面体是（同样可能为空）集合 $\{ x \in R^n: Ax \leq b \}$ ，在这里 $\leq$ 表示分量不等式（即，$Ax$ 的所有条目均小于或等于它们在 $b$ 中的对应元素）。为了证明这一点，首先考虑 $x,y \in R^n$ 使得 $Ax=Ay=b$，然后对于 $0 \leq \theta \leq 1$ 有：

  <center>$A(\theta x + (1-\theta)y) = \theta Ax+ (1-\theta)Ay = \theta b + (1-\theta)b = b$</center></br>

  类似的，对于 $x,y \in R^n$ 且满足 $Ax \leq b, Ay \leq b, 0\leq \theta \leq 1$，有

  <center>$A(\theta x + (1-\theta)y) = \theta Ax + (1-\theta)Ay \leq \theta b + (1 - \theta)b =b$</center></br>

* 凸集的交集。假设 $C_1, C_2,\cdots,C_k$ 是凸集，然后他们的交集

  <center>$\bigcap_{i=1}^k C_i = \{x:x \in C_i, \ ∀_i = 1,\cdots,k \}$</center></br>

  也是个凸集。为了证明这一点，考虑 $x,y \in \bigcap_{i=1}^k C_i$ 和 $0\leq \theta \leq 1$，根据凸集的定义有

  <center>$\theta x + (1-\theta)y \in C_i , ∀_i = 1,\cdots,k$</center></br>

  因此

  <center>$\theta x + (1-\theta) y \in \bigcap _{i=1}^k C_i$</center></br>

  注意的是，通常凸集的并集不会是凸集。

* 正半定矩阵。所有对称正半定矩阵的集合（通常称为正半定锥，表示为 $S^n_{+}$）是一个凸集（通常 $S^n \in R^{n \times n }$ 表示 $n$ 阶对称方阵的集合）。回想一下，当且仅当所有 $x \in R^n,x^TAx \geq 0$ 且 $A = A^T$ 矩阵 $A \in R^{n \times n}$ 才是对称半正定矩阵。现在仅考虑两个对称半正定矩阵 $A,B \in S^n_+$ 和 $0 \leq \theta \leq 1$。对于所有 $x \in R^n$ 有

  <center>$x^T(\theta A +(1-\theta)B)x = \theta x^TAx +(1-\theta)x^T Bx \geq 0$</center></br>

  同样，上述过程适用于所有正定，负定和半负定矩阵，都是凸集。

## Convex Functions

> 凸优化的核心概念是凸函数。

如果 则函数 $f:R^b \rightarrow R$ 是凸的

## Convex Optimization Problems
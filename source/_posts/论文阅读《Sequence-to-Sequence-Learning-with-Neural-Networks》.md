---
title: 论文阅读《Sequence to Sequence Learning with Neural Networks》
toc: true
mathjax: true
tags:
  - NLP
  - 人工智能
  - Seq2seq
  - LSTM
  - 机器翻译
categories:
  - 自然语言处理
excerpt: 论文《Sequence to Sequence Learning with Neural Networks》阅读笔记。
abbrlink: d1bb6beb
date: 2020-01-29 14:57:30
---

## 前言

本篇论文提出了一种对序列结构做最小假设的端到端序列学习方法。使用一个多层的长短时记忆（LSTM）将输入序列映射到一个固定维数的向量上，然后再使用另一个深度 LSTM 从向量上解码目标序列。这种模型结构称为 Seq2seq 的模型。它可以处理任意长度的序列数据，弥补了 DNN 模型只能处理固定维度的输入输出的缺点。Seq2seq 在机器翻译，语音识别等多个领域发挥着重要作用。

## 介绍

DNN 模型功能强大，但是只能处理固定输入输出的数据，这限制了它在序列数据中的功能。为了可以处理任意长度的序列问题（如语音识别，机器翻译等），论文提出了一种端到端的神经网络模型 - Seq2seq。

模型使用两个 LSTM 结构，一个将源序列映射到一个固定维数的向量上，然后使用另一个 LSTM 从该向量中提取输出序列。第二个 LSTM 相当于一个条件语言模型，取决于输入序列。LSTM 可以很好的学习到长序列数据，所以可以满足这种需求。



## 模型

Seq2seq 结构的一般形式如下图：

![](https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_paper_seq2seq_001.png)

RNN 循环神经网络模型是前馈神经网络在序列问题上的一种推广，给入一个输入序列（$x_1,\cdots,x_t$），可以得到对应的输出序列（$y_1,\cdots,y_t$）：

<center>$\begin{align} h_t &= sigm(W^{hx}x_t + W^{hh}h_{t-1}) \\ y_t &=W^{yh}h_t  \end{align}$</center></br>
如果数据是对齐的，那么 RNN 很容易将序列映射到序列，反之如果输入输出不是对齐的，那么就不确定是否可以达到要求。一般的策略是用一个 RNN 将源序列映射到一个向量，然后使用另外一个 RNN 从中提取出目标序列。但是由于 RNN 自身的局限性，很难学习到太长的序列数据，所以使用 LSTM 替换 RNN。

LSTM 的目标是估计条件概率 $P(y_1,\cdots,y_T | x_1,\cdots,x_T)$，其中 $x_1,\cdots,x_T$ 是输入序列，$y_1,\cdots,y_T$ 是对应的输出序列。首先，LSTM 计算输入序列（$x_1,\cdots,x_T$） 的条件概率得到最后一个隐藏状态 v，然后使用一个标准的 LSTM-LM 计算输出的条件概率（$y_1,\cdots,y_T$），初始的隐藏状态为 v ：

<center>$P(y_1,\cdots,y_T|x_1,\cdots,x_T) = \prod^T_{t=1}P(y_t|v,y_1,\cdots,y_{t-1})$</center></br>
在这个公式中，概率分布 $P(y_t|v,y_1,\cdots,y_{t-1})$ 使用 softmax 计算。同时需要注意的是，每个序列结尾需要使用 *<EOS>* 结尾，这么做的目的是以便预测输出序列的结尾。

在该论文中，作者使用的模型与上述的一般形式有三点不同：

* 使用了两种不同的 LSTM 结构，一种用于输入序列，一种用于输出序列。这么做的好处是增加了模型参数数量，但是成本却可以忽略不计。
* 作者发现深层次的 LSTM 比浅层次的 LSTM 更有效，所以在实验中使用了 4 层 LSTM。
* 并且发现了一个惊人的现象，将输入句子的单词顺序颠倒，极大的提高了性能（个人认为这跟双向 LSTM 的原理是一样的）。



## 实验

### Dataset details

作者使用从英语到法语的 WMT'14 数据集，模型训练在一个由 3.48 亿个法语单词和 3.04 亿个英语单词组成的 1200 万个句子的子集上。

在源语言中使用了 16 万个最常用的单词，在目标语言中使用了 8 万个最常用的单词。每一个词汇表以外的单词都被替换成一个特殊的 *UNK* 标记。

### Decoding and Rescoring

模型的目标函数为给定输入句子 S 得到输出的翻译句子 T ，最大化 log 概率：

<center>${1\over |S|}\sum_{(T,S) \in S}log P(T|S)$</center></br>
一但训练完成就可以通过模型找到最佳的翻译：

<center>$\hat{T} = argmaxP(T|S)$</center></br>
并且使用 Beam search 来寻找最佳的翻译句子。

### Reversing the Source Sentences

在实验中发现，将源语句反转（目标语句没有反转）后，LSTM 可以更好的学习。通过这样做，LSTM 的困惑度（perplexity）从 5.8 下降到 4.7，并且 BLEU 分数从 25.9 提高到 30.6。

虽然这个现象没有很好的解释，但作者们认为这是由于数据集引入了许多短期依赖关系造成的。通常，当把一个源句和一个目标句连接起来时，源句中的每个单词都与目标句中的对应单词相差甚远。因此，这个问题有一个很大的 *最小时间延迟*。通过倒装原句中的词，使原语中对应词与目标语中对应词的平均距离不变。

### Training details

实验发现 LSTM 模型很容易训练。作者使用了 4 层深度 LSTMs，每层有 1000 个单元，1000 个多维词嵌入，输入词汇量为 16 万个，输出词汇量为 8 万个。因此深层 LSTM 使用 8000 个实数来表示一个句子。我们发现深层的LSTMs明显优于浅层的LSTMs，每一层增加的LSTMs使perplexity减少了近10%，这可能是因为它们的隐藏状态更大。我们在每个输出中使用了一个朴素的 softmax，超过 80,000 个单词。得到的 LSTM 有 384M 的参数，其中 64M 是纯循环连接（编码器LSTM 为 32M，解码器 LSTM 为 32M）。完整的训练详情如下：

* 初始化 LSTM 的所有参数，使其均匀分布在-0.08和0.08之间。

* 使用了没有动量的随机梯度下降法 SGD，学习速率固定为 0.7。

* Batch size 为 128 个。

* 虽然 LSTM 没有梯度消失的问题，但是可能会发生梯度爆炸，所以使用范数进行约束。对于每个 batch，计算 $s = \left\| g\right\|_2$ ，其中 g 是梯度除以 128 ，如果 $s > 5$ ，设置 $g = {5g\over s}$。

* 不同的句子有不同的长度。大多数句子都很短（比如 20-30 个），但也有一些句子很长（比如> 100 个），所以 128个随机选择的训练句子就会有很多短句子和很少的长句子，这样就浪费了大量的计算量。为了解决这个问题，确保在一个小批处理中的所有句子长度大致相同。

  

## 结论

该论文提出的 Seq2seq 结构，为后续的序列问题处理打下了不错的基础。同时实验中的发现也很有意思，将源语句的单词反转，可以提高不少的性能。
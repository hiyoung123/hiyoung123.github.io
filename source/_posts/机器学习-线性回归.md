---
title: 机器学习 - 线性回归
top: false
toc: true
mathjax: true
tags:
  - 机器学习
  - 线性回归
  - 岭回归
  - Lasso回归
categories:
  - 机器学习
abbrlink: '19883263'
date: 2019-11-28 17:45:45
password:
summary:
excerpt: 在监督学习中（也就是有标签的数据中），标签值为连续值时是回归任务，标志值是离散值时是分类任务。
---

## 简介

今天要说一下机器学习中大多数书籍第一个讲的（有的可能是KNN）模型-线性回归。说起线性回归，首先要介绍一下机器学习中的两个常见的问题：回归任务和分类任务。那什么是回归任务和分类任务呢？简单的来说，在监督学习中（也就是有标签的数据中），标签值为连续值时是回归任务，标志值是离散值时是分类任务。而线性回归模型就是处理回归任务的最基础的模型。

## 形式

在只有一个变量的情况下，线性回归可以用方程：$y = ax+b$表示。而如果有多个变量，也就是n元线性回归的形式如下：

<center> $h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \cdots$ <br><br> </center >
<center> $h_\theta(x) = \sum^n_{i=0}{\theta_ix_i} = {\theta^Tx}$<br><br> </center >
在这里我们将截断$b$用$\theta_0$代替，同时数据集X也需要添加一列1用于与$\theta_0$相乘，表示$+b$。最后写成矩阵的形式就是$\theta$的转置乘以x。其中如果数据集有n个特征，则$\theta$就是$n+1$维的向量并非矩阵，其中包括截断$b$。

## 目的

线性回归的目的就是求解出合适的$\theta$，在一元的情况下拟合出一条直线（多元情况下是平面或者曲面），可以近似的代表各个数据样本的标签值。所以最好的直线要距离各个样本点都很接近，而如何求出这条直线就是本篇文章重点要将的内容。

![图1](https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_lr_compare.webp)

## 最小二乘法

求解线性回归模型的方法叫做最小二乘法，最小二乘法的核心就是保证所有数据偏差的平方和最小。它的具体形式是：

<center> $J(\theta) = {1\over2}\sum^m_{i=1}(h_\theta(x^{(i)}) - y^{(i)})^2$ <br><br> </center >
其中$h_\theta(x^{(i)})$代表每个样本通过我们模型的预测值，$y^{(i)}$代表每个样本标签的真实值，$m$为样本个数。因为模型预测值和真实值间存在误差$e$，可以写作：

<center> $y^{(i)} = {\theta^Tx^{(i)} + \epsilon^{(i)}}$ <br><br> </center >
根据中心极限定理，$e^{(i)}$是独立同分布的(IID)，服从均值为0，方差为某定值$σ$的平方的正太分布。具体推导过程如下：

![图2](https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_lr_zuixiaoercheng.webp)

## 求解最小二乘法

我们要求得就是当$\theta$取某个值时使$J(\theta)$最小，求解最小二乘法的方法一般有两种方法:矩阵式和梯度下降法。

### 矩阵式求解

当我们的数据集含有m个样本，每个样本有n个特征时，数据x可以写成$m\cdot(n+1)$维的矩阵（$+1$是添加一列1，用于与截断$b$相乘），$\theta$则为$n+1$维的列向量（$+1$是截断b），y为m维的列向量代表每m个样本结果的预测值。则矩阵式的推导如下所示：

![图3](https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_lr_zuixiaoercheng_qiujie_juzhen.webp)

因为$X^TX$为方阵，如果$X^TX$是可逆的，则参数$\theta$得解析式可以写成：

<center> $\theta = (X^TX)^{-1}X^Ty$ <br><br> </center >
如果$X$的特征数n不是很大，通常情况下$X^TX$是可以求逆的，但是如果n非常大，$X^TX$不可逆，则用梯度下降法求解参数$\theta$的值。

### 梯度下降法求解（GD）

在一元函数中叫做求导，在多元函数中就叫做求梯度。梯度下降是一个最优化算法，通俗的来讲也就是沿着梯度下降的方向来求出一个函数的极小值。比如一元函数中，加速度减少的方向，总会找到一个点使速度达到最小。通常情况下，数据不可能完全符合我们的要求，所以很难用矩阵去求解，所以机器学习就应该用学习的方法，因此我们采用梯度下降，不断迭代，沿着梯度下降的方向来移动，求出极小值。梯度下降法包括批量梯度下降法和随机梯度下降法（SGD）以及二者的结合mini批量下降法（通常与SGD认为是同一种，常用于深度学习中）。

梯度下降法的一般过程如下：

1. 初始化$\theta$（随机）
2. 求$J(\theta)$对$\theta$的偏导：

![图4](https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_lr_zuixiaoercheng_gd.webp)

3. 更新$\theta$

   <center> $\theta = \theta - \alpha \cdot {\partial J(\theta)\over{\partial \theta}}$ <br><br> </center >



其中$\alpha$为学习率，调节学习率这个超参数也是建模中的一个重要内容。因为$J(\theta)$是凸函数，所以GD求出的最优解是全局最优解。

批量梯度下降法是求出整个数据集的梯度，再去更新$\theta$ ，所以每次迭代都是在求全局最优解。

![图5](https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_lr_gd_1.webp)

而随机梯度下降法是求一个样本的梯度后就去跟新$\theta$，所以每次迭代都是求局部最优解，但是总是朝着全局最优解前进，最后总会到达全局最优解。

![图6](https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_lr_gd_2.webp)

## 其他回归模型

在机器学习中，有时为了防止模型太复杂容易过拟合，通常会在模型上加入正则项，抑制模型复杂度，防止过拟合。在线性回归中有两种常用的正则，一个是$L1$正则，一个是$L2$正则，加入$L1$正则的称为$Lasso$回归，加入$L2$正则的为$Ridge$回归也叫岭回归。

### Lasso回归

<center> $J({\vec\theta}) = {1\over2}\sum^m_{i=1}(h_{\vec\theta}(x^{(i)}) - y^{(i)}) + \lambda\sum^n_{j=1}{|\theta_j|}$ <br><br> </center>
-- -- --

### 岭回归

<center> $J({\vec\theta}) = {1\over2}\sum^m_{i=1}(h_{\vec\theta}(x^{(i)}) - y^{(i)}) + \lambda\sum^n_{j=1}{\theta_j^2}$ <br><br> </center>
-- -- --


## 总结

下图是个人实现代码结果与真实值对比图：

![图7](https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_lr_compare_result.webp)

详细代码可参考[GitHub](https://github.com/hiyoung123/ML)


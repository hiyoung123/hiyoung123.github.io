---
title: 论文阅读《Efficient Estimation of Word Representations in Vector Space》
abbrlink: fcba888f
tags:
---

## 前言

这篇论文发表于 2013 年，作者是 Tomas Mikolov，也就是提出 Word2vec（Google 时期）和 Fasttext（Facebook 时期）的大佬。本篇文章主要讲的就是 Word2vec 框架，也就是从这开始，Mikolov 将大家从语言模型时代带入了词嵌入的时代。



## 摘要

论文提出了两种可以从大规模语料库中学习到词向量表示的模型，可以用于计算词相似度的任务。并且相比以往的模型取得了不错的性能和准确度。在 1.6 亿规模的数据集训练只花了一天的时间。



## 介绍

传统的 NLP 系统和技术都将单词作为基本计算单位，词之间没有相关性，只是认为是词库的索引。这样的选择虽然简单，鲁棒性好和可观察，比如语言模型 N-Gram。虽然可以用到大多数任务中，但是还是具有局限性。

随着计算机算力水平的提高，现在可以实现在大数据集进行复杂模型的计算。最成功的可能就是分布式词表示，广泛用于神经网络语言模型等任务，并且性能表现优于 N-Gram 模型。

### 论文目标

本篇论文的目的是可以从数十亿级别的语料库中训练出高质量的词向量，并且词向量之间具有相似度关系，如意思相近的词挨得比较近，而且每个单词可以具有多个相似度。

该论文还发现了单词的向量表示，不仅可以简单的表示相似性，还可以通过词偏移技术进行代数运算：

<center> $\vec{King} - \vec{Man} + \vec{Woman} = \vec{Queen}$</center> </br></br>
### 先前工作



## 模型结构

已经有许多种类的模型用来表示连续的词向量，比较出名的有 LSA 和 LDA。而本篇论文着力于使用神经网络模型来学习词向量的表示。实验证明，在保持词向量的线性规律方面，神经网络模型比 LSA 等模型有更好的性能。而且 LDA 在大数据量情况下计算会很吃力。

训练模型的复杂度跟如下公式成正比，试图最大化精度并且最小化训练复杂度。

<center>$O = E \times T \times Q$</center> </br></br>

其中 O 代表训练模型复杂度，E 代表迭代（Epochs）次数，T 代表词汇表中单词数量，Q 表示模型本身结构复杂度（根据参数决定）。通常情况下 E = 3 - 50，T 的值也就是单词数会达到上亿。本文所有模型使用随机梯度下降法和反向传播。

### 前向神经网络语言模型（NNLM）

概率前向神经网络语言模型已经在[1]提出了，它包括输入层 Input，投影层 Projection，隐藏层 Hidden 和输出层 Output。输入层中前 N 个词编码为 1-of-V 的向量，V 是词汇表的大小。Input 和 Projection 之间是一个 N$\times$D 的权重矩阵，经过矩阵变换后，在 Projection 中任意时刻只有 N 个输入是激活的。而隐藏层用来计算整个词汇表的概率分布，所以输出层的维度是词汇表的大小 V。如果设隐藏层的节点个数为 H，那么每个训练实例的计算复杂度为：

<center>$Q = N\times D + N\times D\times H + H\times V$</center></br></br>

主要的计算量是在H $\times$ V，但是可以通过层次 Softmax，避免数据归一化或者对词汇表使用二叉树存储等操作减少计算量至 log(v)。这样主要的复杂度就落在了 N$\times$ D$\times$ H的头上。

该论文提出的模型使用的就是层次 Softmax，使用霍夫曼编码树，根据词出现的频率进行建树，实验证明词频对神经网络语言模型有很大影响。使用基于霍夫曼二叉树的层次 Softmax 可以将计算量减少至 log(unigram_perplexity(V))。举个例子来说，如果词汇表有 100 万个单词，那么可以减少两倍的计算量。

### 循环神经网络语言模型（RNNLM）

循环神经网络语言模型已经在[2]中被提出，以克服前向神经网络语言模型的局限性，例如需要指定上下文长度。RNNs 比浅层神经网络更能表达复杂的模式，它没有投影层 Projection，只有输入层 Input 隐藏层 Hidden和输出层 Output。RNN 的不同之处在于可以保存短暂的记忆或者说是上文的信息可以用于下文中。每个训练实例的计算复杂度为：

<center>$Q = H\times H + H\times V$</center></br></br>

同样的，H $\times$ V 可以通过层次 Softmax 进行优化减少计算量至 log(v)，所以主要计算量在于H $\times$ H。

### 并行训练

为了在大数据集上进行实验，作者在一个分布式框架（DistBelief）上实现了论文中的几个模型。这个框架允许并行运行一个模型的多个副本，每个副本的梯度更新同步通过中央服务器来保持所有参数的一致。对于这种并行训练方式，作者采用mini-batch异步梯度以及自适应的学习速率，整个过程称为Adagrad。采用这种框架，使用100多个模型副本，多个机器的多个CPU核。



## 新的对数线性模型

### 继续词袋模型（CBOW）

### 跳词模型（Skip-gram）

## 结果

### 任务描述

### 最好结果

### 比较模型结构

### 大规模并行训练模型

### Microsoft Research Sentence Completion Challenge

## Examples of the Learned Relationships

## 结论


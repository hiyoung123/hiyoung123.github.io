<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="yXNWLn28ULu1e2oNTKIDLTMhXLOEd7DwMtrEmdJ0pgE">
  <meta name="baidu-site-verification" content="m7vWhnTCyq">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('https://hiyoungai.com').hostname,
    root: '/',
    scheme: 'Mist',
    version: '7.6.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="CS224n 深度学习自然语言处理 2019 版 Lecture-2 学习笔记。">
<meta name="keywords" content="NLP,CS224n,词向量,GloVe">
<meta property="og:type" content="article">
<meta property="og:title" content="CS224n-lecture2 Word Vectors and Word Senses">
<meta property="og:url" content="https:&#x2F;&#x2F;hiyoungai.com&#x2F;posts&#x2F;ad770643.html">
<meta property="og:site_name" content="Young Blog">
<meta property="og:description" content="CS224n 深度学习自然语言处理 2019 版 Lecture-2 学习笔记。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https:&#x2F;&#x2F;hiyoungai.com&#x2F;medias&#x2F;loading.gif">
<meta property="og:image" content="https:&#x2F;&#x2F;hiyoungai.com&#x2F;medias&#x2F;loading.gif">
<meta property="og:image" content="https:&#x2F;&#x2F;hiyoungai.com&#x2F;medias&#x2F;loading.gif">
<meta property="og:image" content="https:&#x2F;&#x2F;hiyoungai.com&#x2F;medias&#x2F;loading.gif">
<meta property="og:image" content="https:&#x2F;&#x2F;hiyoungai.com&#x2F;medias&#x2F;loading.gif">
<meta property="og:image" content="https:&#x2F;&#x2F;hiyoungai.com&#x2F;medias&#x2F;loading.gif">
<meta property="og:image" content="https:&#x2F;&#x2F;hiyoungai.com&#x2F;medias&#x2F;loading.gif">
<meta property="og:image" content="https:&#x2F;&#x2F;hiyoungai.com&#x2F;medias&#x2F;loading.gif">
<meta property="og:image" content="https:&#x2F;&#x2F;hiyoungai.com&#x2F;medias&#x2F;loading.gif">
<meta property="og:image" content="https:&#x2F;&#x2F;hiyoungai.com&#x2F;medias&#x2F;loading.gif">
<meta property="og:image" content="https:&#x2F;&#x2F;hiyoungai.com&#x2F;medias&#x2F;loading.gif">
<meta property="og:image" content="https:&#x2F;&#x2F;hiyoungai.com&#x2F;medias&#x2F;loading.gif">
<meta property="og:image" content="https:&#x2F;&#x2F;hiyoungai.com&#x2F;medias&#x2F;loading.gif">
<meta property="og:image" content="https:&#x2F;&#x2F;hiyoungai.com&#x2F;medias&#x2F;loading.gif">
<meta property="og:image" content="https:&#x2F;&#x2F;hiyoungai.com&#x2F;medias&#x2F;loading.gif">
<meta property="og:image" content="https:&#x2F;&#x2F;hiyoungai.com&#x2F;medias&#x2F;loading.gif">
<meta property="og:updated_time" content="2020-01-12T09:45:43.252Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;hiyoungai.com&#x2F;medias&#x2F;loading.gif">

<link rel="canonical" href="https://hiyoungai.com/posts/ad770643.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>CS224n-lecture2 Word Vectors and Word Senses | Young Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Young Blog" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Young Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Hi Young !</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-coding">

    <a href="/coding/" rel="section"><i class="fa fa-fw fa-code"></i>编程</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hiyoungai.com/posts/ad770643.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Hiyoung">
      <meta itemprop="description" content="人生何所求，致富和自由。 -《基督山伯爵》">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Young Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          CS224n-lecture2 Word Vectors and Word Senses
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-01-08 17:45:45" itemprop="dateCreated datePublished" datetime="2020-01-08T17:45:45+08:00">2020-01-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-01-12 17:45:43" itemprop="dateModified" datetime="2020-01-12T17:45:43+08:00">2020-01-12</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" itemprop="url" rel="index">
                    <span itemprop="name">自然语言处理</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <blockquote>
<p>CS224n 深度学习自然语言处理 2019 版 Lecture-2 学习笔记。 </p>
</blockquote>
<p>在这节课程中，首先回顾了上一节的内容：word2vec，以及两个主要模型 Skip-gram 和 CBOW。还有就是一些技术优化，如负采样等。我们知道，word2vec 的主要思想是通过词的上下文窗口去进行训练，是一种预测模型通过中词预测上下文，或者通过上下文预测中心词。那为什么不可以直接通过共现计数来统计共现词呢？这就是本节课的主要内容，一部分讲了基于共现矩阵的模型，最后又提出了一个基于两者的新模型 - GloVe.</p>
<h2 id="Why-not-capture-co-occurrence-counts-directly"><a href="#Why-not-capture-co-occurrence-counts-directly" class="headerlink" title="Why not capture co-occurrence counts directly?"></a>Why not capture co-occurrence counts directly?</h2><h3 id="Co-occurrence-Matrix"><a href="#Co-occurrence-Matrix" class="headerlink" title="Co-occurrence Matrix"></a>Co-occurrence Matrix</h3><p>在 NLP 中构建共现矩阵有两种方法：</p>
<ul>
<li>Windows：与 Word2vec 类似，通过指定一个窗口，每个单词周围 window 内出现的单词都认为是共现的，对应计数增加，可以捕捉到位置（POS）信息和语义（semantic）信息。</li>
<li>Word-document：我们假设在同一篇文章中出现的单词关联性更大。假设单词 $i$ 出现在文章 $j$ 中，则矩阵元素 $X_{i j}$ 计数加一，当处理完所有文档后，就会得到一个 $|V| \times M$ 的矩阵。其中 $|V|$ 为词汇表大小，M 为文档数量。这一构建共现矩阵的方法也是 Latent Semantic Analysis (LSA) 浅语义分析中使用的经典方法。</li>
</ul>
<p>下面我们来举例说明一下 windows 方法，假设我们的数据包含以下几个句子：</p>
<ol>
<li><p>I like deep learning.</p>
</li>
<li><p>I like NLP.</p>
</li>
<li><p>I enjoy flying。</p>
</li>
</ol>
<p>则我们可以得到如下的word-word co-occurrence matrix：</p>
<p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/CDN/img/img_cs224n_19_lec2_window_matrix_001.png" alt=""></p>
<p>在这里设置的 window 为 1，所以只取中心词周围一个词的距离计数。比如单词 “I” 与 “like” 在上面的三句话中在 window 为 1的距离内共同出现了 2 次，所以矩阵中对应位置是 2。统计完所有的词后就得到了一个 co-occurrence matrix。通过共现矩阵的共现计数来衡量两个单词之间的相关性。</p>
<blockquote>
<p>一般情况下 window 取 5 ~ 10。而且从上图可以看出，矩阵是对称的，与左右上下文无关。</p>
</blockquote>
<p>共现矩阵的缺点也很明显，随着词汇量增加，矩阵也在不断地变大，而且变得更稀疏，需要更多的存储空间。后续的分类模型也会由于矩阵的稀疏性而存在稀疏性问题，使得效果不佳。我们需要对这一矩阵进行降维，获得像 word2vec 那样的低维 (25-1000) 的稠密向量。</p>
<h3 id="SVD-奇异值分解"><a href="#SVD-奇异值分解" class="headerlink" title="SVD 奇异值分解"></a>SVD 奇异值分解</h3><p>奇异值分解 SVD (Single Value Decomposition) 就是一种常用的降维模型。通过 SVD 可以将共现矩阵 X 分解成 $UΣV^⊤$ 的形式，其中 $Σ$ 是对角线矩阵，对角线上的值是矩阵的奇异值。$U$ 和 $V$ 是对应行和列的正交基。</p>
<p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/CDN/img/img_cs224n_19_lec2_svd_001.png" alt=""></p>
<p>为了减少维度同时尽量保存有效信息，可保留对角矩阵的最大 k 个值，并将矩阵 $U$,$V$ 的相应的行列保留。这是经典的线性代数算法，对于大型矩阵而言，计算代价比较高。</p>
<p>使用代码演示一下上面例子：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
la <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg
words <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"I"</span><span class="token punctuation">,</span> <span class="token string">"like"</span><span class="token punctuation">,</span> <span class="token string">"enjoy"</span><span class="token punctuation">,</span> <span class="token string">"deep"</span><span class="token punctuation">,</span> <span class="token string">"learning"</span><span class="token punctuation">,</span> <span class="token string">"NLP"</span><span class="token punctuation">,</span> <span class="token string">"flying"</span><span class="token punctuation">,</span> <span class="token string">"."</span><span class="token punctuation">]</span>
X <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
              <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
              <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
              <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
              <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
              <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
              <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
              <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

U<span class="token punctuation">,</span> s<span class="token punctuation">,</span> Vh <span class="token operator">=</span> la<span class="token punctuation">.</span>svd<span class="token punctuation">(</span>X<span class="token punctuation">,</span> full_matrices<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>可以通过下图看一下，降维到 2 个维度的词向量：</p>
<p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/CDN/img/img_cs224n_19_lec2_svd_002.png" alt=""></p>
<h3 id="Hacks-to-X-several-used-in-Rohde-et-al-2005"><a href="#Hacks-to-X-several-used-in-Rohde-et-al-2005" class="headerlink" title="Hacks to X (several used in Rohde et al. 2005)"></a>Hacks to X (several used in Rohde et al. 2005)</h3><p>不管是什么模型，一些高频词对模型的结果影响很大，比如一些虚词（the, he, she）等等。一般有如下几个方法对高频词进行处理：</p>
<ul>
<li>使用 log 进行缩放</li>
<li>$min(X, t), t\approx 100$</li>
<li>直接舍去</li>
<li>在基于window的计数中，提高更加接近的单词的计数</li>
<li>使用 Person 相关系数替代共现计数，如果值为复数则用 0 代替。</li>
</ul>
<p>在论文《An Improved Model of Semantic Similarity Based on Lexical Co-Occurrence Rohde et al. ms., 2005》中的COALS模型，通过改善计数，取得了不错的效果：</p>
<p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/CDN/img/img_cs224n_19_lec2_hacktoX_001.png" alt=""></p>
<p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/CDN/img/img_cs224n_19_lec2_hacktoX_002.png" alt=""></p>
<p>在向量中出现的有趣的句法模式：语义向量基本上是线性组件，虽然有一些摆动，但是基本是存在动词和动词实施者的方向。</p>
<h3 id="计数模型和-Word2vec-对比"><a href="#计数模型和-Word2vec-对比" class="headerlink" title="计数模型和 Word2vec 对比"></a>计数模型和 Word2vec 对比</h3><p>基于计数（LSA，HAL等模型）：使用整个矩阵的全局统计数据来直接估计。</p>
<ul>
<li>优点<ul>
<li>训练快速</li>
<li>统计数据高效利用</li>
</ul>
</li>
<li>缺点<ul>
<li>主要用于捕捉单词相似性</li>
<li>对大量数据给予比例失调的重视</li>
</ul>
</li>
</ul>
<p>直接预测：定义概率分布并试图预测单词</p>
<ul>
<li>优点<ul>
<li>提高其他任务的性能</li>
<li>能捕获除了单词相似性以外的复杂的模式</li>
</ul>
</li>
<li>缺点<ul>
<li>与语料库大小有关的量表</li>
<li>统计数据的低效使用</li>
</ul>
</li>
</ul>
<p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/CDN/img/img_cs224n_19_lec2_compare_001.png" alt=""></p>
<h2 id="GloVe-模型"><a href="#GloVe-模型" class="headerlink" title="GloVe 模型"></a>GloVe 模型</h2><p>将两个流派的想法结合起来，在神经网络中使用计数矩阵 - GloVe (Global Vectors的缩写)。表示可以有效的利用全局的统计信息。那么如何利用 word-word co-occurrence count 并能学习到词语背后的含义呢？</p>
<p>首先我们在上文说到的共现矩阵符号基础上加入几个符号，$X_i = \sum _k X_{i k}$ 代表所有出现在单词 $i$ 的上下文中的单词次数，用$P_{i j} = P{j|i} = X_{i j} / X_i$ 来表示单词 $j$ 出现在单词 $i$ 上下文中的概率。</p>
<p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/CDN/img/img_cs224n_19_lec2_glove_001.png" alt=""> </p>
<blockquote>
<p>重点不是单一的概率大小，重点是他们之间的比值，其中蕴含着meaning component。</p>
</blockquote>
<p>例如我们想区分热力学上两种不同状态ice冰与蒸汽 steam，它们之间的关系可通过与不同的单词 x 的 co-occurrence probability 的比值来描述，例如对于 solid 固态，虽然 $P(solid | icce)$ 与 $P(solid|steam)$ 本身很小，没有什么有效信息。但是他们的比值 $P(solid|ice) \over P(solid|steam)$ 却较大，因为solid更常用来描述 ice​ 的状态而不是 ​steam​ 的状态，所以在 ice 的上下文中出现几率较大。</p>
<p>对于 gas 则恰恰相反，而对于 water 这种描述 ice 与 steam 均可或者 fashion 这种与两者都没什么联系的单词，则比值接近于1。所以相较于单纯的 co-occurrence probability，实际上 co-occurrence probability 的相对比值更有意义。</p>
<p>那么问题来了，如何在词向量空间中以线性 meaning component 的形式捕获共现概率的比值？</p>
<ul>
<li>Log-bilinear model：$w_i \cdot w_j = logP(i|j)$</li>
<li>Vector differences：$w_x \cdot (w_a - w_b) = log{P(x|a)\over P(x|b)}$</li>
</ul>
<p>基于这些直接给出了 GloVe 的损失函数：</p>
<center>$J = \sum_{i,j=1}^V f(X_{ij})(w_i^T \widetilde{w}_j+b_i+\widetilde{b}_j -logX_{ij})^2$</center></br>
GloVe 模型的优点有：

<ul>
<li>训练快速</li>
<li>可扩展到大型语料库</li>
<li>即使使用小的语料库和小的向量，也能获得良好的性能</li>
</ul>
<p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/CDN/img/img_cs224n_19_lec2_glove_002.png" alt=""></p>
<h2 id="How-to-evaluate-word-vectors"><a href="#How-to-evaluate-word-vectors" class="headerlink" title="How to evaluate word vectors?"></a>How to evaluate word vectors?</h2><p>和一般的 NLP 的评估一样，非为内在和外在：</p>
<ul>
<li>内在<ul>
<li>在特定子任务中的表现</li>
<li>计算速度快</li>
<li>有助于理解系统</li>
<li>不清楚是否真的有帮助，除非与真正的任务建立相关性</li>
</ul>
</li>
<li>外在<ul>
<li>对实际任务的评估</li>
<li>计算精确度可能需要很长时间</li>
<li>不清楚子系统是问题所在，是交互问题，还是其他子系统</li>
<li>如果用另一个子系统替换一个子系统可以提高精确度</li>
</ul>
</li>
</ul>
<h3 id="Intrinsic-word-vector-evaluation"><a href="#Intrinsic-word-vector-evaluation" class="headerlink" title="Intrinsic word vector evaluation"></a>Intrinsic word vector evaluation</h3><p>可以通过类比的形式评估词向量，比如 man 和 woman 之间的关系是男女性别的差异，那么 king 和什么词也有这种关系呢？下图表示了这种类比评估的方法：</p>
<p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/CDN/img/img_cs224n_19_lec2_glove_eval.png" alt=""></p>
<p>整体思想在第一节课中已经提到过，并且还有用于测试的测试集合，这里不多说了。</p>
<h3 id="Glove-Visualizations"><a href="#Glove-Visualizations" class="headerlink" title="Glove Visualizations"></a>Glove Visualizations</h3><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/CDN/img/img_cs224n_19_lec2_glove_vs_001.png" alt=""></p>
<p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/CDN/img/img_cs224n_19_lec2_glove_vs_002.png" alt=""></p>
<p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/CDN/img/img_cs224n_19_lec2_glove_vs_003.png" alt=""></p>
<p>从图中可以看出，具有相同类比含义的几组词都是平行的。</p>
<h3 id="Analogy-evaluation-and-hyperparameters"><a href="#Analogy-evaluation-and-hyperparameters" class="headerlink" title="Analogy evaluation and hyperparameters"></a>Analogy evaluation and hyperparameters</h3><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/CDN/img/img_cs224n_19_lec2_glove_compare_001.png" alt=""></p>
<p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/CDN/img/img_cs224n_19_lec2_glove_compare_002.png" alt=""></p>
<p>从上面两个对比数据可以看出：</p>
<ul>
<li>300 是一个很好的词向量维度</li>
<li>不对称上下文（只使用单侧的单词）不是很好，但是这在下游任务重可能不同</li>
<li>window size 设为 8 对 Glove 来说比较好</li>
</ul>
<p>与 Skip-gram + Neg 对比，可以看出训练时间越长效果越好：</p>
<p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/CDN/img/img_css224n_19_lec2_glove_compare_003.png" alt=""></p>
<p>从下面这组对比数据可知，数据集越大越好，并且维基百科数据集比新闻文本数据集要好。这是因为 Wiki 百科是解释性文本语料库，里面包含了文本本身的含义与相关语句。而新闻类的文本只是在胡说八道（@_@ 哈哈教授原话！）。</p>
<p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/CDN/img/img_cs224n_19_lec2_glove_compare_004.png" alt=""></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总的来说，各种词向量模型各有好坏，每篇论文选取的数据肯定都是对自己的模型有利的，所以不能只靠论文中的数据就去一味选取模型，自己使用的时候要根据实际任务以及各个模型的优缺点去选取模型。</p>
<h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><p><a href="https://hiyoungai.com/posts/fb292938.html">论文阅读《GloVe: Global Vectors for Word Representation》</a></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"># NLP</a>
              <a href="/tags/CS224n/" rel="tag"># CS224n</a>
              <a href="/tags/%E8%AF%8D%E5%90%91%E9%87%8F/" rel="tag"># 词向量</a>
              <a href="/tags/GloVe/" rel="tag"># GloVe</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/posts/28911bbb.html" rel="prev" title="论文阅读《Distributed Representations of Words and Phrases and their Compositionality》">
      <i class="fa fa-chevron-left"></i> 论文阅读《Distributed Representations of Words and Phrases and their Compositionality》
    </a></div>
      <div class="post-nav-item">
    <a href="/posts/fb292938.html" rel="next" title="论文阅读《GloVe: Global Vectors for Word Representation》">
      论文阅读《GloVe: Global Vectors for Word Representation》 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
              
    <div class="comments" id="gitalk-container"></div>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Why-not-capture-co-occurrence-counts-directly"><span class="nav-text">Why not capture co-occurrence counts directly?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Co-occurrence-Matrix"><span class="nav-text">Co-occurrence Matrix</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SVD-奇异值分解"><span class="nav-text">SVD 奇异值分解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hacks-to-X-several-used-in-Rohde-et-al-2005"><span class="nav-text">Hacks to X (several used in Rohde et al. 2005)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#计数模型和-Word2vec-对比"><span class="nav-text">计数模型和 Word2vec 对比</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GloVe-模型"><span class="nav-text">GloVe 模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#How-to-evaluate-word-vectors"><span class="nav-text">How to evaluate word vectors?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Intrinsic-word-vector-evaluation"><span class="nav-text">Intrinsic word vector evaluation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Glove-Visualizations"><span class="nav-text">Glove Visualizations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Analogy-evaluation-and-hyperparameters"><span class="nav-text">Analogy evaluation and hyperparameters</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-text">总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#推荐阅读"><span class="nav-text">推荐阅读</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hiyoung"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">Hiyoung</p>
  <div class="site-description" itemprop="description">人生何所求，致富和自由。 -《基督山伯爵》</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">18</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">49</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
    <span class="links-of-author-item">
    <a href="https://github.com/hiyoung123/ " class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fa fa-fw fa-github"></i>GitHub
    </a>
    </span>
    <span class="links-of-author-item">
    <a href="mailto:hiyoungliu@gmail.com " class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fa fa-fw fa-envelope"></i>Email
    </a>
    </span>
    <span class="links-of-author-item">
    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=787420787 " class="tooltipped" target="_blank" data-tooltip="QQ联系我: 787420787 " data-position="top" data-delay="50">
        <i class="fa fa-fw fa-qq"></i>QQ
    </a>
    </span>
    <span class="links-of-author-item">
    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fa fa-fw fa-rss"></i>RSS
    </a>
    </span>


<!--
      <span class="links-of-author-item">
        <a href="https://github.com/hiyoung123/" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hiyoung123&#x2F;" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/hiyoungliu@gmail.com" title="Email → hiyoungliu@gmail.com"><i class="fa fa-fw fa-envelope"></i>Email</a>
      </span>
      <span class="links-of-author-item">
        <a href="/787420787" title="QQ → 787420787"><i class="fa fa-fw fa-qq"></i>QQ</a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fa fa-fw fa-rss"></i>RSS</a>
      </span>
-->
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://godweiyang.com/" title="https:&#x2F;&#x2F;godweiyang.com" rel="noopener" target="_blank">godweiyang</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.dreamwings.cn/" title="https:&#x2F;&#x2F;www.dreamwings.cn" rel="noopener" target="_blank">千千</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://wdxtub.com/" title="https:&#x2F;&#x2F;wdxtub.com" rel="noopener" target="_blank">小土刀</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://codewithzhangyi.com/" title="http:&#x2F;&#x2F;codewithzhangyi.com" rel="noopener" target="_blank">Zhang Yi</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hiyoung</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.6.0
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="fa fa-area-chart"></i>
      </span>
      <span class="site-uv" title="站点总字数">
        <span class="post-count">37.8k</span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/muse.js"></script>
<script src="/js/next-boot.js"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>















  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '2809c671d3e9827bdb20',
      clientSecret: '5f216884763890b0ef3751b9206ab5eade091708',
      repo: 'hiyoung123.github.io',
      owner: 'hiyoung123',
      admin: ['hiyoung123'],
      id: '8315c54449cddb4119f21721af56dd4a',
        language: '',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script>

<script>!function(e){var r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function t(){for(var c=0;c<r.length;c++)t=r[c],void 0,0<=(n=t.getBoundingClientRect()).top&&0<=n.left&&n.top<=(e.innerHeight+240||document.documentElement.clientHeight+240)&&function(){var t,n,e,i,o=r[c];t=o,n=function(){r=r.filter(function(t){return o!==t})},e=new Image,i=t.getAttribute("data-original"),e.onload=function(){t.src=i,n&&n()},e.src=i}();var t,n}t(),e.addEventListener("scroll",function(){!function(t,n){clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(n)},500)}(t,e)})}(this);</script></body>
</html>

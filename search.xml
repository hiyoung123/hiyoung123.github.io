<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>多项式分布</title>
      <link href="posts/7147e84f.html"/>
      <url>posts/7147e84f.html</url>
      
        <content type="html"><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>多项式分布是二项分布的推广，也是一种离散概率分布。二项分布的典型例子是扔硬币，硬币正面朝上概率为 $p$, 重复扔 $n$ 次硬币，$k$ 次为正面的概率即为一个二项分布概率。而多项分布就像扔骰子，有 $6$ 个面对应 $6$ 个不同的点数。二项分布随机事件 $X$ 只有 $2$ 种取值，而多项分布的随机事件 $X$ 有多种取值，多项分布的概率公式为：</p><center>$P(X_1=x_1,X_2=x_2,\cdots,X_k=x_k) = \begin{cases} {n!\over x_1!\cdots x_k!} p_1^{x_1}\cdots p_k^{x_k} &when \sum_{i=1}^k x_i = n  \\ 0 & otherwise.\end{cases}$</center></br><p>  其中，$\sum_{i=1}^k p_i = 1$。</p><h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>二项分布的系数 $C_n^k$ 是在 $n$ 次试验中抽取 $k$ 个出现为正，而多项式的系数则是进行 $k$ 次抽取，每次抽取 $C_{n-x}^x$。其中 $k$ 为抽取次数，也是多项式的项数。而 $x$ 为出现某一项的次数，$n$ 试验总次数。</p><h3 id="多项式定理"><a href="#多项式定理" class="headerlink" title="多项式定理"></a>多项式定理</h3><p>$(a_1+a_2+\cdots+a_k)^n = (a_1+x_2+\cdots+a_k)\cdots(a_1+a_2+\cdots+a_k), 从每个因式中抽取,a_1^{x_1} \cdot a_2^{x_2} \cdots a_k^{x_k}, 抽取 C_n^{x_1}\cdot C_{n-x_1}^{x_2} \cdots = {n! \over x_1!\cdot x_2!\cdots x_k!}$</p><p>多项式分布通式：${n!\over x_1!\cdot x_2!\cdots x_k!}a_1^{x_1}\cdot a_2^{x_2}\cdots a_k^{x_k}$, 其中 $x_1+x_2 + \cdots + x_k = n$，当 $k=2$ 时,就是二项分布。</p><h2 id="期望"><a href="#期望" class="headerlink" title="期望"></a>期望</h2><center>$E[X_i] = np_i$</center></br><h2 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h2><center>$D(X_i) = np_i(1-p_i)$</center></br><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://blog.csdn.net/qq_15111861/article/details/80481168">多项式分布的理解概率公式的理解</a></li><li><a href="https://baijiahao.baidu.com/s?id=1648025208172346848&wfr=spider&for=pc">深度学习必懂的13种概率分布</a></li><li><a href="https://zhuanlan.zhihu.com/p/66732781">机器学习中常见的几种概率分布</a></li><li><a href="https://face2ai.com/Math-Probability-5-9-Multinomial-Distribution/">多项式分布(The Multinomial Distributions)</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 概率论与数理统计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 多项式分布 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>伯努利分布和二项分布</title>
      <link href="posts/9d7e2111.html"/>
      <url>posts/9d7e2111.html</url>
      
        <content type="html"><![CDATA[<h2 id="伯努利分布"><a href="#伯努利分布" class="headerlink" title="伯努利分布"></a>伯努利分布</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>伯努利分布（Bernoulli），也叫两点分布，$0-1$ 分布，是一种离散型概率分布。</p><p>设试验 $E$ 只有两个可能结果：$A$ 与 $\overline{A}$ ，则称 $E$ 为伯努利试验．设 $P(A) = p \ (0&lt;p&lt;1)$，此时 $P(\overline{A}) = 1-p$。将 $E$ 独立重复地进行 $n$ 次，则称这一串重复的独立试验为 $n$ 重伯努利试验（二项分布）。</p><p>若离散随机变量 $X$ 的服从参数为 $p$ 伯努利分布，令 $q=1-p$，则 $X$ 的概率函数为：</p><center>$f(x|p) = \begin{cases} p^xq^{1-x}, \ \  & x = 0,1 \\ 0, \ \ &x \neq 0,1 \end{cases}$</center></br>### 期望<center>$ E(x) = ∑x∗p(x) = 1∗p+0∗(1−p) = p$ </center></br>### 方差<center>$D(x) = E(x^2)−(E^2)(x) = p−p^2 = p(1−p)$</center></br>### 例子<ul><li>抛一次硬币是正面向上吗？</li><li>刚出生的小孩是个女孩吗？</li></ul><h2 id="二项分布"><a href="#二项分布" class="headerlink" title="二项分布"></a>二项分布</h2><h3 id="概念-1"><a href="#概念-1" class="headerlink" title="概念"></a>概念</h3><p>二项分布（Binomial Distribution）也是一种离散型概率分布。二项分布是指进行 $n$ 次伯努利实验（只有出现和不出现两种情况），所期望的结果出现次数的概率，又称为 $n$ 重伯努利分布。</p><p>二项分布是建立在有放回的采样基础上，无放回采样可以使用超几何分布计算概率。当样本总量 $N$ 很大，而实验次数 $n$ 很小时，${n\over N} \leq 0.1$ 时可以用二项分布近似超几何分布。</p><h3 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h3><center>$b(x,n,p) = C_n^xp^x(1-p)^{n-x}$</center></br>其中 $C_n^x = {n \cdot (n-1) \cdot \cdots (n-x+1) \over x \cdot (x-1) \cdots 1}$，$n$ 为实验次数​。从公式上可以看出，伯努利分布是二项分布在 $n=1$ 时的特例。<h3 id="期望"><a href="#期望" class="headerlink" title="期望"></a>期望</h3><center>$E[X] = np$</center></br>### 方差<center>$D(X) = np(1-p)$</center></br>### 例子<ul><li>扔 $n$ 次硬币，出现 $k$ 次正面的概率。</li></ul><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">double</span> <span class="token function">binomial</span><span class="token punctuation">(</span><span class="token keyword">int</span> N<span class="token punctuation">,</span> <span class="token keyword">int</span> k<span class="token punctuation">,</span> <span class="token keyword">double</span> p<span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>N<span class="token operator">==</span><span class="token number">0</span> <span class="token operator">&amp;&amp;</span> k<span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>        <span class="token keyword">return</span> <span class="token number">1.0</span><span class="token punctuation">;</span>    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>N<span class="token operator">&lt;</span><span class="token number">0</span> <span class="token operator">||</span> k<span class="token operator">&lt;</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>        <span class="token keyword">return</span> <span class="token number">0.0</span><span class="token punctuation">;</span>    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token punctuation">(</span><span class="token number">1.0</span><span class="token operator">-</span>p<span class="token punctuation">)</span><span class="token operator">*</span><span class="token function">binomial</span><span class="token punctuation">(</span>N<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> k<span class="token punctuation">,</span> p<span class="token punctuation">)</span> <span class="token operator">+</span> p<span class="token operator">*</span><span class="token function">binomial</span><span class="token punctuation">(</span>N<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> k<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> p<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">//binomial(n,k,p) = (1-p)*binomial(n-1,k,p)+p*bimomial(n-1,k-1,p)</span><span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/24692791">统计基础篇之十：怎么理解二项分布</a></li><li><a href="https://www.zhihu.com/question/316063270">什么是二项分布？</a></li><li><a href="https://zhuanlan.zhihu.com/p/43514413">伯努利分布、二项分布、多类别分布、多项分布</a></li><li><a href="https://www.cnblogs.com/jmilkfan-fanguiju/p/10589773.html">统计与分布之伯努利分布与二项分布</a></li><li><a href="https://www.cnblogs.com/ehomeshasha/p/3820512.html">伯努利分布详解(包含该分布数字特征的详细推导步骤)</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 概率论与数理统计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 伯努利分布 </tag>
            
            <tag> 二项分布 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>遗传算法</title>
      <link href="posts/9cfaa3ca.html"/>
      <url>posts/9cfaa3ca.html</url>
      
        <content type="html"><![CDATA[<h2 id="遗传算法"><a href="#遗传算法" class="headerlink" title="遗传算法"></a>遗传算法</h2><p>遗传算法（$GA$）：是一种基于自然选择和群体遗传机制的搜索算法，它模拟了自然选择和自然遗传过程中的繁殖，杂交和突变的现象．</p><blockquote><p> 能够生存下来的往往不是最强大的物种，也不是最聪明的物种，而是最能适应环境的物种。</p><p> ​                                                                                                                    - 查尔斯·达尔文</p></blockquote><h2 id="求解思想"><a href="#求解思想" class="headerlink" title="求解思想"></a>求解思想</h2><ul><li>利用遗传算法求解问题时，随机产生一些个体（问题的解），</li><li>根据预定的目标函数对每一个个体进行评估，给出一个适应度，选择一些个体用来产生下一代个体．选择操作体现了适者生存的原理，好的个体被用来产生下一代，坏的个体被淘汰．</li><li>然后选择出来的个体经过杂交，突变再组合产生新的个体</li></ul><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_ga_001.png" alt=""></p><h2 id="遗传操作"><a href="#遗传操作" class="headerlink" title="遗传操作"></a>遗传操作</h2><ol><li>选择<ul><li>选择是指从群体中选择优良的个体并淘汰劣质的个体，他建立在适应度评估的基础上．适应度越大的个体，被选中的概率越大，他的子孙在下一代中的数量越多．</li><li>选择的方法有：轮赌盘方法，最佳个体保留法，期望值法，排序选择法，竞争法，线性标准化法．</li></ul></li><li>交叉<ul><li>交叉是指把两个父代个体的部分结构加以替换组合而生成新的个体的操作，交叉的目的是为了产生下一代新的个体．通过交叉操作，遗传算法的搜索能力得到飞跃的提升．</li><li>交叉是遗传算法获取优良个体的重要手段，交叉操作是按照一定的交叉概率在匹配库中随机的选取两个个体进行的，交叉的位置也是随机的，交叉概率一般取得很大 $0.6~0.9$.</li></ul></li><li>变异<ul><li>变异是以很小的概率 $p$ 随机改变种群中个体的某些基因的值</li><li>变异操作的过程是：<ul><li>产生一个$［０，１］$之间的随机数 $rand$．</li><li>如果 $rand &lt; p$ ，则进变异操作．</li></ul></li><li>变异操作本身是一种局部随机搜索，与选择和交叉结合一起使用，能够避免由于选择和交叉导致某些局部信息永久丢失，保证了遗传算法的有效性．保证了群体多样性．</li><li>变异概率不宜取过大，如果 $p &gt; 0.5$ ，那么遗传算法退化为随机搜索．</li></ul></li></ol><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>遗传算法是一种迭代的选择算法，从初始解，以及后续繁殖出来的子解中选择最优的解．通常的做法是，将问题的解转化为二进制表示，然后进行遗传算法选择最优解．</p><ol><li>特征选择：每个特征用二进制中的位表示，$０$ 不选择该特征，$1$ 选择该特征．则有多组特征选择，用遗传算法进行选择最优解.</li><li>背包问题</li><li>路线规划</li></ol><h2 id="python-相关库"><a href="#python-相关库" class="headerlink" title="python 相关库"></a>python 相关库</h2><ul><li>tpot：树形传递优化技术</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://blog.csdn.net/jzp1083462154/article/details/80032987">遗传算法的基本原理</a></li><li><a href="https://www.cnblogs.com/jingsupo/p/genetic-algorithm-python.html">一文读懂遗传算法工作原理（附Python实现）</a></li><li><a href="https://www.jianshu.com/p/ae5157c26af9">超详细的遗传算法(Genetic Algorithm)解析</a></li><li><a href="https://www.zhihu.com/question/19885905">遗传算法有哪些比较直观的应用呢？</a> （列举一些有用的应用和蛋疼的应用，比如用于搜索神经网络参数以及k-mean的ｋ值都是比较蛋疼的．．．）</li><li><a href="https://github.com/anopara/genetic-drawing">Genetic Drawing</a></li><li><a href="https://arxiv.org/abs/1412.1897">攻击神经网络</a></li><li><a href="https://www.zhihu.com/question/20085479">遗传算法有哪些有趣应用？</a></li><li><a href="https://www.zhihu.com/question/23293449?sort=created">如何通俗易懂地解释遗传算法？有什么例子？</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 智能优化算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 遗传算法 </tag>
            
            <tag> 智能优化算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>凸优化概述</title>
      <link href="posts/b0f73140.html"/>
      <url>posts/b0f73140.html</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>机器学习中出现了许多情况，我们都想优化某些函数的值。也就是说，给定一个函数 $f:R^n \rightarrow R$，我们想要最小化（或者最大化）$f(x)$。我们现在已经知道一些优化问题的示例：最小二乘，逻辑回归和支持向量机都被看作是优化问题。</p><p>事实证明，在一般情况下，找到函数的全局最优值可能是一项非常艰难的任务。但是对于称为凸优化的一类特殊的优化问题，我们可以在许多情况下有效地找到全局解。在这里，”有效地“ 具有实践和理论上的含义：这意味着我们可以在合理的时间内解决许多实际问题。也意味着从理论上讲，我们可以及时解决问题，并且只需要多项式级别的问题大小。</p><h2 id="Convex-Sets"><a href="#Convex-Sets" class="headerlink" title="Convex Sets"></a>Convex Sets</h2><p>如果对于任何 $x,y \in C$ 和 $\theta \in R$ 且 $0\leq \theta \leq 1$，符合下面的式子，那么集合 $C$ 就是凸的：</p><center>$\theta x + (1-\theta)y \in C$</center></br>直观地讲，这意味着如果我们在 $C$ 中采用任意两个元素，并在这两个元素之间绘制一条线段，则该线段上的每个点也都属于 $C$。下图展示了一个凸集和非凸集。<p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_conv_001.png" alt=""></p><p>点 $\theta x + (1-\theta)y$ 称为点 $x,y$ 的凸组合。</p><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><ul><li><p>全部的 $R^n$。显然，给定任何 $x,y \in R^n$ ，有 $\theta x + (1-\theta)y \in R^n$。</p></li><li><p>非负象限 $R^n_{+}$。非负象限包含了所有 $R^n$ 中的向量，并且元素都是非负的：$R^n_{+} = {x: x_i \geq 0,∀<em>i =1,\cdots,n }$。为了表明这是一个凸集，只需注意给定任何 $x,y \in R^n</em>+$ 和 $0\leq \theta \leq 1$，有</p><center>$(\theta x + (1-\theta)y)_i= \theta x_i +(1-\theta)y_i \geq 0 \ ∀_i$</center></br></li><li><p>范式球（what？）：让 $||\cdot||$ 是关于 $R^n$ 的一些范式（如欧几里得范式，$||x||<em>2 = \sqrt{\sum</em>{i=1}^n x_i^2}$，集合 ${x: ||x||\leq 1}$ 是一个凸集。为了证明这一点，假设 $x,y \in R^n$ 且 $||x|| \leq 1, ||y|| \leq 1,0\leq \theta \leq 1$，然后有：</p><center>$||\theta x+ (1-\theta)y||\leq ||\theta x|| + ||(1-\theta)y|| = \theta||x|| + (1-\theta )||y|| \leq 1$</center></br>在这里使用了三角形不等式和范式的正其次性。</li><li><p>仿射子空间和正多面体。给定矩阵 $A \in R^{m \times n}$ 和向量 $b \in R^m$，则仿射子空间是集合 ${x \in R^n: Ax=b }$（如果 $b$ 不在 $A$ 里面，那么值是空）。同样的，多面体是（同样可能为空）集合 ${ x \in R^n: Ax \leq b }$ ，在这里 $\leq$ 表示分量不等式（即，$Ax$ 的所有条目均小于或等于它们在 $b$ 中的对应元素）。为了证明这一点，首先考虑 $x,y \in R^n$ 使得 $Ax=Ay=b$，然后对于 $0 \leq \theta \leq 1$ 有：</p><center>$A(\theta x + (1-\theta)y) = \theta Ax+ (1-\theta)Ay = \theta b + (1-\theta)b = b$</center></br>类似的，对于 $x,y \in R^n$ 且满足 $Ax \leq b, Ay \leq b, 0\leq \theta \leq 1$，有</li></ul><center>$A(\theta x + (1-\theta)y) = \theta Ax + (1-\theta)Ay \leq \theta b + (1 - \theta)b =b$</center></br>* 凸集的交集。假设 $C_1, C_2,\cdots,C_k$ 是凸集，然后他们的交集  <center>$\bigcap_{i=1}^k C_i = \{x:x \in C_i, \ ∀_i = 1,\cdots,k \}$</center></br>也是个凸集。为了证明这一点，考虑 $x,y \in \bigcap_{i=1}^k C_i$ 和 $0\leq \theta \leq 1$，根据凸集的定义有<center>$\theta x + (1-\theta)y \in C_i , ∀_i = 1,\cdots,k$</center></br>  因此  <center>$\theta x + (1-\theta) y \in \bigcap _{i=1}^k C_i$</center></br>注意的是，通常凸集的并集不会是凸集。<ul><li><p>正半定矩阵。所有对称正半定矩阵的集合（通常称为正半定锥，表示为 $S^n_{+}$）是一个凸集（通常 $S^n \in R^{n \times n }$ 表示 $n$ 阶对称方阵的集合）。回想一下，当且仅当所有 $x \in R^n,x^TAx \geq 0$ 且 $A = A^T$ 矩阵 $A \in R^{n \times n}$ 才是对称半正定矩阵。现在仅考虑两个对称半正定矩阵 $A,B \in S^n_+$ 和 $0 \leq \theta \leq 1$。对于所有 $x \in R^n$ 有</p><center>$x^T(\theta A +(1-\theta)B)x = \theta x^TAx +(1-\theta)x^T Bx \geq 0$</center></br>同样，上述过程适用于所有正定，负定和半负定矩阵，都是凸集。</li></ul><h2 id="Convex-Functions"><a href="#Convex-Functions" class="headerlink" title="Convex Functions"></a>Convex Functions</h2><blockquote><p>凸优化的核心概念是凸函数。</p></blockquote><p> 如果函数 $f:R^b \rightarrow R$ 的定义域（$D(f)$）是一个凸集，并且对于所有 $x,y\in D(f)$ 且 $\theta \in R,0\leq \theta \leq 1$，有</p><center>$f(\theta x + (1-\theta)y) \leq \theta f(x) + (1-\theta)f(y)$</center></br>那么该函数就是凸的。<p>直观的，直观地，考虑该定义的方式是，如果我们在凸函数的图形上选取任意两个点，然后在它们之间画一条直线，则这两个点之间的函数部分将位于该直线之下。如下图所示：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_conv_002.png" alt=""></p><p>如果在上述定理中 $x\neq y, 0 &lt; \theta &lt; 1$ 则严格不等式成立，那么我们说函数 $f$ 是严格凸的。如果 $-f$ 是凸的，那么 $f$ 则是凹的，如果 $-f$ 是严格凸的，那么 $f$ 则是严格凹的。</p><h3 id="凸的一阶条件"><a href="#凸的一阶条件" class="headerlink" title="凸的一阶条件"></a>凸的一阶条件</h3><p>假设函数 $f:R^n \rightarrow R$ 是可微的（在 $f$ 的定义域内，任何点 $x$ 的梯度 $\nabla_x f(x)$ 都存在）。当且仅当 $D(f)$ 是一个凸集，对于所有 $x,y \in D(f)$ ，有如下式子成立，那么函数 $f$ 是凸的。</p><center>$f(y) \geq f(x) + \nabla_x f(x)^T (y-x)$</center></br>函数 $f(x) + \nabla_x(x)^T(y-x)$ 在点 $x$ 处被称为函数 $f$ 的一阶逼近。直观的讲，可以认为是 $f$ 与 $x$ 点处的切线近似。凸的一阶条件说，当且仅当切线是函数f的整体低估量时，$f$ 才是凸的。换句话说，如果我们采用函数并在任意点绘制切线，则该线上的每个点都将位于 $f$ 上对应点的下方。<p>类似于凸度的定义，如果 $f$ 包含严格不等式，则 $f$ 将为严格凸；如果不等式被逆，则 $f$ 将为凹；如果反向不等式为严格，则 $f$ 将为严格凹。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_conv_003.png" alt=""></p><h3 id="凸的二阶条件"><a href="#凸的二阶条件" class="headerlink" title="凸的二阶条件"></a>凸的二阶条件</h3><p>假设函数 $f: R^n \rightarrow R$ 是二次可微的（在 $f$ 的定义域内，任何点 $x$ 的 $Hessian \ \nabla_x^2f(x)$ 是存在的）。当且仅当 $D(f)$ 是凸集，且 $Hessian$ 是半正定时，函数 $f$ 才是凸的：即对于所有 $x \in D(f)$</p><center>$\nabla_x^2 f(x) \geq 0$</center></br>在一维度上，这等效于二阶导 $f^{''}(x)$ 始终是非负的。<p>再次类似于凸集的定义和一阶条件，如果 $f$ 的 $Hessian$ 为正定，则 $f$ 为严格凸，如果 $Hessian$ 为负半定，则为凹，如果 $Hessian$ 为负，则 $f$ 为严格凹。</p><h3 id="Jensen不等式"><a href="#Jensen不等式" class="headerlink" title="Jensen不等式"></a>Jensen不等式</h3><p>假设我们从凸函数的基本定义中的不等式开始</p><center>$f(\theta x+(1-\theta)y) \leq  \theta f(x) + (1-\theta)f(y) \ , \ for \ \ 0 \leq \theta \leq 1  $</center></br>使用归纳法，可以很容易地扩展到超过一个点的凸组合<center>$f(\sum_{i=1}^k \theta_ix_i) \leq \sum_{i=1}^k \theta_i f(x_i) \ , \ \ for \ \ \sum _{i=1}^k \theta_i = 1, \theta_i \geq 0 \ \ ∀_i $</center></br>实际上，这也可以扩展为无限的和或积分。 在后一种情况下，不等式可以写成<center>$f(\int p(x)xdx) \leq \int p(x)f(x)dx \ , \ \ for \ \ \int p(x)dx = 1, p(x) \leq 0, \  ∀_x$</center></br>因为 $p(x)$ 积分为 $1$，所以通常将其视为概率密度，在这种情况下，可以使用期望改写上一个式子<center>$f(E[x]) \leq E[f(x)]$</center></br>这个式子就是 $Jensen$ 不等式。<h3 id="子集"><a href="#子集" class="headerlink" title="子集"></a>子集</h3><p>凸函数产生一个特别重要的凸集类型，称为 $\alpha$ 子集。给定凸函数 $f: R^n \rightarrow R$ 和实数 $\alpha \in R$ ，则 $\alpha$ 子集为</p><center>$\{x \in D(f) : f(x) \leq \alpha \}$</center></br>换句话说，$\alpha$ 子集是所有满足 $f(x) \leq \alpha$ 的点 $x$ 集合。<p>为了表明这是一个凸集，对于任何 $x,y \in D(f)$ 使 $f(x) \leq \alpha $ 和 $f(y) \leq \alpha $，有</p><center>$f(\theta x+(1-\theta)y) \leq \theta f(x) + (1-\theta) f(y) \leq \theta \alpha +(1-\theta)\alpha = \alpha$ </center></br>### 例子<p>下面从一个变量的凸函数的几个例子开始，然后转到多个变量的凸函数。</p><ul><li><p>指数函数：令 $f:R \rightarrow R, \ f(x) = e^{ax}$，为了证明 $f$ 是凸的，只需要证明二次导数 $f^{‘’}(x) = a^2e^{ax}$ 对所有 $x$ 都是正数即可。</p></li><li><p>负对数函数：令 $f: R \rightarrow R, \ f(x) = -logx $ ，其定义域  $D(f) \in R_{++}$ 表示为严格正数。则 $f^{‘’}(x) = {1\over x^2} &gt; 0$。 </p></li><li><p>仿射函数：令 $f:R^n \rightarrow R, \ f(x) = b^Tx +c$，其中 $b \in R^n, C\in R$。在这里 $Hessian$ 矩阵 $\nabla_x^2 f(x) = 0$。因为零矩阵都是正半定和负半定，所以 $f$ 既是凸的又是凹的。 实际上，这种形式的仿射函数是唯一既凸又凹的函数。 </p></li><li><p>二次函数：令 $f: R^n \rightarrow R, \ f(x) = {1\over 2} x^T Ax + b^Tx +c$ ，其中 $A \in S^n, b \in R^n, c \in R$。 该函数的 $Hessian$ 矩阵为</p><center>$\nabla_x^2 f(x) = A$</center></br>因此，函数 $f$ 的凹凸性由 $A$ 是否是半正定确定。如果 $A$ 为正半定，则函数为凸（类似地，同样适用于严格凸，严格凹）；如果A是不确定的，则 $f$ 既不是凸也不是凹。</li></ul><blockquote><p>欧几里得范式 $f(x) = ||x||^2_2 = x^Tx$，是二次函数的特殊形式，其中 $A= I,b=0,c=0$。因此它是严格的凸函数。</p></blockquote><ul><li><p>范式：令 $f: R^n \rightarrow R$ 是一些范式，通过三角函数和范式的正齐次性，对于所有 $x,y\in R^n, 0 \leq \theta \leq 1$ 有</p><center>$f(\theta x+(1-\theta)y) \leq f(\theta x) + f((1-\theta)y) = \theta f(x) + (1-\theta)f(y)$</center></br>这是一个特殊的示例，因为通常范式都是不可微的。</li><li><p>凸函数的非负加权和：令 $f_1,\cdots,f_k$ 是凸函数，$w_1,\cdots,w_k$ 是非负实数，则 </p><center>$f(x) = \sum_{i=1}^k w_i f_i(x)$</center></br>是凸函数，因为</li></ul><center>$\begin{align} f(\theta x + (1-\theta)y) &= \sum_{i=1}^k w_if_i(\theta x +(1-\theta)y) \\ &\leq \sum_{i=1}^k w_i(\theta f_i(x) + (1-\theta)f_i(y)) \\ &= \theta \sum_{i=1}^k w_i f_i(x) + (1-\theta) \sum_{i=1}^k w_if_i(y) \\ &= \theta f(x) + (1-\theta) f(y)  \end{align}$</center></br>## Convex Optimization Problems<p>有了凸函数和凸集合的定义，我们现在可以考虑凸优化问题。 形式上的，凸优化问题是如下形式的最优化问题</p><center>$\begin {align}minimize \ \ & f(x) \\ subject \ to \ \ & x\in C  \end{align}$</center></br><p>其中 $f$ 是凸函数，$C$ 是凸集，$x$ 是优化变量。更具体一点的写法是</p><center>$\begin{align} minimize \ \ & f(x)  \\ subject \ to \ \ & g_i(x) \leq 0, \ \ i = 1 ,\cdots,m \\ & h_i(x) = 0, \ \ i = 1 , \cdots, p \end{align}$</center></br><p>其中，$f$ 是凸函数，$g(x)$ 是凸函数，$h(x)$ 是仿射函数，$x$ 是优化变量。</p><p>注意的是，这些不等式的方向很重要：$g(x)$ 是必须小于 $0$ 的凸函数，这是因为 $g(x)$ 的 $0$ 子级集是凸集，所以可行区域（是许多凸集的交集）也是凸的（回想一下仿射子空间也是凸集）。如果我们对于某个凸 $g_i$ 要求 $g_i≥0$，则可行区域将不再是凸集，并且将不再保证我们解决这些问题的算法找到全局最优值。还要注意，仅仿射函数被允许为等式约束。 直观地，可以认为这是由于等式约束等于两个不等式 $h_i≤0$ 和 $h_i≥0$。但是，当且仅当 $h_i$ 既是凸面又是凹面，即仿射必定是仿射的，这些都是有效约束。</p><p>最优化问题的最优值表示为 $p^<em>$（有时为 $f^</em>$），它等于可行区域中目标函数的最小可能值。</p><center>$p^* = min\{f(x):g_i(x)\leq 0,i=1,\cdots,m, h_i(x)=0,i=1,\cdots,p \}$</center></br><p>如果问题无解（可行域为空），或者无下界（存在点使 $f(x)\rightarrow -\infty$）时，允许 $p^<em>$ 取 $\infty$ 或者 $-\infty$。如果 $f(x^</em>)=p^<em>$ ，则 $x^</em>$ 是最佳值点。 请注意，即使最佳值为有限，也可以有多个最佳点。</p><h3 id="凸问题的全局最优解"><a href="#凸问题的全局最优解" class="headerlink" title="凸问题的全局最优解"></a>凸问题的全局最优解</h3><p>在陈述凸问题的全局最优结果之前，让我们正式定义局部最优和全局最优的概念。 直观地，如果没有“较低”目标值的“附近”可行点，则可行点称为局部最优。 类似地，如果根本不存在具有较低目标值的可行点，则将可行点称为全局最优。 为了使它更正式一点，我们给出以下两个定义。</p><p>如果一个点 $x$ 是可行的（即满足优化问题的约束），并且存在某个 $R&gt; 0$，使得所有满足 $||x-z||_2    ≤R$ 的可行点 $z$ 都满足 $f（x ）≤f（z)$。</p><p>如果一个点 $x$ 可行，并且对于所有可行点 $z$，有 $f（x）≤f（z)$。</p><p>现在，我们讨论凸优化问题的关键要素，它们将从中得到大部分效用。 关键思想是对于凸优化问题，所有局部最优点都是全局最优的。</p><p>让我们通过矛盾来快速证明此属性。 假设 $x$ 不是全局最优的局部最优点，即存在一个可行点 $y$ 有 $f(x) &gt; f(y)$。根据局部最优的定义，不存在可行点 $z$ 使 $||x-z||_2\leq R$ 和 $f(z) &lt; f(x)$ 。但是现在假设有这么一个点，</p><center>$z = \theta y + (1-\theta)x , \  with \ \ \theta = {R\over 2 ||x -y||_2}$</center></br><p>然后有</p><center>$\begin{align} ||x-z||_2 &= ||x - ({R\over 2||x-y||_2} y + (1- {R\over 2||x-y||_2})x)||_2 \\ &= ||{R\over 2||x-y||_2 }(x-y)||_2 \\ &= {R\over 2} \leq R \end{align}$</center></br><p>另外，通过 $f$ 的凸性，我们有</p><center>$f(z) = f(\theta y + (1-\theta)x) \leq \theta f(y) + (1-\theta)f(x) \leq f(x)$</center></br><p>此外，由于可行集是凸集，并且由于 $x$ 和 $y$ 都是可行的，因此，$z = \theta y + (1-\theta)$ 也是可行的。因此，$z$ 是一个可行点，其中 $||x-z||_2 \leq R , f(z) &lt; f(x)$。这与我们的假设相矛盾，表明 $x$ 不可能是局部最优的。 </p><h3 id="凸问题的特例"><a href="#凸问题的特例" class="headerlink" title="凸问题的特例"></a>凸问题的特例</h3><p>由于各种原因，考虑一般凸规划公式的特殊情况通常很方便。 对于这些特殊情况，我们通常可以设计出非常有效的算法来解决非常大的问题，因此，当人们使用凸优化技术时，您可能会看到这些特殊情况。</p><ul><li><p>线性规划：如果目标函数 $f$ 和不等式 $g_i$ 都是仿射函数，并且有如下形式，我们就说这是线性规划问题（LP）</p><center>$\begin{align} minimize \ \  & c^Tx + d \\ subject \  to \ \ & Gx \leq h  \\ & Ax  = b\end{align}$</center></br><p>其中，$x \in R^n$ 是优化变量，$c\in R^n，d\in R, G \in R^{m\times n}, h\in R^m , A\in R^{p\times n}, b \in R^p$． </p></li><li><p>二次规划：如果不等式 $g_i$ 是仿射的，但是目标函数是二次凸函数，那么该凸优化问题称为二次规划问题（OP）</p><center>$\begin{align} minimize \ \ & {1\over 2} x^TPx + c^T x +d \\ subject \ to \ \ & Gx \leq h \\ & Ax = b  \end{align}$</center></br></li><li><p>二次约束二次规划</p></li><li><p>半定规</p></li></ul><h3 id="例子-1"><a href="#例子-1" class="headerlink" title="例子"></a>例子</h3><ul><li><p>SVM</p></li><li><p>约束最小二乘</p></li><li><p>最大似然估计的逻辑回归</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 凸优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 凸优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>概率论基本概念</title>
      <link href="posts/69aaf0dc.html"/>
      <url>posts/69aaf0dc.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>概率论是不确定性的研究</p></blockquote><h2 id="Elements-of-probability"><a href="#Elements-of-probability" class="headerlink" title="Elements of probability"></a>Elements of probability</h2><p>为了定义一个集合的概率，我们需要一些基本元素：</p><ul><li><p>样本空间 $\Omega$：随机试验的所有结果的集合。在这里，每个结果 $\omega \in \Omega$ 都可以看作是实验结束时，真实世界状态的完整描述。</p></li><li><p>事件集（事件空间）$F$：其元素 $A\in F$（称为事件）是 $\Omega$ 的子集（即 $A \subseteq \Omega$ 是实验可能结果的集合）。</p></li><li><p>概率测度：满足以下性质的函数 $P: F \rightarrow R$：</p><ul><li><p>$P(A) \geq 0 \ for \ all \ A\in F$</p></li><li><p>$P(\Omega) = 1$</p></li><li><p>如果 $A_1,A_2,\cdots$ 是不相交事件，则有</p><center>$P(\cup_i A_i) = \sum_i P(A_i)$</center></br></li></ul></li></ul><p>这三个属性称为概率公理。</p><h3 id="其他概念"><a href="#其他概念" class="headerlink" title="其他概念"></a>其他概念</h3><ol><li>样本空间：随机试验 $E$ 所有可能产生的结果的集合 $S$ 叫做样本空间，其中每个结果称为样本点．</li><li>随机事件：试验 $E$ 的样本空间 $S$ 的子集为 $E$ 的随机事件．每次试验中，当且仅当这一子集中的一个样本点出现时称这一事件发生．</li><li>必然事件：每次试验一定发生的事件．</li><li>不可能事件：每次试验一定不发生的事件，空集不包括任何样本点，每次试验一定不发生空集，所以是不可能事件．</li><li>概率：当试验次数 $n$ 趋近于无穷大时，频率逐渐稳定，可以用来表征某个事件发生可能性的大小．但是实际中无法进行多次的试验，所以使用概率进行表征事件发生的可能性，记为 $P(A)$．</li></ol><h3 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h3><ul><li><p>如果 $A \subseteq B =&gt; P(A) \leq P(B)$</p></li><li><p>$P(A \cap B)\leq min(P(A), P(B))$</p></li><li><p>$P(A \cup B) \leq P(A) + P(B)$</p></li><li><p>非负性：$P(A) \geq 0$ </p></li><li><p>规范性：对于必然事件，$P(A) = 1$</p></li><li><p>有限可加性：当 $A$ 和 $B$ 是互不相容的两个事件，则 $P(A \cup B) = P(A) + P(B)$</p></li><li><p>$P(\null) = 0$</p></li><li><p>逆事件的概率：$P(!A) = 1 - P(A)$</p></li><li><p>对任意的两个事件 $A$ 和 $B$，有 $P(A \cup B) = P(A) + P(B) - P(AB)$</p></li><li><p>全概率公式：如果 $A_1,\cdots,A_k$ 是一系列不相交事件，如 $\bigcup_{i=1}^k A_i = \Omega$，那么：</p><center>$\sum_{i=1}^k P(A_i) = 1$</center></br></li></ul><h3 id="条件概率和独立性"><a href="#条件概率和独立性" class="headerlink" title="条件概率和独立性"></a>条件概率和独立性</h3><p>令 $B$ 是非零的概率的事件，在给定条件 $B$ 的条件下 $A$ 的概率为：</p><center>$P(A|B) = {P(A \cap B) \over P(B)}$</center></br>换句话说，$P(A|B)$ 是观察事件 $B$ 发生后事件 $A$ 发生的概率。当且仅当 $P(A \cap B) = P(A)P(B)$ （或 $P(A|B) = P(A)$）时，两个事件才称为相互独立，因此，独立性等同于说观察 $B$ 对 $A$ 的概率没有任何影响。<h3 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h3><p>略</p><h3 id="古典概型"><a href="#古典概型" class="headerlink" title="古典概型"></a>古典概型</h3><p>详细内容：<a href="https://hiyoungai.com/posts/d50294d6.html">古典概型</a></p><h2 id="Random-variables"><a href="#Random-variables" class="headerlink" title="Random variables"></a>Random variables</h2><ol><li><p>随机变量：</p></li><li><p>离散型随机变量：随机变量的值是有限个或者是可列无限多个．</p><center>$P(X=k) = P(\{ \omega:X(\omega)=k\})$</center></br></li><li><p>连续型随机变量：随机变量的值是无限个的. </p><center>$P(a\leq X \leq b) = P(\{\omega: a \leq X(\omega) \leq b\})$</center></br></li></ol><h3 id="累积分布函数"><a href="#累积分布函数" class="headerlink" title="累积分布函数"></a>累积分布函数</h3><p>累积分布函数 $F(x)$ 是对离散型和非离散型随机变量取值的概率，描述了随机变量统计规律性．已知 $X$ 的分布函数，就知道 X 落在任意区间 $(x_1, x_2]$ 的概率，简写为（$Cumulative \  distribution \ functions，CDF$），又叫做分布函数．</p><blockquote><p>可以认为是给定一个随机变量值，得到了从 $0$ 到该值之间的概率累加．</p></blockquote><p>累积分布函数（$CDF$）是一个 $F_X : R \rightarrow [0,1]$ 特殊的概率度量函数：</p><center>$F_X(x) = P(X\leq x)$</center></br><p>通过使用此函数，可以计算出F中任何事件的概率。下面是 $CDF$ 的图：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_prob_cdf.png" alt=""></p><h4 id="性质-1"><a href="#性质-1" class="headerlink" title="性质"></a>性质</h4><ul><li>$0 \leq F_X(x) \leq 1$</li><li>$\lim_{x\rightarrow -\infty} F_X(x) = 0$</li><li>$\lim_{x \rightarrow \infty}F_X(x) = 1$</li><li>$x\leq y =&gt; F_X(x) \leq F_X(y)$</li><li>$F(x+0)= F(x)$</li><li>$F(X)$ 是一个不减函数．</li></ul><p>离散型随机变量的累积分布函数是阶跃型的，连续随机变量的累积分布函数是连续型的．</p><h3 id="概率质量函数"><a href="#概率质量函数" class="headerlink" title="概率质量函数"></a>概率质量函数</h3><p>当随机变量 $X$ 具有一组有限的可能值时（即 $X$ 是离散随机变量变量），代表与随机变量相关的概率测度的一种更简单的方法是直接指定随机变量可以假设的每个值的概率。概率质量函数（$Probability \ mass \ functions，PMF$）是一个 $p_X:\Omega \rightarrow R$ 的函数：</p><center>$p_X(x) = P(X=x)$</center></br><blockquote><p>分布函数 $F(x)$ 不等于概率质量函数（$PMF$）, $F(x)= P(x_0)+\cdots + P(x)$ , 即 $F(X) = \sum_{x_k \leq x} p_{x_k}$ 或者 $F(x)=P(X\leq x) $</p></blockquote><h4 id="性质-2"><a href="#性质-2" class="headerlink" title="性质"></a>性质</h4><ul><li>$0 \leq p_X(x) \leq 1$</li><li>$\sum_{x\in X}p_X(x)=1$</li><li>$\sum_{x\in A} p_X(x) = P(X\in A)$</li></ul><h3 id="概率密度函数"><a href="#概率密度函数" class="headerlink" title="概率密度函数"></a>概率密度函数</h3><p>在连续随机变量的情况下，累积分布函数 ($CDF$) 是到处可微的．在这种情况下，我们把 $CDF$ 的导数称之为概率密度函数 ($Probability \ density \ functions，PDF$)．</p><center>$f_X(x) = {dF_X(x)\over d_x}$</center></br><blockquote><p>从分布函数引出连续型随机变量的分布律: $F(x) = \int_{-\infty}^x f(t)dt$, 其中 $f(t)$ 就是概率密度函数 $(PDF)$.</p></blockquote><p>概率密度函数的值不代表概率，代表了密度或者趋势，所以 $f_X(x)$ 的值可能大于 $1$．</p><blockquote><p>其实，累积分布函数是为了研究连续型随机变量引出的概念．由于连续值不好研究每个值的概率，所以使用了累积的概率．而累积分布函数的微分，就代表了每个随机变量点处的概率情况，并不是具体的概率．</p></blockquote><p>概率密度函数并不是总是可见的，因为累积分布函数并不是处处可微的．</p><h4 id="性质-3"><a href="#性质-3" class="headerlink" title="性质"></a>性质</h4><ul><li>$f_X(x) \geq 1$</li><li>$\int_{-\infty}^\infty f_X(x)dx =1$</li><li>$\int_{x\in A} f_X(x)dx = P(X\in A)$</li><li>对于任意实数 $x_1, x_2, (x_1 \leq x_2): P(x_1 &lt; X \leq x_2) = F(x_2) - F(x_1) = \int_{x_1}^{x_2}f(x)dx$</li><li>若 f(x) 在 x 点连续, 则有 $F’(x) = f(x)$</li></ul><h3 id="期望"><a href="#期望" class="headerlink" title="期望"></a>期望</h3><p>假设 $X$ 是离散型随机变量 $PMF$ 为 $p_X(x)$, $g: R \rightarrow R$ 是任意函数．在这种情况下，$g(X)$ 可以认为是随机变量，我们可以定义 $g(X)$ 的期望（Expectation）和期望值：</p><center>$E[g(X)] = \sum_{x\in X} g(x)p_X(x)$</center></br><p>如果 $X$ 是连续型随机变量 $PDF$ 为 $f_X(x)$，$g(X)$ 的期望值为：</p><center>$E(g(X)) = \int _{-\infty}^\infty g(x)f_X(x)dx$</center></br><p>直观的，可以将 $g(X)$ 的期望看做是 $g(x)$ 对 $x$ 不同值取值的加权平均，其中权重由 $p_X(x)$ 和 $f_X(x)$ 给出．一种特殊的情况，令 $g(x) = x$，可以找到随机变量本身的期望 $E[X]$，这也可以称为随机变量 $X$ 的平均值． </p><h4 id="性质-4"><a href="#性质-4" class="headerlink" title="性质"></a>性质</h4><ul><li>$E[C] = C$</li><li>$E[Cf(X)] = CE[f(X)]$</li><li>$E[f(X) + g(X)] = E[f(X)] + E[g(X)]$</li><li>如果 $X,Y$ 相互独立，$E[XY] = E[X]E[Y]$</li></ul><h3 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h3><p>随机变量 $X$ 的方差（Variance）是对随机变量 $X$ 的分布在其均值附近的集中程度的度量。 形式上，随机变量 $X$ 的方差定义为：</p><center>$D(X) = Var[X] = E[(X-E[X])^2]$</center></br><p>其中开方之后叫做标准差或者均方差 $\sqrt{D(X)}$．使用上一节的性质，可以得出方差的替代表达式：</p><center>$\begin{align}D(X) =  E[(X-E[X])^2] &= E[X^2 - 2XE[X] + E[X]^2] \\ &= E[X^2] - 2E[X]E[X] + E[X]^2 \\ &= E[X^2] - E[X]^2 \end{align}$</center></br><p>其中，第二个等式成立是由于，期望的值是一个常数和期望线性的原因．</p><blockquote><p>随机变量的方差表达了 $X$ 的取值与数学期望的偏离程度，若 $D(X)$ 很小，说明 $X$ 的取值比较集中在 $E(X)$ 附近，反之若 $D(X)$ 很大，说明 $X$ 的取值很分散．</p></blockquote><ul><li>方差实际上是求 $[X-E(X)]^2$ 的期望．</li><li>离散型随机变量：$D(X) = \sum_{k=1}^\infty [x_k-E(X)]^2 p_k$</li><li>连续型随机变量：$D(X) = \int_{-\infty}^\infty [x_k-E(X)]^2 f(x)dx$</li></ul><h4 id="性质-5"><a href="#性质-5" class="headerlink" title="性质"></a>性质</h4><ul><li>$D(C) = 0$</li><li>$D(Cf(X)) = C^2 D(f(X))$</li><li>$D(X+C) = D(X)$</li><li>$D(X+Y) = D(X) + D(Y) + 2E{(X-E(X))(Y-E(Y))}$</li><li>如果 $X,Y$ 相互独立，那么 $D(X+Y) = D(X) + D(Y)$</li><li>$D(X) = 0$ 的充要条件是 $P{X+E(X)} = 1$．</li></ul><h4 id="切比雪夫不等式"><a href="#切比雪夫不等式" class="headerlink" title="切比雪夫不等式"></a>切比雪夫不等式</h4><p>设随机变量 $X$ 具有数学期望 $E(X)=\mu$，方差 $D(X) = \sigma^2$，对于任意正数 $\xi$，不等式 $P{|X-x|\geq \xi } \leq {\sigma^2 \over \xi^2}$. </p><h3 id="常用的随机变量"><a href="#常用的随机变量" class="headerlink" title="常用的随机变量"></a>常用的随机变量</h3><h4 id="离散型随机变量"><a href="#离散型随机变量" class="headerlink" title="离散型随机变量"></a>离散型随机变量</h4><ul><li><p>伯努利分布：$X ～Bernoulli(p)$</p><center>$p(x) = \begin {cases} p  &  if \ p = 1 \\ 1-p  & if \ p =0 \end{cases}$</center></br></li><li><p>二项分布：$X～Binomial(n, p)$</p><center>$p(x) = \begin{pmatrix}n \\ x \end{pmatrix} p^x (1-p)^{n-x}$</center></br></li><li><p>几何分布：$X～Geometric(p)$</p><center>$p(x) = p(1-p)^{x-1}$</center></br></li><li><p>泊松分布：$X～P oisson(λ)$</p><center>$p(x) = e^{-\lambda} {\lambda^x\over x!}$</center></br><p>详细内容：<a href="https://hiyoungai.com/posts/9e79f075.html">泊松分布</a></p></li></ul><h4 id="连续型随机变量"><a href="#连续型随机变量" class="headerlink" title="连续型随机变量"></a>连续型随机变量</h4><ul><li><p>均匀分布：$X～U nif orm(a, b)$</p><center>$f(x) = \begin{cases}{1\over b-a} & if \ a\le x \le b \\ 0 & otherwise\end{cases}$</center></br><p>详细内容：</p></li><li><p>指数分布：$X～Exponential(λ)$</p><center>$f(x) = \begin{cases}\lambda e^{-\lambda x} & if \ x \ge 0 \\ 0 & otherwise \end{cases}$</center></br><p>详细内容：<a href="https://hiyoungai.com/posts/53cfa4f7.html">指数分布</a></p></li><li><p>正态分布：$X～N ormal(μ, σ 2 )$</p><center>$f(x) = {1\over \sqrt{2\pi} \sigma} e^{-{1\over 2\sigma^2}(x-\mu)^2} $</center></br><p>详细内容：<a href="https://hiyoungai.com/posts/46e4f7d7.html">正态分布(高斯分布)</a></p></li></ul><p>下面是这些函数的图形：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_prob_cdf_pmf_001.png" alt=""></p><p>下面是分布的性质公式摘要：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_prob_cdf_pmf_002.png" alt=""></p><h2 id="Two-random-variables"><a href="#Two-random-variables" class="headerlink" title="Two random variables"></a>Two random variables</h2><p>目前为止，只考虑了单变量的情况，但是在许多情况下，随机变量的个数都是多个的．下面首先来考虑两个随机变量的情况．</p><h3 id="联合和边际分布函数"><a href="#联合和边际分布函数" class="headerlink" title="联合和边际分布函数"></a>联合和边际分布函数</h3><p>设 $E$ 是一个随机试验，它的样本空间是 $S = {e}$，设 $X = X(e)$ 和 $Y = Y(e)$ 是定义在 $S$ 上的随机变量，由他们构成的一个向量（$X,Ｙ$）叫做二维随机向量或二维随机变量．</p><p>假设我们有两个随机变量 $X$ 和 $Y$。 处理这两个随机变量的一种方法是分别考虑每个变量。 如果这样做，我们只需要 $F_X(x)$ 和 $F_Y(y)$．但是，如果我们想知道随机实验结果中 $X$ 和 $Y$ 同时取的值，我们需要一个更复杂的结构，称为联合累积 $X$ 和 $Y$ 的分布函数．</p><center>$F_{XY}(x,y) = P(X\le x, Y\le y)$</center></br><p>可以证明，通过知道联合分布函数，可以计算出涉及 $x,y$ 的事件概率．</p><center>$F_X(x) = \lim_{y\rightarrow \infty} F_{XY} (x,y) dy$</center></br><center>$F_Y(y) = \lim_{x\rightarrow \infty} F_{XY} (x,y)dx$</center></br><p>称 $F_X(x)$ 和 $F_Y(y)$ 为联合分布函数 $F_{XY}(x,y)$ 的边缘分布函数．</p><blockquote><p>设 $(X，Ｙ)$ 是二维随机变量，对于任意实数 $x, y$, 二元函数：$F(x,y)=P{ (X\leq x) \cap (Y\leq y)}=P{ X\leq x , Y\leq y}$称为二维随机变量 $(X, Y)$ 的分布函数，或随机变量 $X, Y$ 的联合分布函数．</p></blockquote><h4 id="性质-6"><a href="#性质-6" class="headerlink" title="性质"></a>性质</h4><ul><li>$0 \leq F_{XY}(x,y)\le 1$</li><li>$\lim_{x,y \rightarrow \infty} F_{XY}(x,y)=1$</li><li>$lim_{x,y\rightarrow -\infty} F_{XY}(x,y) = 0$</li><li>$F_X(x)= lim_{y\rightarrow \infty} F_{XY}(x,y)$</li><li>$F(x, y)$ 是变量 $x, y$ 的不减函数，且当 $x_2 &gt; x_1$ 时有 $F(x_2,y) &gt; F(x_1,y)$.</li></ul><blockquote><p>$F(x,y)$ 是二维随机变量$(X,Y)$的分布函数，而$X,Y$各自也有分布函数，称为二维随机变量$(X,Y)$关于随机变量$X,Y$ 的边缘分布函数．$F_x(x)=P{X\leq x}=P{X\leq x,Y&lt;\infty}=F(x,\infty)$．同理$F_y(y)=F(\infty,y)$</p></blockquote><h3 id="联合和边际概率质量函数"><a href="#联合和边际概率质量函数" class="headerlink" title="联合和边际概率质量函数"></a>联合和边际概率质量函数</h3><p>如果 $X,Y$ 是离散型随机变量，那么联合概率质量函数 $p_{XY}:R \rightarrow R$ 为：</p><center>$p_{XY}(x,y) = P(X=x,Y=y)$</center></br><p>其中，$0 \leq P_{XY}(x,y)\leq 1$，$\sum_{x\in X} \sum_{y\in Y} P_{XY}(x,y) = 1$</p><p>那么如何区分两个变量的联合 $PMF$ 和每个变量的 $PMF$ :</p><center>$p_X(x) = \sum_{y} p_{xy}(x,y)$</center></br><p>同样适用于 $p_Y(y)$，在这种情况下，我们将 $p_X(x)$ 称为边际概率质量函数．在统计中，通过求和另一个变量来形成一个变量的边际分布的过程通常称为边际化。</p><blockquote><p>如果二维随机变量是离散的，那么$P{X=x_i,Y=y_j} = p_{ij},i=1,2\cdots, j=1,2\cdots$为二维离散随机变量的分布律，也称随机变量 $X,Y$ 的联合分布律．</p></blockquote><h3 id="联合和边际概率密度函数"><a href="#联合和边际概率密度函数" class="headerlink" title="联合和边际概率密度函数"></a>联合和边际概率密度函数</h3><p>如果 $X,Y$ 是具有联合分布函数 $F_{XY}(x,y)$ 的两个连续型随机变量，在这种情况下，$F_{XY}(x,y)$ 在 $x,y$ 上处处可微．那么定义联合概率密度函数为：</p><center>$f_{XY}(x,y)={\partial^2 F_{XY}(x,y) \over \partial x \partial y}$</center></br><p>与一维度的情况一样 $f_{XY}(x,y) \neq P(X=x,Y=y)$，但是</p><center>$\int \int_{x\in A} f_{XY}(x,y)dxdy = P((X,Y)\in A)$</center></br><p>概率密度函数 $f_{XY}(x,y)$ 的值总是非负的，但是可能会大于 $1$．尽管如此，在某些情况下 $\int \int f_{XY}(x,y) = 1$．</p><p>类似离散型随机变量，我们定义：</p><center>$f_X(x) = \int_{-\infty} ^\infty f_{XY} (x,y)dy$</center></br><p>称为 $X$ 的边际概率密度函数，同样适用于 $Y$．</p><blockquote><p>对于二维随机变量$X,Y$ 有分布函数 $F(X,Y)$,　对于任意$x,y$有: $F(x,y)=\int_{-\infty}^\infty\int_{-\infty}^\infty(\mu,v)d\mu dv$ ，则称$f(\mu, v)$　是二维随机变量$(X,Y)$ 的概率密度，或随机变量 $X,Y$ 的联合概率密度．称二维随机变量 $X,Y$ 是连续型随机变量．</p></blockquote><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><ol><li>如果是离散的，那么就叫做边缘分布律，如果是连续的，那么就叫做边缘概率密度．</li><li>知道边缘分布，无法确定联合分布．</li></ol><h3 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h3><p>条件分布试图回答以下问题：当我们知道 $X$ 必须取某个值 $x$ 时，$Y$ 上的概率分布是多少？ 在离散情况下，给定 $Y$ 的 $X$ 的条件概率质量函数很简单：</p><center>$p_{Y|X} (y|x) = {p_{XY}(x,y)\over p_X(x)}$</center></br><p>假设 $p_X(x) \neq 0$．</p><p>在连续情况下，这种情况在技术上要复杂一些，因为连续随机变量 $X$ 取特定值 $x$ 的概率等于零。 忽略此技术要点，我们仅通过类似于离散情况的定义，将给定 $X = x$ 的 $Y$ 的条件概率密度定义为：</p><center>$f_{Y|X}(y|x) = {f_{XY}(x,y)\over f_X(x)}$</center></br><p>假设 $f_X(x) \neq 0$．</p><blockquote><p>条件分布律：$P{X=x_i|Y=y_j}={P{X=x_i, Y=y_j} \over P{ Y=y_j}} = {p_{i,j}\over p_j}$</p></blockquote><h3 id="贝叶斯定则"><a href="#贝叶斯定则" class="headerlink" title="贝叶斯定则"></a>贝叶斯定则</h3><p>贝叶斯定律是一个有用的公式，当试图针对一个给定另一个变量的条件概率来推导表达式时，经常会出现这种公式．</p><p>对于离散随机变量 $X$ 和 $Y$：</p><center>$P_{Y|X} (y|x) = {P_{XY}(x,y)\over P_X(x)} = {P_{X|Y}(x|y)P_Y(y) \over \sum_{y\in Y} P_{X|Y}(x|y) P_Y(y)} $</center></br><p>简单点写：</p><center>$P(y|x) = {P(xy)\over p(x)} = {P(x|y)P(y) \over \sum_y (P(x|y) P(y))}$</center></br><p>对于连续随机变量 $X$ 和 $Y$：</p><center>$f_{Y|X}(y|x) = {f_{XY}(x,y)\over f_X(x)} = {f_{X|Y}(x|y) f_Y(y)\over \int_{-\infty}^\infty f_{X|Y}(x|y)f_Y(y) dy}$</center></br><h3 id="独立性"><a href="#独立性" class="headerlink" title="独立性"></a>独立性</h3><p>对于所有 $x,y$，如果 $F_{XY}(x,y) = F_X(x)F_Y(y)$，那么两个随机变量 $X，Y$ 相互独立． 等效的：</p><ul><li>对于离散随机变量，$p_{XY}(x,y) = p_X(x)p_Y(y)$</li><li>对于离散随机变量，$p_{Y|X}(y|x) = p_Y(y)$</li><li>对于连续随机变量，$f_{XY}(x,y)=f_X(x)f_Y(y)$</li><li>对于连续随机变量，$f_{Y|X}(y|x) = f_Y(y)$</li></ul><p>非正式地，如果知道一个变量的值永远不会对另一个变量的条件概率分布有任何影响，则两个随机变量 $X$ 和 $Y$ 是独立的，也就是说，您知道有关该对（$X，Y$）的所有信息。 只知道 $f(x)$ 和 $f(y)$ 。 以下引理将这种观察形式化：</p><p>如果 $X,Y$ 相互独立，那么对于任何子集 $A,B \in R$ 有：</p><center>$P(X\in A, Y\in B) = P(X\in A) P(Y \in B)$</center></br><p>通过使用上述引理，可以证明如果 $X$ 独立于 $Y$，那么 $X$ 的任何函数都独立于 $Y$ 的任何函数。</p><h3 id="期望和协方差"><a href="#期望和协方差" class="headerlink" title="期望和协方差"></a>期望和协方差</h3><p>假设有两个随机离散变量 $X,Y$ 和 $g:R^2 \rightarrow R$ 是关于这两个随机变量的函数．他们的期望 $E$ 可以用如下等式表示：</p><center>$E[g(X,Y)] = \sum_{x\in X}\sum_{y\in Y} g(x,y)p_{XY}(x,y)$</center></br><p>对于连续随机变量 $X,Y$，期望为：</p><center>$E[g(X,Y)] = \int\int g(x,y)f_{XY}(x,y)dxdy$</center></br><p>我们可以使用期望的概念来研究两个随机变量之间的关系。 特别地，两个随机变量 $X$ 和 $Y$ 的协方差定义为：</p><center>$Cov(X,Y) = E[(X-E[X])(Y-E[Y])]$</center></br><p>与方差类似，可以改写为：</p><center>$\begin{align}Cov(X,Y) &= E[(X-E[X])(Y-E[Y])] \\ &= E[XY -XE[Y]-YE[X] + E[X]E[Y]] \\ & = E[XY] - E[X]E[Y] - E[X]E[Y] + E[X]E[Y] \\ & = E[XY] -E[X]E[Y] \end{align}$</center></br><p>当 $Cov(X,Y) = 0$ 时，我们说 $X,Y$ 不相关．</p><h4 id="性质-7"><a href="#性质-7" class="headerlink" title="性质"></a>性质</h4><ul><li>$E[f(X,Y)+g(X,Y)] = E[f(X,Y)] + E[g(X,Y)]$</li><li>$D(X+Y) = D(X)+D(Y) + 2Cov(X,Y)$</li><li>$Cov(X,Y) = Cov(Y,X)$</li><li>$Cov(X,X) = D(X)$</li><li>$Cov(aX, bY) = abCov(X,Y)$</li><li>$Cov(X_1 +X_2, Y)= Cov(X_1,Y) +Cov(X_2,Y)$</li><li>如果 $X$ 和 $Y$ 是相互独立的，那么 $Cov(X,Y) = 0$</li><li>如果 $X$ 和 $Y$ 是相互独立的，那么 $E[f(X)g(Y)]= E[f(X)]E[g(Y)]$</li></ul><h2 id="Multiple-random-variables"><a href="#Multiple-random-variables" class="headerlink" title="Multiple random variables"></a>Multiple random variables</h2><h3 id="基本性质"><a href="#基本性质" class="headerlink" title="基本性质"></a>基本性质</h3><p>定义 $X_1,\cdots,X_n$ 的联合分布函数和联合概率密度函数，给定 $X_2,\cdots,X_n$ 情况下的 $X_1$ 的边缘概率密度函数和条件概率密度函数</p><center>$F_{X_1,\cdots,X_n}(x_1,\cdots,x_n) = P(X_1\leq X,\cdots,X_n\leq X)$</center></br><center>$f_{X_1\cdots,X_n} (x_1,\cdots,x_n) = {\partial^n F_{X_1,\cdots,X_n} \over \partial x_1 \cdots \partial x_n}$</center></br><center>$f_{X_1}(X_1) = \int \cdots \int f_{X_1,\cdots, X_n}(x_1,\cdots,x_n)dx_2,\cdots,dx_n$</center></br><center>$f_{X_1|X_2,\cdots,X_n}(x_1|x_2,\cdots,x_n) = {f_{X_1,\cdots,X_n}(x_1,\cdots,x_n)\over f_{X_2,\cdots,X_n}(x_2,\cdots,x_n)}$</center></br><p>计算 $A$ 的概率：</p><center>$P((x_1,\cdots,x_n) \in A) = \int_{(x_1,\cdots,x_n)\in A} f_{X_1,\cdots,x_n} (x_1,\cdots,x_n)dx_1\cdots dx_n$</center></br><h4 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h4><p>根据多个随机变量的条件概率的定义，可以证明：</p><center>$\begin {align} f(x_1,\cdots,x_n) &= f(x_n|x_1,\cdots,x_{n-1})f(x_1,\cdots,x_{n-1}) \\ &= f(x_n|x_1,\cdots,x_{n-1})f(x_{n-1}|x_1,\cdots, x_{n-2}) f(x_1,\cdots,x_{n-2}) \\ &=  \cdots \\ &= f(x_1) \prod_{i=2}^n f(x_i|x_1,\cdots,x_{i-1}) \end{align}$</center></br><h4 id="独立性-1"><a href="#独立性-1" class="headerlink" title="独立性"></a>独立性</h4><p>对于多个随机事件，$A_1,\cdots,A_k$，如果对于任意子集 $S \in {1,2,\cdots,k}$ 都是相互独立的，那么：</p><center>$P(\cap_{i\in S}A_i) = \prod_{i\in S} P(A_i)$</center></br><p>同样的，对于随机变量 $X_1,\cdots,X_n$，如果是相互独立的，那么：</p><center>$f(x_1,x_2,\cdots,x_n) = f(x_1)f(x_2)\cdots f(x_n)$</center></br><p>独立随机变量经常出现在机器学习算法中，其中我们假设属于训练集的训练示例代表来自某些未知概率分布的独立样本。为了清楚说明独立性的重要性，请考虑一个不良训练集，在该训练集中，我们首先从一些未知分布中采样单个训练示例$（x^{（1）}，y^{（1）}）$，然后添加 $m − 1$ 个副本 与训练集完全相同的训练示例。 在这种情况下，我们有（有些滥用符号）</p><center>$P((x^{(1)}, y^{(1)}), \cdots, (x^{(m)}, y^{(m)})) \neq \prod _{i=1}^m P(x^{(i)},y^{(i)})$</center></br><blockquote><p>$(x^{(i)}, y^{(i)})$ 是样本，可能是不独立同分布的，并不是多随机变量，所以是不等式。</p></blockquote><p>尽管训练集的大小为 $m$，但示例并非独立！ 虽然这里描述的过程显然不是为机器学习算法构建训练集的明智方法，但事实证明，在实践中，确实经常出现样本不独立的情况，并且具有减小训练集有效大小的作用。</p><h3 id="随机向量"><a href="#随机向量" class="headerlink" title="随机向量"></a>随机向量</h3><p>假设有 $n$ 个随机变量，当它们在一起时，可以放入一个向量中，称之为随机向量 $X = [X_1,X_2,\cdots, X_n]^T$。随机向量只是 $n$ 个随机变量的替代表示方法，所以对应的 $CDF$ 和 $PDF$ 等性质同样适用于随机向量。</p><h4 id="期望-1"><a href="#期望-1" class="headerlink" title="期望"></a>期望</h4><p>对于多随机变量的期望：</p><center>$E[g(X)]= \int g(x_1,\cdots,x_n)f_{X_1,\cdots,X_n}(x_1,\cdots,x_n)dx_1 dx_2\cdots dx_n$</center></br><p>如果换成向量的形式：</p><center>$g(X) = \begin{bmatrix} g_1(x) \\ g_2(x) \\ \vdots \\ g_m(x) \end{bmatrix}$</center></br><center>$E[g(X)] = \begin{bmatrix} E[g_1(X)] \\ E[g_2(X)] \\ \vdots \\ E[g_m(X)] \end{bmatrix}$</center></br><h4 id="协方差矩阵"><a href="#协方差矩阵" class="headerlink" title="协方差矩阵"></a>协方差矩阵</h4><p>对于给定的随机向量 $X:\Omega \rightarrow R^n$，其协方差矩阵 $\Sigma$ 是一个 $n\times n$ 的方阵，每个元素为 $\Sigma_{ij} = Cov(X_i,X_j)$。</p><p>根据协方差的定义，有：</p><center>$\begin{align} \Sigma &= \begin{bmatrix} Cov(X_1,X_1) & \cdots & Cov(X_1,X_n) \\ \vdots & \ddots & \vdots  \\ Cov(X_n,X_1) & \cdots & Cov(X_n,X_n) \end{bmatrix} \\ &= \begin{bmatrix} E[X^2_1]-E[X_1]E[X_1] & \cdots & E[X_1X_n]-E[X_1]E[X_n] \\ \vdots & \ddots & \vdots \\ E[X_nX_1]-E[X_n]E[X_1] & \cdots & E[X_N^2]-E[X_n]E[X_n] \end{bmatrix} \\ &= \begin{bmatrix} E[X^2_1] & \cdots & E[X_1X_n] \\ \vdots & \ddots & \vdots \\ E[X_nX_1] & \cdots & E[X^2_n] \end{bmatrix} - \begin{bmatrix} E[X_1]E[X_1] & \cdots & E[X_1]E[X_n] \\ \vdots & \ddots & \vdots \\ E[X_n]E[X_1] & \cdots & E[X_n]E[X_n] \end{bmatrix} \\ &= E[XX^T] - E[X]E[X]^T \\ &= \cdots \\ &= E[(X-E[X])(X-E[X])^T]\end{align}$</center></br><p>协方差矩阵有很多有用的性质：</p><ul><li>$\Sigma \geq 0$，那么 $\Sigma$ 为半正定。</li><li>$\Sigma = \Sigma^T$， 那么 $\Sigma$ 是对称的。 </li></ul><h3 id="多变量高斯分布"><a href="#多变量高斯分布" class="headerlink" title="多变量高斯分布"></a>多变量高斯分布</h3><p>随机向量 $X$ 上概率分布的一个特别重要的例子称为多元高斯分布或多元正态分布。一个随机向量 $X \in R^n$ 表示具有均值 $\mu \in R^n$，协方差矩阵 $\Sigma \in S^n_{++}$ 的多元正态分布（或高斯分布，其中 $S^n_{++}$ 表示 $n\times n$ 的正定方阵 ）。</p><center>$f_{X_1,\cdots ,X_n}(x_1,\cdots,x_n;\mu,\Sigma) = {1\over (2\pi)^{n\over 2}|\Sigma|^{1\over 2}} exp(-{1\over 2}(x-\mu)^T \Sigma ^{-1}(x-\mu))$</center></br><p>通常写为：$X～ N(\mu, \Sigma)$。</p><p>一般而言，高斯随机变量在机器学习和统计中非常有用，主要有两个原因。首先，在统计算法中对“噪声”进行建模时，它们非常常见。通常，噪声可以认为是影响测量过程的大量小独立随机扰动的累积；根据中心极限定理，独立随机变量的总和将趋于看起来是高斯。其次，高斯随机变量对于许多分析操作很方便，因为在实践中出现的许多涉及高斯分布的积分都具有简单的封闭形式解。</p><h2 id="其他-1"><a href="#其他-1" class="headerlink" title="其他"></a>其他</h2><h3 id="大数定律"><a href="#大数定律" class="headerlink" title="大数定律"></a>大数定律</h3><p>大数定律是叙述随机变量序列的前一些项的算术平均值在某种条件下收敛到这些项的均值的算术平均值。</p><h3 id="中心极限定理"><a href="#中心极限定理" class="headerlink" title="中心极限定理"></a>中心极限定理</h3><p>中心极限定理是确定在什么条件下，大量随机变量之和的分布逼近于正态分布。</p>]]></content>
      
      
      <categories>
          
          <category> 概率论与数理统计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 概率论 </tag>
            
            <tag> 随机变量 </tag>
            
            <tag> 条件概率 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>正态分布(高斯分布)</title>
      <link href="posts/46e4f7d7.html"/>
      <url>posts/46e4f7d7.html</url>
      
        <content type="html"><![CDATA[<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>正态分布又名高斯分布（Gaussian distribution），是一种常见的连续概率分布．正态分布在统计学上非常重要，经常用来在自然和社会科学来代表一个不明的随机变量．正态分布的概率密度函数曲线呈钟形，因此人们又经常称之为钟形曲线．</p><p>若随机变量 $X$ 服从一个位置参数为 $\mu$ ，尺度参数为 $\sigma$ 的正态分布，记为 $X ～ N(\mu, \sigma^2)$．</p><p>概率密度函数为：</p><center>$f(x) = {1\over \sqrt{2\pi}\sigma}e^{-{(x-\mu)^2}\over 2\sigma^2}$    </center></br>图像为：<p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_gauss_001.png" alt=""></p><p>则累积分布函数为：</p><center>$F(x) = {1\over \sqrt{2\pi}\sigma}\int_{-\infty}^x e^{-{(t-\mu)^2}\over 2\sigma^2}dt$</center></br>图像为：<p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_gauss_002.png" alt=""></p><p>其中期望 $\mu$ 决定了分布的位置，标准差 $\sigma$ 或者方差 $\sigma^2$ 决定了分布的幅度．</p><h2 id="标准状态分布"><a href="#标准状态分布" class="headerlink" title="标准状态分布"></a>标准状态分布</h2><p>通常情况下，我们把期望 $\mu$ 为 0，方差 $\sigma^2$ 为 1 的正态分布，叫做标准正态分布．</p><h2 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h2><ul><li>密度函数 $f(x)$ 关于平均值线 $x=\mu$ 对称．</li><li>平均值与它的众数以及中位数同一数值．</li><li>函数曲线下 68.268949% 的面积在平均数左右的一个标准差 $\sigma$ 范围内．</li><li>95.449974% 的面积在平均数左右的两个标准差 $\sigma$ 范围内．</li><li>99.730020% 的面积在平均数左右的三个标准差 $\sigma$ 范围内．</li><li>99.993666% 的面积在平均数左右的四个标准差 $\sigma$ 范围内．</li><li>函数曲线的拐点位置在离平均数一个标准差 $\sigma$ 的位置．</li></ul><h2 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h2><ul><li>某高校男生的身高．</li><li>计算学生智力高低的概率．</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="[https://zh.wikipedia.org/wiki/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83](https://zh.wikipedia.org/wiki/正态分布)">正态分布-wiki</a></li><li><a href="https://blog.csdn.net/hhaowang/article/details/83898881#t1">正态分布（高斯分布）- CSDN</a></li><li><a href="https://blog.csdn.net/pipisorry/article/details/49516209">概率论：高斯/正态分布 - CSDN</a></li><li><a href="https://baijiahao.baidu.com/s?id=1621087027738177317&wfr=spider&for=pc">透彻理解高斯分布 - 百度百科</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 概率论与数理统计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 正态分布 </tag>
            
            <tag> 高斯分布 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>指数分布</title>
      <link href="posts/53cfa4f7.html"/>
      <url>posts/53cfa4f7.html</url>
      
        <content type="html"><![CDATA[<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><h2 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h2><h2 id="与泊松分布的关系"><a href="#与泊松分布的关系" class="headerlink" title="与泊松分布的关系"></a>与泊松分布的关系</h2>]]></content>
      
      
      <categories>
          
          <category> 概率论与数理统计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 指数分布 </tag>
            
            <tag> 泊松分布 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>泊松分布</title>
      <link href="posts/9e79f075.html"/>
      <url>posts/9e79f075.html</url>
      
        <content type="html"><![CDATA[<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>泊松分布（Poisson distribution），是一种统计与概率学里常见到的离散分布，由法国数学家西莫恩·德尼·泊松在1838年时发表。</p><p>泊松分布适合于描述单位时间内随机事件发生的次数的概率分布。</p><p>课本中的定义为：设随机变量 $X$ 所有可能取的值为 $0,1,2,3 \cdots$，而取得各个值的概率为</p><center>$P \{ X=k\} = {\lambda^ke^{-\lambda} \over k!}, k = 0,1,2,3\cdots$</center></br>其中 $\lambda > 0 $ 是常数，则称 $X$ 服从参数为 $\lambda$ 的泊松分布，记为 $X ～\pi(\lambda)$。<blockquote><p>吐槽一句，课本真的是晦涩难懂，就不能像wiki中的那么解释的简单明了一些，然后再引出公式吗？</p></blockquote><h2 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h2><p>生活中具有泊松分布的例子有很多：</p><ol><li>一本书一页中的印刷错误数；</li><li>某地区在一天内邮递遗失的信件数；</li><li>某医院平均每小时出生 3 个婴儿；</li><li>某公司平均每 10 分钟接到 1 个电话；</li><li>某超市平均每天销售 4 包xx牌奶粉；</li><li>某网站平均每分钟有 2 次访问；</li><li>等等。。</li></ol><h2 id="柏松定理"><a href="#柏松定理" class="headerlink" title="柏松定理"></a>柏松定理</h2><p>我们知道二项分布是进行了 $n$ 次伯努利实验</p><center>$P\{X = k \} = C_n^k p^k(1-p)^{n-k}$</center></br>如果当 $n$ 很大时，趋近于无限大的时候，如果计算随机变量的概率分布呢？也就是去计算下面的式子（这里用到了自然对数 e 的概念），令 $\lambda = np$：<center>$\lim_{n\to\infty} C_n^k p^k(1-p)^{n-k} = {\lambda^ke^{-\lambda} \over k!}$</center></br>其中 $\lambda = np$ 是常数，是二项分布的均值，当 $n$ 很大且 $p$ 很小的时候，那么可以使用泊松分布近似二项分布。<blockquote><p>一般 $n \geq 20, p \leq 0.05$</p></blockquote><h2 id="与二项分布的关系"><a href="#与二项分布的关系" class="headerlink" title="与二项分布的关系"></a>与二项分布的关系</h2><p>从泊松定理可以看出，二项分布在 $n$ 趋近于无限的情况下就是泊松分布。其实就是把从 $n$ 中取 $k$ 个值的概率，变成了从一段时间中取 $k$ 个值的概率。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_poisson_001.jpeg" alt=""></p><p>当 $p$ 取很小的值时两个分布几乎相同。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://zh.wikipedia.org/wiki/%E6%B3%8A%E6%9D%BE%E5%88%86%E4%BD%88">泊松分布-wiki</a></li><li><a href="https://www.zhihu.com/question/26441147">知乎-马同学</a></li><li><a href="http://www.ruanyifeng.com/blog/2015/06/poisson-distribution.html">阮一峰-泊松分布</a></li><li><a href="https://zhuanlan.zhihu.com/p/26263743">知乎-泊松分布 (Poisson Distributions) 的推导</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 概率论与数理统计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 二项分布 </tag>
            
            <tag> 泊松分布 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>古典概型</title>
      <link href="posts/d50294d6.html"/>
      <url>posts/d50294d6.html</url>
      
        <content type="html"><![CDATA[<h2 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h2><ol><li><p>乘法原理：如果进行 $A_1$ 过程有 $N_1$ 种方法，进行 $A_2$ 过程有 $N_2$ 种方法，那么进行 $A_1$ 后再进行 $A_2$ 过程，共有 $N_1 \cdot N_2$ 种方法。</p></li><li><p>加法原理：如果进行 $A_1$ 过程有 $N_1$ 种方法，进行 $A_2$ 过程有 $N_2$ 种方法，若 $A_1$ 过程与 $A_2$ 过程是平行的，那么进行 $A_1$ 过程或者进行 $A_2$ 过程一共有 $N_1 + N_2$ 种方法。</p></li><li><p>排列：从 n 个不同元素中取出 r 个元素进行排列，</p><ul><li>有放回采样，有重复的排列，共有 $n^r$  种排法。</li><li>无放回采样，也就是全排列，共有 $A_n^r = n\cdot (n-1)\cdots (n-r+1)$ 种排法。</li></ul></li><li><p>组合：从 n 个不同元素中取出 r 个元素而不考虑取出顺序，称为组合。由于不考虑取出的顺序，也就是不进行排列，所以去掉排列的 $r!$ 种可能，则 $C_n^r = {A_n^r \over r!}$。</p></li></ol><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><h3 id="古典概型"><a href="#古典概型" class="headerlink" title="古典概型"></a>古典概型</h3><ol><li>定义: 如果试验中所有可能出现的基本事件只有有限个，并且每个基本事件出现的可能性相等，则称此概率为古典概型。</li><li>特点: 1) 试验结果的有限性; 2) 所有结果的等可能性.</li><li>解题步骤:<ul><li>求出试验所有的基本事件数</li><li>求出事件 A 所包含的基本事件数</li></ul></li><li>公式: $P(A) = {A 包含的基本事件数 \over 基本事件总数} = {m \over n}$</li></ol><h3 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h3><ol><li>基本事件是事件的最小单位, 所有随机事件是由基本事件组成的.</li><li>任何两个基本事件是互斥的, $P(A\or B) = P(A) + P(B); P(A \and B) = P(A)P(B)$.</li><li>任何事件都可以表示成基本事件之和(不可能事件除外).</li></ol><h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><ol><li><p>题目：盒子里共有大小相同的3只白球，1 只黑球．若从中随机摸出两只球，则它们颜色相同的概率是?</p><p>四个球取出两球有 6 种等可能基本事件：(黑，白1)，(黑，白2)，(黑，白3)，(白1，白2)，(白1，白3)，(白2，白3)．两只球颜色相同有 3 种：(白1，白2)，(白1，白3)，(白2，白3)．<br>所以所求概率为 $P = {3\over 6} = {1 \over 2}$.</p></li><li><p>将 n 个球随机放入 N 个盒子，每个盒子至多有一个球的概率是?</p><p>每个球都可以放入 N 个盒子, 所以共有 $N^n$ 个基本事件, 而每个盒子最多一个球共有 $N(N-1)\cdots (N-n+1)$ 种放法, 所以所求概率为 $P = {N(N-1)\cdots (N-n+1) \over N^n} = {A_N^n \over N^n}$</p></li><li><p>n 个人中，他们的生日各不相同的概率是?</p><p>每个人可能有365种情况, 那么总的基本事件数是 $365^n$ , 各不相同的情况有 $365\cdot(365-1)\cdots(365-n+1)$种, 所以所求概率为 $P = {A_{356}^n \over  365^n}$.</p><p>至少有两个人生日相同的概率: $P = 1 - {A_{365}^n \over 365^n}$</p><p>在 n = 64 的情况下, 至少有两个人生日相同的概率接近 1 。</p></li><li><p>N 件产品中有 D 件次品，问从中取出 n 件恰好有 k 件次品的概率是?</p><p>一共有 $C_N^n$ 种取法, 其中取次品的可能数是 $C_D^k$, 取正品的可能数是 $C_{N-D}^{n-k}$, 则所求概率 $P = {C_D^k C_{N-D}^{n-k} \over C_N^n}$， 也叫超几何概率公式。</p></li><li><p>题目：甲乙两人赌技相同，各出赌本 500 元。约定：谁先胜出三局，则谁拿走全部的 1000 元。现已赌了三局，甲二胜一负而因故要终止赌博，问这 1000 元要如何分，才算公平？</p><p>分析：平均分对甲欠公平，全归甲则对乙欠公平，合理的分法是按一定的比例甲拿大头，乙拿小头。一种看来可以接受的方法是按已胜局数分，即甲拿 2/3，乙拿 1/3。仔细分析，发现这不合理，道理如下：</p><p>设想继续赌两局，则结果无非以下四种情况之一：</p><p>甲甲、甲乙、乙甲、乙乙</p><p>其中“甲乙”表示第一局甲胜第二局乙胜，其余类推。把已赌过的三局与上面这四个结果结合（即甲、乙赌完五局），我们看到：对前三个结果都是甲胜三局，因而的 1000 元，只有在最后一个结果才由乙得 1000 元。在赌技相同的情况下，四个结果应有等可能性。因此，甲、乙最终获胜的可能性的大小之比为 3：1。全部赌本应按这比例分，即甲分 750 元，乙分 250 元，才算公平合理。</p></li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="http://www.mofangge.com/qlist/shuxue/691/">关于“古典概型的定义及计算”的所有试题</a></li><li><a href="https://jpkc.jmpt.cn/suite/portal/displayItem/1122027/3%E5%8F%A4%E5%85%B8%E6%A6%82%E5%9E%8B.htm">古典概型详解+经典案例</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 概率论与数理统计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 古典概型 </tag>
            
            <tag> 概率论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>循环神经网络（Recurrent Neural Network，RNN）</title>
      <link href="posts/eb3fbcc4.html"/>
      <url>posts/eb3fbcc4.html</url>
      
        <content type="html"><![CDATA[<p>循环神经网络（Recurrent Neural Network, RNN）是一类用于处理序列数据的神经网络，已经在众多自然语言处理任务中取得了巨大成功以及广泛应用。</p><h2 id="序列数据"><a href="#序列数据" class="headerlink" title="序列数据"></a>序列数据</h2><p>序列数据是一种不定长度的数据，最典型的就是人类说的自然语言，还有其他的音乐/股票序列等。它是随着时间变化而变化的，所以有时也称为时间序列。</p><p>序列数据根据任务的不同，具有不同的表现形式。输入和输出可能都是序列，也可能只有其中一个是序列，还有可能是两个不同长度的序列。</p><h2 id="RNN-应用"><a href="#RNN-应用" class="headerlink" title="RNN 应用"></a>RNN 应用</h2><p>RNN 类神经网络在 NLP 起到了重要作用，在很多领域都将性能提高了很多。下面是一些主要的应用场景：</p><ul><li>语言模型（LM）<ul><li>预测下一个词。</li><li>拼写纠错。</li></ul></li><li>翻译模型（NMT）</li><li>文本摘要/文本生成</li><li>等等</li></ul><p>可以参考 Andrej Karpathy 的博<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">文章</a>，有更多丰富的应用。</p><h2 id="RNN-网络结构"><a href="#RNN-网络结构" class="headerlink" title="RNN 网络结构"></a>RNN 网络结构</h2><p>下图是一个简单的 RNN 结构，它由输入层，隐藏层和输出层组成。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_rnn_dl_jiegou_002.png" alt=""></p><p>从图中可以很容易看出，它的每个时间步 t 的输出 $\hat{y}^{(t)}$ 都与当前时刻的输入 $x^{(t)}$ 和前一时间的隐藏状态 $h^{(t)}$ 有关。</p><blockquote><p>通常实践时，t=0 时刻前一个的隐藏状态随机初始化，并且在预测时也为该值。</p></blockquote><h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p>下面用公式表示 RNN 的前向传播：</p><p>计算 t 时刻的隐藏状态，这里先假设为 tanh，W，V，U为隐藏单元的权重矩阵，。那么完整的表达式为:</p><center>$h^{(t)} = tanh(W_{hh}^T h^{(t-1)} + U_{xh}^Tx^{(t)} + b)$</center></br>然后计算 t 时刻的输出，这里使用 softmax 作为激活函数：<center>$\begin{align} o^{(t)} &= V_{ho}^Th^{(t)}+c \\ \hat{y}^{(t)} &= softmax(o^{(t)}) \end{align}$</center></br>> 注意，这里要搞清楚各个参数的下标，表示向量和矩阵的维度，方便在后续代码实现时认清参数。<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>一般都是用交叉熵损失函数 - Cross-entropy：</p><center>$L = \sum_t L^{(t)} = - \sum_t y^{(t)} log \ \hat{y}^{(t)}$</center></br>每一次进行计算，都需要前向传播一次，反向传播一次，而且不可以并行计算，所以时间复杂度为 O(t)。同时每个节点都需要存储当前的梯度，所以空间复杂度为 O(t)。<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>RNN 的反向传播 Backpropagation Through Time(BPTT) 由于是序列结构的原因，跟前馈神经网络略有不同。BPTT 的中心思想和 BP 算法相同，沿着需要优化的参数的负梯度方向不断寻找更优的点直至收敛，只不过RNN处理时间序列数据，所以要基于时间反向传播，故叫随时间反向传播。</p><h2 id="RNN-的局限性"><a href="#RNN-的局限性" class="headerlink" title="RNN 的局限性"></a>RNN 的局限性</h2><h3 id="梯度消散"><a href="#梯度消散" class="headerlink" title="梯度消散"></a>梯度消散</h3><p>梯度消散可以通过改变 RNN 结构如采用 LSTM 的方式抑制。</p><h3 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h3><p>梯度爆炸可以采用梯度剪裁（Gradient clipping）的方式避免，如梯度大于 5 的时候就强制梯度等于 5。</p><h3 id="长期依赖"><a href="#长期依赖" class="headerlink" title="长期依赖"></a>长期依赖</h3><p>即当前时刻无法从序列中间隔较大的那个时刻获得需要的信息。在理论上，RNN 完全可以处理长期依赖问题，但实际处理过程中，RNN 表现得并不好。同样采用 LSTM 等变体可以缓解该问题。</p><h2 id="其他结构"><a href="#其他结构" class="headerlink" title="其他结构"></a>其他结构</h2><p>RNN 的网络结构根据不同应用可以分为以下几种：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_rnn_one_to_many_001.png" alt=""></p><ul><li>One-to-one ，Vanilla Neural Networks<br>最简单的结构，然而效果不怎么好</li><li>One-to-many，Image Captioning, image -&gt; sequence of works<br>输入一个图片，输出一句描述图片的话；</li><li>Many-to-one，Sentiment Classification, sequence of words -&gt; sentiment<br>输入一句话，判断是正面还是负面情绪</li><li>Many-to-many，Machine Translation, seq of words -&gt; seq of words<br>有个延时的，譬如机器翻译。</li><li>Many-to-many，Video classification on frame level<br>输入一个视频，判断每帧分类。</li></ul><h3 id="Bi-RNN"><a href="#Bi-RNN" class="headerlink" title="Bi-RNN"></a>Bi-RNN</h3><p>在上面的标准 RNN 结构里，每个时间步的输出都是依赖于历史信息的，也就是后面的输出会包含了前面的信息内容。但是有时候文本的主要信息在后面，那么前面的输出就没有包含重要信息内容。双向循环神经网络（bi-RNN）为满足这种需要而被发明（Schuster and Paliwal, 1997）。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_birnn_jiegou_001.png" alt=""></p><p>Bi-RNN 的思路很简单，就是将输入序列从前向后计算一遍，然后从后向前计算一遍，将两个输出合并作为最后的输出。</p><p>这样的想法可以扩展到 2 维输入，比如图片，由四个 RNN 组成。每一个沿着四个方向中的一个计算：上、下、左、右。如果RNN 能够学习到承载长期信息，那在 2 维网络中，每个点（i; j）的输出 $O_{ij}$ 就能计算一个能捕捉到大多局部信息但仍依赖于长期输入的表示。相比卷积网络，应用于图像的RNN 计算成本通常更高，但允许同一特征图的特征之间存在长期横向的相互作用（Visin et al., 2015; Kalchbrenner et al., 2015）。</p><h3 id="Deep-RNN"><a href="#Deep-RNN" class="headerlink" title="Deep-RNN"></a>Deep-RNN</h3><p>深度 RNN，通过叠加 RNN 层数来实现。下面是三种不同叠加方法：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_deeprnn_jiegou_001.png" alt=""></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="http://norvig.com/chomsky.html">http://norvig.com/chomsky.html</a></li><li><a href="http://www.deeplearningbook.org/contents/rnn.html">http://www.deeplearningbook.org/contents/rnn.html</a></li><li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RNN </tag>
            
            <tag> BPTT </tag>
            
            <tag> LSTM </tag>
            
            <tag> GRU </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>N-gram语言模型</title>
      <link href="posts/853fc4e2.html"/>
      <url>posts/853fc4e2.html</url>
      
        <content type="html"><![CDATA[<h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><p>计算文本序列概率的模型叫做语言模型（LMs）。下面简单介绍如何使用 n-gram 模型来估计给定前一个单词的n-gram的最后一个单词的概率，并将概率分配给整个序列。把 n-gram 一些重点内容记录一下：</p><p>形式</p><center>$P(the|its \ water \ is \ so \ transparent \ that)$</center></br>简单直观的计算方式使用统计计数<center>$P(the|its \ water \ is \ so \ transparent \ that) = {C(its \ water \ is \ so \ transparent \ that \ the) \over C(its \ water \ is \ so \ transparent \ that)}$</center></br>这种方式在有新的句子加入时不利于维护，所以可以使用链式法则求出概率：<center>$\begin{align} P(x^{(1)},\cdots,x^{(T)}) &= P(x^{(1)})\times P(x^{(2)}|x^{(1)})\times \cdots \times P(x^{(T)}|,\cdots,x^{(1)}) \\ &= \prod^T_{t=1} P(x^{(t)}|x^{(t-1)},\cdots,x^{(1)})\end{align}$</center></br>但是根据前面整个句子的概率去计算下一个词，计算量太大，并且也不是完全依赖前面所有的词。可以根据前几个词来计算概率，所以有了 n-gram 模型。<p>为了计算语言模型，我们需要计算词的概率，以及一个词在给定前几个词的情况下的条件概率，即语言模型参数。设训练数据集为一个大型文本语料库，如维基百科的所有条目。词的概率可以通过该词在训练数据集中的相对词频来计算。例如，$P(w_1)$可以计算为 $w_1$ 在训练数据集中的词频（词出现的次数）与训练数据集的总词数之比。因此，根据条件概率定义，一个词在给定前几个词的情况下的条件概率也可以通过训练数据集中的相对词频计算。例如，$P(w_2∣w_1)$可以计算为$w_1,w_2$两词相邻的频率与 $w_1$ 词频的比值，因为该比值即 $P(w_1,w_2)$ 与 $ P( w_1) $ 之比；而 $P(w_3|w_1,w_2)$ 同理可以计算为$w_1、w_2$ 和 $w_3$ 三词相邻的频率与 $w_1$ 和 $w_2$ 两词相邻的频率的比值。以此类推。</p><h2 id="N-gram-语言模型"><a href="#N-gram-语言模型" class="headerlink" title="N-gram 语言模型"></a>N-gram 语言模型</h2><p>N-gram 是一个由 n 个连续单词组成的块，它的思想是一个单词出现的概率与它前 n-1 个出现的词有关。也就是每个词依赖于前 n-1 个词。下面是一些常见的术语以及示例，可以帮助你更好的理解 N-gram 语言模型：</p><ul><li>Unigrams：一元文法，由一个单词组成的 token，例如： “the”, “students”, “opened”, ”their”。</li><li>Bigrams：二元文法，也叫一元马尔科夫链。由连续两个单词组成的 token，例如：“the students”, “students opened”, “opened their”。</li><li>Trigrams：三元文法，由连续三个单词组成的 token，例如：“the students opened”, “students opened their”。</li><li>4-grams：四元文法，由连续四个单词组成的 token，例如：“the students opened their”。</li></ul><p>如何估计这些 n-gram 概率？估计概率的一种直观方法叫做极大似然估计（MLE）。可以通过从正态语料库中获取计数，并将计数归一化，使其位于 0 到 1 之间，从而得到 n-gram 模型参数的最大似然估计。</p><p>例如，要计算一个给定前一个单词为 x ，y 的 bigram 概率，将计算 bigram C(x y)的计数，并通过共享相同第一个单词 x的所有 bigram 的总和进行标准化。</p><center>$P(X_n|X_{n-1}) = {C(X_{n-1} X_n) \over \sum_X C(X_{n-1}X)}$</center></br>其中分子为 bigram C(x y) 在语料库中的计数，分母为前一个词为 x，后一个为任意词的 bigram 计数的总和。为了简单可以写成下面的形式：<center>$P(X_n|X_{n-1}) = {C(X_{n-1} X_n) \over C(X_{n-1})}$</center></br>这样就通过极大似然估计求得了概率值。但是有个问题是，在其他语料库中出现次数很多的句子可能在当前语料库中没有，所以很难进行泛化。下面是 n-gram 模型的稀疏性问题：<ol><li><p>如果要求的词没有在文本中出现，也就是分子的概率为 0。解决办法是添加一个很小的值给对应的词，这种方法叫做平滑，例如拉普拉斯平滑。这使得词表中的每个单词都至少有很小的概率。</p></li><li><p>如果前 n-1 个词没有出现在文本中，也就是分母的概率无法计算。解决办法如使用 “water  is  so  transparent  that“ 替代，这种方法叫做后退。保证作为条件的分母概率值存在。（还有其他平滑技术）</p></li><li><p>需要注意的是，概率是一个大于 0 小于 1 的数，随着相乘会变得很小。所以通常使用 log 的形式：</p><p>$p_1 \times p_2 \times p_3 \times p_4 = exp(log\ p_1 + log\ p_2 + log \ p_3 + log\ p_4)$</p></li><li><p>还有的是提高 n 的值会使稀疏性变得更糟糕，还会增加存储量，所以 n-gram 一般不会超过 5。</p></li><li><p>当n&gt;2时，比如 tigram，可能需要在头部添加两个start-token，后续看看效果如何。</p></li></ol><h2 id="平滑技术"><a href="#平滑技术" class="headerlink" title="平滑技术"></a>平滑技术</h2><p>上面提到过，一些 OOV 词汇，模型会分配 0 概率，这样是错误的。因而，必须分配给所有可能出现的字符串一个非零的概率值来避免这种错误的发生。平滑（smoothing）技术就是用来解决这类零概率问题的。提高低概率（如零概率），降低高概率，尽量使概率分布趋于均匀。</p><h3 id="拉普拉斯平滑"><a href="#拉普拉斯平滑" class="headerlink" title="拉普拉斯平滑"></a>拉普拉斯平滑</h3><p>Laplace Smoothing，做平滑的最简单的方法是在我们将它们归一化为概率之前，在所有的计数上加1。拉普拉斯平滑在现代 n-gram 模型中并没有得到很好的应用，但它很好地引入了我们在其他平滑算法中看到的许多概念，给出了一个有用的基线，也是文本分类等其他任务的实用平滑算法。下面在 1-gram 上看看拉普拉斯平滑：</p><center>$P_{Laplace}(w_i) = {c_i + 1 \over N+V}$</center></br>### 加法平滑<p>拉普拉斯平滑也算是加法平滑的一种，加法平滑的一般形式：</p><center>$P_{Add-k}(w_n|w_{n-1}) = {C(w_{n-1}w_n)+k \over C(w_{n-1}+kV)}$</center></br>尽管 add-k 在某些任务(包括文本分类)中很有用，但它在语言建模中仍然不能很好地工作，生成的计数方差很差。<h3 id="Katz-平滑"><a href="#Katz-平滑" class="headerlink" title="Katz 平滑"></a>Katz 平滑</h3><h3 id="Kneser-Ney-平滑"><a href="#Kneser-Ney-平滑" class="headerlink" title="Kneser-Ney  平滑"></a>Kneser-Ney  平滑</h3><h3 id="Backoff-and-Interpolation"><a href="#Backoff-and-Interpolation" class="headerlink" title="Backoff and Interpolation"></a>Backoff and Interpolation</h3><h2 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h2><h3 id="困惑度"><a href="#困惑度" class="headerlink" title="困惑度"></a>困惑度</h3><p>在 NLP 中，通常使用困惑度（Perplexity，PPL）作为衡量语言模型好坏的内部指标，此外还有外部指标需要人工评价</p><center>$\begin{align} P(W) &= P(w_1 w_2 \cdots w_N)^{-{1\over N}} \\ &= \sqrt[N]{1\over P(w_1 w_2 \cdots w_N)} \\ &= \sqrt[N]{\prod^N_{i=1}{1\over P(w_i|w_1\cdots w_{i-1})}} \end{align}$</center></br>对于 bigram 模型的困惑度：<center>$PP(W) = \sqrt[N]{\prod^N_{i=1} {1\over P(w_i|w_{i-1})}}$</center></br>困惑度越低，说明生成的语言越接近真实语言，常用于机器翻译和文本生成等 NLP 任务中。困惑度等价于交叉熵损失函数：<center>$\begin{align} &= \prod^T_{t=1}({1\over \hat{y}^{(t)}_{x_{t+1}}})^{1\over T} \\  &=exp({1\over T \sum^T_{t=1}}-log\hat{y}^{(t)}_{x_{t+1}}) \\ &= exp(J(\theta)) \end{align}$ </center></br>## References<ol><li><a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf">N-gram Language Models</a></li><li>《统计自然语言处理》- 宗成庆</li></ol>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 语言模型 </tag>
            
            <tag> N-gram </tag>
            
            <tag> 困惑度 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Arxiv查询接口详解</title>
      <link href="posts/8ba6751b.html"/>
      <url>posts/8ba6751b.html</url>
      
        <content type="html"><![CDATA[<p>Arxiv API 允许以编程方式获取 <a href="https://arxiv.org/">https://arxiv.org</a> 上的论文。API 的基本结构为：</p><pre class="line-numbers language-bash"><code class="language-bash">http://export.arxiv.org/api/<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;method_name&amp;#125;?&amp;#123;parameters&amp;#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="查询接口"><a href="#查询接口" class="headerlink" title="查询接口"></a>查询接口</h2><p>查询接口的的 method_name 为 query，下面是查询方法的参数，参数之间以 <em>&amp;</em> 分隔。</p><table><thead><tr><th align="center">parameters</th><th align="center">type</th><th align="center">defaults</th><th align="center">required</th></tr></thead><tbody><tr><td align="center">search_query</td><td align="center">string</td><td align="center">None</td><td align="center">No</td></tr><tr><td align="center">id_list</td><td align="center">comma-delimited string<br />（以 ‘，’ 分隔的字符串）</td><td align="center">None</td><td align="center">No</td></tr><tr><td align="center">start</td><td align="center">int</td><td align="center">0</td><td align="center">No</td></tr><tr><td align="center">max_results</td><td align="center">int</td><td align="center">10</td><td align="center">No</td></tr></tbody></table><h3 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h3><ul><li>如果 API 只包含 search_query（不包含 id_list），那么返回与 search_query 内容匹配的结果。</li><li>如果 API 只包含 id_list（不包含 search_query），那么返回 id_list 中每一项的结果。</li><li>如果 API 中包含了 search_query 和 id_list，那么返回在 id_list 中，并且与 search_query 匹配的文章。</li></ul><h3 id="分页查询"><a href="#分页查询" class="headerlink" title="分页查询"></a>分页查询</h3><p>通常情况下，一个查询可能有成百上千个返回结果。有时候我们不希望一次性查询到这么多数量，那么可以使用 <em>start</em> 和 <em>max_results</em> 两个字段来进行分页查询。</p><ul><li>start 是查询的起始索引，以 0 为第一个。</li><li>max_results 是查询返回的集合数。</li></ul><p>下面来举例说明一下：</p><pre class="line-numbers language-bash"><code class="language-bash">http://export.arxiv.org/api/query?search_query<span class="token operator">=</span>all:electron<span class="token operator">&amp;</span>start<span class="token operator">=</span>0<span class="token operator">&amp;</span>max_results<span class="token operator">=</span>10 <span class="token punctuation">(</span>1<span class="token punctuation">)</span>http://export.arxiv.org/api/query?search_query<span class="token operator">=</span>all:electron<span class="token operator">&amp;</span>start<span class="token operator">=</span>10<span class="token operator">&amp;</span>max_results<span class="token operator">=</span>10 <span class="token punctuation">(</span>2<span class="token punctuation">)</span>http://export.arxiv.org/api/query?search_query<span class="token operator">=</span>all:electron<span class="token operator">&amp;</span>start<span class="token operator">=</span>20<span class="token operator">&amp;</span>max_results<span class="token operator">=</span>10 <span class="token punctuation">(</span>3<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>查询结果分别为：</p><ol><li>0 - 9</li><li>10 - 19</li><li>20 - 29</li></ol><p>需要注意的是，由于 API 的限制，在多次调用 API 的情况下，建议每次调用的时间间隔为 3 秒。每次调用返回的最大数量为 2000 个。arXiv的硬限制约为 50,000 条记录； 对于与 50,000 多个原稿匹配的查询，无法接收全部结果. 解决这个问题的最简单的解决方案是将中断查询成小块，例如使用的时间片，与一系列日期的<code>submittedDate</code>或<code>lastUpdatedDate</code> 。</p><h3 id="排序查询"><a href="#排序查询" class="headerlink" title="排序查询"></a>排序查询</h3><p>对查询的结果进行排序有两个选项：<em>sortBy</em> 和 <em>sortOrder</em>。</p><ul><li>sortBy 的值有：relevance，lastUpdatedDate 和 submittedDate。</li><li>sortOrder 的值有：ascending 和 descending。</li></ul><p>示例：</p><pre class="line-numbers language-bash"><code class="language-bash">http://export.arxiv.org/api/query?search_query<span class="token operator">=</span>ti:%22electron%20thermal%20conductivity%22<span class="token operator">&amp;</span>sortBy<span class="token operator">=</span>lastUpdatedDate<span class="token operator">&amp;</span>sortOrder<span class="token operator">=</span>ascending<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="结果响应"><a href="#结果响应" class="headerlink" title="结果响应"></a>结果响应</h2><p>API 的 Response 内容中是以 <em>Atom 1.0</em> 为主体的，<em>Atom</em> 是 XML 的一种语法。下面分别来说明各个标签的含义。</p><h3 id="Feed-Metadata"><a href="#Feed-Metadata" class="headerlink" title="Feed Metadata"></a>Feed Metadata</h3><p>每个 Response 都会包含的内容：</p><ol><li>版本和命名空间</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>?xml version<span class="token operator">=</span><span class="token string">"1.0"</span> encoding<span class="token operator">=</span><span class="token string">"UTF-8"</span>?<span class="token operator">></span><span class="token operator">&lt;</span>feed xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span><span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol start="2"><li>Title：feed 的标题，通常为查询 URL 的字符串。</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>title xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span><span class="token operator">></span>    ArXiv Query:  search_query<span class="token operator">=</span>all:electron<span class="token operator">&amp;</span>amp<span class="token punctuation">;</span>id_list<span class="token operator">=</span><span class="token operator">&amp;</span>amp<span class="token punctuation">;</span>start<span class="token operator">=</span>0<span class="token operator">&amp;</span>amp<span class="token punctuation">;</span>max_results<span class="token operator">=</span>1<span class="token operator">&lt;</span>/title<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ol start="3"><li>Id：查询的唯一标识（注意不是查询的每个文章的 id），保证每个查询 id 是唯一的。</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>id xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span><span class="token operator">></span>    http://arxiv.org/api/cHxbiOdZaP56ODnBPIenZhzg5f8<span class="token operator">&lt;</span>/id<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ol start="4"><li>Link：查询 URL 的规范化。</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>link xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span> href<span class="token operator">=</span><span class="token string">"http://arxiv.org/api/query?search_query=all:electron&amp;amp;id_list=&amp;amp;start=0&amp;amp;max_results=1"</span> rel<span class="token operator">=</span><span class="token string">"self"</span> type<span class="token operator">=</span><span class="token string">"application/atom+xml"</span>/<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol start="5"><li>Updated：提供了 feed 内容最后一次更新的时间。</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>updated xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span><span class="token operator">></span>2007-10-08T00:00:00-04:00<span class="token operator">&lt;</span>/updated<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol start="6"><li>Opensearch：扩展元素，包含了查询的返回数量以及分页信息等。</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>opensearch:totalResults xmlns:opensearch<span class="token operator">=</span><span class="token string">"http://a9.com/-/spec/opensearch/1.1/"</span><span class="token operator">></span>   1000<span class="token operator">&lt;</span>/opensearch:totalResults<span class="token operator">></span><span class="token operator">&lt;</span>opensearch:startIndex xmlns:opensearch<span class="token operator">=</span><span class="token string">"http://a9.com/-/spec/opensearch/1.1/"</span><span class="token operator">></span>   0<span class="token operator">&lt;</span>/opensearch:startIndex<span class="token operator">></span><span class="token operator">&lt;</span>opensearch:itemsPerPage xmlns:opensearch<span class="token operator">=</span><span class="token string">"http://a9.com/-/spec/opensearch/1.1/"</span><span class="token operator">></span>   1<span class="token operator">&lt;</span>/opensearch:itemsPerPage<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Entry-Metadata"><a href="#Entry-Metadata" class="headerlink" title="Entry Metadata"></a>Entry Metadata</h3><p>正常情况下，Response 返回结果中的 <em>feed</em> 标签会包含 0 个或者多个 <em>entry</em> 标签。每个 entry 表示一个查询的返回文章，下面分别说一下 entry 中的各个元素。</p><ol><li>Title：返回文章的标题</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>title xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span><span class="token operator">></span>    Multi-Electron Production at High Transverse Momenta <span class="token keyword">in</span> ep Collisions at HERA<span class="token operator">&lt;</span>/title<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ol start="2"><li>Id：文章的 URL ，可以认为是文章的绝对路径。最后一个字段是文章的唯一标识符。</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>id xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span><span class="token operator">></span>    http://arxiv.org/abs/hep-ex/0307015<span class="token operator">&lt;</span>/id<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ol start="3"><li>Published/Updated：文章的发布日期和更新日期。</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>published xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span><span class="token operator">></span>    2007-02-27T16:02:02-05:00<span class="token operator">&lt;</span>/published<span class="token operator">></span><span class="token operator">&lt;</span>updated xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span><span class="token operator">></span>    2007-06-25T17:09:59-04:00<span class="token operator">&lt;</span>/updated<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="4"><li>Summary：文章的摘要。</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>summary xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span><span class="token operator">></span>    Multi-electron production is studied at high electron transverse momentum    <span class="token keyword">in</span> positron- and electron-proton collisions using the H1 detector at HERA.    The data correspond to an integrated luminosity of 115 pb-1. Di-electron    and tri-electron event yields are measured. Cross sections are derived <span class="token keyword">in</span>    a restricted phase space region dominated by photon-photon collisions. In    general good agreement is found with the Standard Model predictions.    However, <span class="token keyword">for</span> electron pair invariant masses above 100 GeV, three    di-electron events and three tri-electron events are observed, compared to    Standard Model expectations of 0.30 \pm 0.04 and 0.23 \pm 0.04,    respectively.<span class="token operator">&lt;</span>/summary<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="5"><li>Author：文章的作者，包含一个或者多个 name 标签，分别表示多个作者。</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>author xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span><span class="token operator">></span>      <span class="token operator">&lt;</span>name xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span><span class="token operator">></span>H1 Collaboration<span class="token operator">&lt;</span>/name<span class="token operator">></span><span class="token operator">&lt;</span>/author<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ol start="6"><li>Category：文章的分类。</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>category xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span> term<span class="token operator">=</span><span class="token string">"cs.LG"</span> scheme<span class="token operator">=</span><span class="token string">"http://arxiv.org/schemas/atom"</span>/<span class="token operator">></span><span class="token operator">&lt;</span>category xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span> term<span class="token operator">=</span><span class="token string">"cs.AI"</span> scheme<span class="token operator">=</span><span class="token string">"http://arxiv.org/schemas/atom"</span>/<span class="token operator">></span><span class="token operator">&lt;</span>category xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span> term<span class="token operator">=</span><span class="token string">"I.2.6"</span> scheme<span class="token operator">=</span><span class="token string">"http://arxiv.org/schemas/atom"</span>/<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ol start="7"><li>Link，对于每个文章，最多有三个 link 元素，通过 ref 和 title 来区别，下面的表格表示 ref 和 title 的内容：</li></ol><table><thead><tr><th align="center">rel</th><th align="center">title</th><th align="center">refers to</th><th align="center">always present</th></tr></thead><tbody><tr><td align="center">alternate</td><td align="center">-</td><td align="center">abstract page</td><td align="center">yes</td></tr><tr><td align="center">related</td><td align="center">pdf</td><td align="center">pdf</td><td align="center">yes</td></tr><tr><td align="center">related</td><td align="center">doi</td><td align="center">resolved doi</td><td align="center">no</td></tr></tbody></table><p>例子：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>link xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span> href<span class="token operator">=</span><span class="token string">"http://arxiv.org/abs/hep-ex/0307015v1"</span> rel<span class="token operator">=</span><span class="token string">"alternate"</span> type<span class="token operator">=</span><span class="token string">"text/html"</span>/<span class="token operator">></span><span class="token operator">&lt;</span>link xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span> title<span class="token operator">=</span><span class="token string">"pdf"</span> href<span class="token operator">=</span><span class="token string">"http://arxiv.org/pdf/hep-ex/0307015v1"</span> rel<span class="token operator">=</span><span class="token string">"related"</span> type<span class="token operator">=</span><span class="token string">"application/pdf"</span>/<span class="token operator">></span><span class="token operator">&lt;</span>link xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span> title<span class="token operator">=</span><span class="token string">"doi"</span> href<span class="token operator">=</span><span class="token string">"http://dx.doi.org/10.1529/biophysj.104.047340"</span> rel<span class="token operator">=</span><span class="token string">"related"</span>/<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ol start="8"><li>arxiv:primary_category：主要分类的扩展元素。</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>arxiv:primary_category xmlns:arxiv<span class="token operator">=</span><span class="token string">"http://arxiv.org/schemas/atom"</span> term<span class="token operator">=</span><span class="token string">"cs.LG"</span> scheme<span class="token operator">=</span><span class="token string">"http://arxiv.org/schemas/atom"</span>/<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol start="9"><li>arxiv:comment：评论扩展元素。</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>arxiv:comment xmlns:arxiv<span class="token operator">=</span><span class="token string">"http://arxiv.org/schemas/atom"</span><span class="token operator">></span>   23 pages, 8 figures and 4 tables<span class="token operator">&lt;</span>/arxiv:comment<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ol start="10"><li>arxiv:affiliation：作者从属关系。</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>author<span class="token operator">></span>   <span class="token operator">&lt;</span>name<span class="token operator">></span>G. G. Kacprzak<span class="token operator">&lt;</span>/name<span class="token operator">></span>   <span class="token operator">&lt;</span>arxiv:affiliation xmlns:arxiv<span class="token operator">=</span><span class="token string">"http://arxiv.org/schemas/atom"</span><span class="token operator">></span>NMSU<span class="token operator">&lt;</span>/arxiv:affiliation<span class="token operator">></span><span class="token operator">&lt;</span>/author<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ol start="11"><li>arxiv:journal_ref：期刊说明</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>arxiv:journal_ref xmlns:arxiv<span class="token operator">=</span><span class="token string">"http://arxiv.org/schemas/atom"</span><span class="token operator">></span>   Eur.Phys.J. C31 <span class="token punctuation">(</span>2003<span class="token punctuation">)</span> 17-29<span class="token operator">&lt;</span>/arxiv:journal_ref<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ol start="12"><li>arxiv:doi：doi 说明</li></ol><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>arxiv:doi xmlns:arxiv<span class="token operator">=</span><span class="token string">"http://arxiv.org/schemas/atom"</span><span class="token operator">></span>   10.1529/biophysj.104.047340<span class="token operator">&lt;</span>/arxiv:doi<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="Errors"><a href="#Errors" class="headerlink" title="Errors"></a>Errors</h3><p>返回错误，如果请求的响应出现错误，会返回一个详细的错误信息。例如下面是一个错误 id 的信息：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>?xml version<span class="token operator">=</span><span class="token string">"1.0"</span> encoding<span class="token operator">=</span><span class="token string">"utf-8"</span>?<span class="token operator">></span><span class="token operator">&lt;</span>feed xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span> xmlns:opensearch<span class="token operator">=</span><span class="token string">"http://a9.com/-/spec/opensearch/1.1/"</span><span class="token operator">></span>  <span class="token operator">&lt;</span>link xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span> href<span class="token operator">=</span><span class="token string">"http://arxiv.org/api/query?search_query=&amp;amp;id_list=1234.12345"</span> rel<span class="token operator">=</span><span class="token string">"self"</span> type<span class="token operator">=</span><span class="token string">"application/atom+xml"</span>/<span class="token operator">></span>  <span class="token operator">&lt;</span>title xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span><span class="token operator">></span>ArXiv Query: search_query<span class="token operator">=</span><span class="token operator">&amp;</span>amp<span class="token punctuation">;</span>id_list<span class="token operator">=</span>1234.12345<span class="token operator">&lt;</span>/title<span class="token operator">></span>  <span class="token operator">&lt;</span>id xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span><span class="token operator">></span>http://arxiv.org/api/kvuntZ8c9a4Eq5CF7KY03nMug+Q<span class="token operator">&lt;</span>/id<span class="token operator">></span>  <span class="token operator">&lt;</span>updated xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span><span class="token operator">></span>2007-10-12T00:00:00-04:00<span class="token operator">&lt;</span>/updated<span class="token operator">></span>  <span class="token operator">&lt;</span>opensearch:totalResults xmlns:opensearch<span class="token operator">=</span><span class="token string">"http://a9.com/-/spec/opensearch/1.1/"</span><span class="token operator">></span>1<span class="token operator">&lt;</span>/opensearch:totalResults<span class="token operator">></span>  <span class="token operator">&lt;</span>opensearch:startIndex xmlns:opensearch<span class="token operator">=</span><span class="token string">"http://a9.com/-/spec/opensearch/1.1/"</span><span class="token operator">></span>0<span class="token operator">&lt;</span>/opensearch:startIndex<span class="token operator">></span>  <span class="token operator">&lt;</span>opensearch:itemsPerPage xmlns:opensearch<span class="token operator">=</span><span class="token string">"http://a9.com/-/spec/opensearch/1.1/"</span><span class="token operator">></span>1<span class="token operator">&lt;</span>/opensearch:itemsPerPage<span class="token operator">></span>  <span class="token operator">&lt;</span>entry xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span><span class="token operator">></span>    <span class="token operator">&lt;</span>id xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span><span class="token operator">></span>http://arxiv.org/api/errors<span class="token comment" spellcheck="true">#incorrect_id_format_for_1234.12345&lt;/id></span>    <span class="token operator">&lt;</span>title xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span><span class="token operator">></span>Error<span class="token operator">&lt;</span>/title<span class="token operator">></span>    <span class="token operator">&lt;</span>summary xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span><span class="token operator">></span>incorrect <span class="token function">id</span> <span class="token function">format</span> <span class="token keyword">for</span> 1234.12345<span class="token operator">&lt;</span>/summary<span class="token operator">></span>    <span class="token operator">&lt;</span>updated xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span><span class="token operator">></span>2007-10-12T00:00:00-04:00<span class="token operator">&lt;</span>/updated<span class="token operator">></span>    <span class="token operator">&lt;</span>link xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span> href<span class="token operator">=</span><span class="token string">"http://arxiv.org/api/errors#incorrect_id_format_for_1234.12345"</span> rel<span class="token operator">=</span><span class="token string">"alternate"</span> type<span class="token operator">=</span><span class="token string">"text/html"</span>/<span class="token operator">></span>    <span class="token operator">&lt;</span>author xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span><span class="token operator">></span>      <span class="token operator">&lt;</span>name xmlns<span class="token operator">=</span><span class="token string">"http://www.w3.org/2005/Atom"</span><span class="token operator">></span>arXiv api core<span class="token operator">&lt;</span>/name<span class="token operator">></span>    <span class="token operator">&lt;</span>/author<span class="token operator">></span>  <span class="token operator">&lt;</span>/entry<span class="token operator">></span><span class="token operator">&lt;</span>/feed<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>下面提供了一些常见的错误：</p><table><thead><tr><th align="center"><strong>Sample query 示例查询</strong></th><th align="center"><strong>Error Explanation 错误解释</strong></th></tr></thead><tbody><tr><td align="center"><a href="http://export.arxiv.org/api/query?start=not_an_int">http://export.arxiv.org/api/query?start=not_an_int</a></td><td align="center"><code>start</code> 一定是个整数</td></tr><tr><td align="center"><a href="http://export.arxiv.org/api/query?start=-1">http://export.arxiv.org/api/query?start=-1</a></td><td align="center"><code>start</code>  必须 &gt;= 0</td></tr><tr><td align="center"><a href="http://export.arxiv.org/api/query?max_results=not_an_int">http://export.arxiv.org/api/query?max_results=not_an_int</a></td><td align="center"><code>max_results</code> 一定是个整数</td></tr><tr><td align="center"><a href="http://export.arxiv.org/api/query?max_results=-1">http://export.arxiv.org/api/query?max_results=-1</a></td><td align="center"><code>max_results</code> 必须 &gt;= 0</td></tr><tr><td align="center"><a href="http://export.arxiv.org/api/query?id_list=1234.1234">http://export.arxiv.org/api/query?id_list=1234.1234</a></td><td align="center">malformed id</td></tr><tr><td align="center"><a href="http://export.arxiv.org/api/query?id_list=cond—mat/0709123">http://export.arxiv.org/api/query?id_list=cond—mat/0709123</a></td><td align="center">malformed id</td></tr></tbody></table><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><h3 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h3><p>python2.7 上的简单请求：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> urlliburl <span class="token operator">=</span> <span class="token string">'http://export.arxiv.org/api/query?search_query=all:electron&amp;start=0&amp;max_results=1'</span>data <span class="token operator">=</span> urllib<span class="token punctuation">.</span>urlopen<span class="token punctuation">(</span>url<span class="token punctuation">)</span><span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span> data<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>python3 上的请求：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> urllib<span class="token punctuation">.</span>request <span class="token keyword">as</span> libreq<span class="token keyword">with</span> libreq<span class="token punctuation">.</span>urlopen<span class="token punctuation">(</span><span class="token string">'http://export.arxiv.org/api/query?search_query=all:electron&amp;start=0&amp;max_results=1'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> url<span class="token punctuation">:</span>    r <span class="token operator">=</span> url<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>r<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="查询的详细结构"><a href="#查询的详细结构" class="headerlink" title="查询的详细结构"></a>查询的详细结构</h3><p>在 arXiv 搜索引擎中，每篇文章都被划分为许多可以单独搜索的字段。 例如，可以搜索一篇文章的标题，以及作者列表、摘要、评论和期刊参考文献。 要搜索其中一个字段，只需在搜索词前加上字段前缀和冒号即可。 例如：</p><pre class="line-numbers language-bash"><code class="language-bash">http://export.arxiv.org/api/query?search_query<span class="token operator">=</span>au:del_maestro<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>下面的表格显示所有字段的前缀：</p><table><thead><tr><th align="center"><strong>prefix</strong></th><th align="center"><strong>explanation</strong></th></tr></thead><tbody><tr><td align="center">ti</td><td align="center">Title</td></tr><tr><td align="center">au</td><td align="center">Author</td></tr><tr><td align="center">abs</td><td align="center">Abstract</td></tr><tr><td align="center">co</td><td align="center">Comment</td></tr><tr><td align="center">jr</td><td align="center">Journal Reference</td></tr><tr><td align="center">cat</td><td align="center">Subject Category</td></tr><tr><td align="center">rn</td><td align="center">Report Number</td></tr><tr><td align="center">id</td><td align="center">Id (use <code>id_list</code> instead)</td></tr><tr><td align="center">all</td><td align="center">All of the above</td></tr></tbody></table><p>并且查询也支持布尔运算，假设我们希望找到作者 Adrian DelMaestro 的所有文章，其标题中也包含单词 checkerboard。 我们可以使用 AND 操作符构造下面的查询：</p><pre class="line-numbers language-bash"><code class="language-bash">http://export.arxiv.org/api/query?search_query<span class="token operator">=</span>au:del_maestro+AND+ti:checkerboard<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>下面是三种可能的布尔值：</p><ul><li>AND</li><li>OR</li><li>ANDNOT</li></ul><p>下面是特殊符号的含义以及转义字符：</p><table><thead><tr><th align="left">symbol</th><th align="left">encoding</th><th align="left">explanation</th></tr></thead><tbody><tr><td align="left">( )</td><td align="left">%28 %29</td><td align="left">用于为布尔运算符优先级对布尔表达式进行分组</td></tr><tr><td align="left">“ “</td><td align="left">%22 %22</td><td align="left">用于将多个单词组合成短语以搜索特定字段</td></tr><tr><td align="left">空格</td><td align="left">+</td><td align="left">用于扩展<code>search_query</code> 包含多个字段</td></tr></tbody></table><h3 id="返回的详细结构"><a href="#返回的详细结构" class="headerlink" title="返回的详细结构"></a>返回的详细结构</h3><p>下表列出了返回的 Atom 结果的每个元素:</p><table><thead><tr><th align="center"><strong>element</strong></th><th align="center"><strong>explanation</strong></th></tr></thead><tbody><tr><td align="center"><strong>feed elements</strong></td><td align="center"></td></tr><tr><td align="center">title</td><td align="center">包含规范化查询字符串的标题</td></tr><tr><td align="center">id</td><td align="center">分配给此查询的唯一 id</td></tr><tr><td align="center">updated</td><td align="center">最后一次更新此查询的搜索结果。 设置为当天的午夜</td></tr><tr><td align="center">link</td><td align="center">通过 GET 请求检索此提要的 url</td></tr><tr><td align="center">opensearch:totalResults</td><td align="center">此查询的搜索结果总数</td></tr><tr><td align="center">opensearch:startIndex</td><td align="center">总结果列表中第一个返回结果的基于0的索引</td></tr><tr><td align="center">opensearch:itemsPerPage</td><td align="center">每页返回的结果数</td></tr><tr><td align="center"><strong>entry elements</strong></td><td align="center"></td></tr><tr><td align="center">title</td><td align="center">文章的标题</td></tr><tr><td align="center">id</td><td align="center">文章的网址<code>http://arxiv.org/abs/id</code></td></tr><tr><td align="center">published</td><td align="center">文章的发布日期</td></tr><tr><td align="center">updated</td><td align="center">文章的更新日期，如果为 v1 版本，那么与发布日期相同</td></tr><tr><td align="center">summary</td><td align="center">文章摘要</td></tr><tr><td align="center">author</td><td align="center">每个作者有一个子元素 name，包含了作者的名字</td></tr><tr><td align="center">link</td><td align="center">可以给定与这篇文章关联的 3 个网址</td></tr><tr><td align="center">category</td><td align="center">文章分类</td></tr><tr><td align="center">arxiv:primary_category</td><td align="center">主要的 arXiv 分类</td></tr><tr><td align="center">arxiv:comment</td><td align="center">作者对此发表的评论</td></tr><tr><td align="center">arxiv:affiliation</td><td align="center">作者的从属关系</td></tr><tr><td align="center">arxiv:journal_ref</td><td align="center">参考文献</td></tr><tr><td align="center">arxiv:doi</td><td align="center">已解析的 DOI 的 url，指向外部资源</td></tr></tbody></table><h3 id="学科的分类"><a href="#学科的分类" class="headerlink" title="学科的分类"></a>学科的分类</h3><p>下面是学科分类字段以及对应的翻译（软件脚本自动翻译，如不对请勿喷）：</p><table><thead><tr><th align="center">字段</th><th align="center">学科（英文）</th><th align="center">学科（中文）</th></tr></thead><tbody><tr><td align="center">astro-ph</td><td align="center">Astrophysics</td><td align="center">天体物理</td></tr><tr><td align="center">astro-ph.CO</td><td align="center">Cosmology and Nongalactic Astrophysics</td><td align="center">宇宙学与非规则天体物理学</td></tr><tr><td align="center">astro-ph.EP</td><td align="center">Earth and Planetary Astrophysics</td><td align="center">地球与行星天体物理学</td></tr><tr><td align="center">astro-ph.GA</td><td align="center">Astrophysics of Galaxies</td><td align="center">星系的天体物理学</td></tr><tr><td align="center">astro-ph.HE</td><td align="center">High Energy Astrophysical Phenomena</td><td align="center">高能天体物理现象</td></tr><tr><td align="center">astro-ph.IM</td><td align="center">Instrumentation and Methods for Astrophysics</td><td align="center">天体物理学的仪器和方法</td></tr><tr><td align="center">astro-ph.SR</td><td align="center">Solar and Stellar Astrophysics</td><td align="center">太阳与恒星天体物理学</td></tr><tr><td align="center">cond-mat.dis-nn</td><td align="center">Disordered Systems and Neural Networks</td><td align="center">无序系统与神经网络</td></tr><tr><td align="center">cond-mat.mes-hall</td><td align="center">Mesoscale and Nanoscale Physics</td><td align="center">中尺度和纳米尺度物理学</td></tr><tr><td align="center">cond-mat.mtrl-sci</td><td align="center">Materials Science</td><td align="center">材料科学</td></tr><tr><td align="center">cond-mat.other</td><td align="center">Other Condensed Matter</td><td align="center">其他凝聚态</td></tr><tr><td align="center">cond-mat.quant-gas</td><td align="center">Quantum Gases</td><td align="center">量子气体</td></tr><tr><td align="center">cond-mat.soft</td><td align="center">Soft Condensed Matter</td><td align="center">软凝聚物</td></tr><tr><td align="center">cond-mat.stat-mech</td><td align="center">Statistical Mechanics</td><td align="center">统计力学</td></tr><tr><td align="center">cond-mat.str-el</td><td align="center">Strongly Correlated Electrons</td><td align="center">强关联电子</td></tr><tr><td align="center">cond-mat.supr-con</td><td align="center">Superconductivity</td><td align="center">超导现象</td></tr><tr><td align="center">cs.AI</td><td align="center">Artificial Intelligence</td><td align="center">人工智能</td></tr><tr><td align="center">cs.AR</td><td align="center">Hardware Architecture</td><td align="center">硬件架构</td></tr><tr><td align="center">cs.CC</td><td align="center">Computational Complexity</td><td align="center">计算复杂性</td></tr><tr><td align="center">cs.CE</td><td align="center">Computational Engineering, Finance, and Science</td><td align="center">计算工程，金融和科学</td></tr><tr><td align="center">cs.CG</td><td align="center">Computational Geometry</td><td align="center">计算几何</td></tr><tr><td align="center">cs.CL</td><td align="center">Computation and Language</td><td align="center">计算与语言</td></tr><tr><td align="center">cs.CR</td><td align="center">Cryptography and Security</td><td align="center">密码学与保安</td></tr><tr><td align="center">cs.CV</td><td align="center">Computer Vision and Pattern Recognition</td><td align="center">计算机视觉与模式识别</td></tr><tr><td align="center">CY</td><td align="center">Computers and Society</td><td align="center">电脑与社会</td></tr><tr><td align="center">cs.DB</td><td align="center">Databases</td><td align="center">数据库</td></tr><tr><td align="center">cs.DC</td><td align="center">Distributed, Parallel, and Cluster Computing</td><td align="center">分布式、并行和集群计算</td></tr><tr><td align="center">cs.DL</td><td align="center">Digital Libraries</td><td align="center">数字仓库</td></tr><tr><td align="center">cs.DM</td><td align="center">Discrete Mathematics</td><td align="center">离散数学</td></tr><tr><td align="center">cs.DS</td><td align="center">Data Structures and Algorithms</td><td align="center">数据结构和算法</td></tr><tr><td align="center">cs.ET</td><td align="center">Emerging Technologies</td><td align="center">新兴科技</td></tr><tr><td align="center">cs.FL</td><td align="center">Formal Languages and Automata Theory</td><td align="center">形式语言与自动机理论</td></tr><tr><td align="center">cs.GL</td><td align="center">General Literature</td><td align="center">一般文学</td></tr><tr><td align="center">cs.GR</td><td align="center">Graphics</td><td align="center">图形</td></tr><tr><td align="center">cs.GT</td><td align="center">Computer Science and Game Theory</td><td align="center">计算机科学与博弈论</td></tr><tr><td align="center">cs.HC</td><td align="center">Human-Computer Interaction</td><td align="center">人机交互</td></tr><tr><td align="center">cs.IR</td><td align="center">Information Retrieval</td><td align="center">信息检索</td></tr><tr><td align="center">cs.IT</td><td align="center">Information Theory</td><td align="center">信息理论</td></tr><tr><td align="center">cs.LG</td><td align="center">Learning</td><td align="center">学习</td></tr><tr><td align="center">cs.LO</td><td align="center">Logic in Computer Science</td><td align="center">计算机科学中的逻辑</td></tr><tr><td align="center">cs.MA</td><td align="center">Multiagent Systems</td><td align="center">多代理系统</td></tr><tr><td align="center">cs.MM</td><td align="center">Multimedia</td><td align="center">多媒体</td></tr><tr><td align="center">cs.MS</td><td align="center">Mathematical Software</td><td align="center">数学软件</td></tr><tr><td align="center">cs.NA</td><td align="center">Numerical Analysis</td><td align="center">数值分析</td></tr><tr><td align="center">cs.NE</td><td align="center">Neural and Evolutionary Computing</td><td align="center">神经和进化计算</td></tr><tr><td align="center">cs.NI</td><td align="center">Networking and Internet Architecture</td><td align="center">网络与互联网架构</td></tr><tr><td align="center">cs.OH</td><td align="center">Other Computer Science</td><td align="center">其他计算机科学</td></tr><tr><td align="center">cs.OS</td><td align="center">Operating Systems</td><td align="center">操作系统</td></tr><tr><td align="center">cs.PF</td><td align="center">Performance</td><td align="center">性能</td></tr><tr><td align="center">cs.PL</td><td align="center">Programming Languages</td><td align="center">编程语言</td></tr><tr><td align="center">cs.RO</td><td align="center">Robotics</td><td align="center">机器人技术</td></tr><tr><td align="center">cs.SC</td><td align="center">Symbolic Computation</td><td align="center">符号计算</td></tr><tr><td align="center">cs.SD</td><td align="center">Sound</td><td align="center">声音</td></tr><tr><td align="center">cs.SE</td><td align="center">Software Engineering</td><td align="center">软件工程</td></tr><tr><td align="center">cs.SI</td><td align="center">Social and Information Networks</td><td align="center">社会和信息网络</td></tr><tr><td align="center">cs.SY</td><td align="center">Systems and Control</td><td align="center">系统及控制</td></tr><tr><td align="center">econ.EM</td><td align="center">Econometrics</td><td align="center">计量经济学</td></tr><tr><td align="center">eess.AS</td><td align="center">Audio and Speech Processing</td><td align="center">音频及语音处理</td></tr><tr><td align="center">eess.IV</td><td align="center">Image and Video Processing</td><td align="center">图像和视频处理</td></tr><tr><td align="center">eess.SP</td><td align="center">Signal Processing</td><td align="center">信号处理</td></tr><tr><td align="center">gr-qc</td><td align="center">General Relativity and Quantum Cosmology</td><td align="center">广义相对论和量子宇宙学</td></tr><tr><td align="center">hep-ex</td><td align="center">High Energy Physics - Experiment</td><td align="center">高能物理实验</td></tr><tr><td align="center">hep-lat</td><td align="center">High Energy Physics - Lattice</td><td align="center">高能物理-晶格</td></tr><tr><td align="center">hep-ph</td><td align="center">High Energy Physics - Phenomenology</td><td align="center">高能物理-现象学</td></tr><tr><td align="center">hep-th</td><td align="center">High Energy Physics - Theory</td><td align="center">高能物理理论</td></tr><tr><td align="center">math.AC</td><td align="center">Commutative Algebra</td><td align="center">交换代数</td></tr><tr><td align="center">math.AG</td><td align="center">Algebraic Geometry</td><td align="center">代数几何</td></tr><tr><td align="center">math.AP</td><td align="center">Analysis of PDEs</td><td align="center">偏微分方程分析</td></tr><tr><td align="center">math.AT</td><td align="center">Algebraic Topology</td><td align="center">代数拓扑</td></tr><tr><td align="center">math.CA</td><td align="center">Classical Analysis and ODEs</td><td align="center">传统分析和微分方程</td></tr><tr><td align="center">math.CO</td><td align="center">Combinatorics</td><td align="center">组合数学</td></tr><tr><td align="center">math.CT</td><td align="center">Category Theory</td><td align="center">范畴理论</td></tr><tr><td align="center">math.CV</td><td align="center">Complex Variables</td><td align="center">复杂变量</td></tr><tr><td align="center">math.DG</td><td align="center">Differential Geometry</td><td align="center">微分几何</td></tr><tr><td align="center">math.DS</td><td align="center">Dynamical Systems</td><td align="center">动力系统</td></tr><tr><td align="center">math.FA</td><td align="center">Functional Analysis</td><td align="center">功能分析</td></tr><tr><td align="center">math.GM</td><td align="center">General Mathematics</td><td align="center">普通数学</td></tr><tr><td align="center">math.GN</td><td align="center">General Topology</td><td align="center">点集拓扑学</td></tr><tr><td align="center">math.GR</td><td align="center">Group Theory</td><td align="center">群论</td></tr><tr><td align="center">math.GT</td><td align="center">Geometric Topology</td><td align="center">几何拓扑学</td></tr><tr><td align="center">math.HO</td><td align="center">History and Overview</td><td align="center">历史和概述</td></tr><tr><td align="center">math.IT</td><td align="center">Information Theory</td><td align="center">信息理论</td></tr><tr><td align="center">math.KT</td><td align="center">K-Theory and Homology</td><td align="center">K 理论与同调</td></tr><tr><td align="center">math.LO</td><td align="center">Logic</td><td align="center">逻辑</td></tr><tr><td align="center">math.MG</td><td align="center">Metric Geometry</td><td align="center">度量几何学</td></tr><tr><td align="center">math.MP</td><td align="center">Mathematical Physics</td><td align="center">数学物理</td></tr><tr><td align="center">math.NA</td><td align="center">Numerical Analysis</td><td align="center">数值分析</td></tr><tr><td align="center">math.NT</td><td align="center">Number Theory</td><td align="center">数论</td></tr><tr><td align="center">math.OA</td><td align="center">Operator Algebras</td><td align="center">算子代数</td></tr><tr><td align="center">math.OC</td><td align="center">Optimization and Control</td><td align="center">优化和控制</td></tr><tr><td align="center">math.PR</td><td align="center">Probability</td><td align="center">概率</td></tr><tr><td align="center">math.QA</td><td align="center">Quantum Algebra</td><td align="center">量子代数</td></tr><tr><td align="center">math.RA</td><td align="center">Rings and Algebras</td><td align="center">环与代数</td></tr><tr><td align="center">math.RT</td><td align="center">Representation Theory</td><td align="center">表示论</td></tr><tr><td align="center">math.SG</td><td align="center">Symplectic Geometry</td><td align="center">辛几何</td></tr><tr><td align="center">math.SP</td><td align="center">Spectral Theory</td><td align="center">光谱理论</td></tr><tr><td align="center">math.ST</td><td align="center">Statistics Theory</td><td align="center">统计学理论</td></tr><tr><td align="center">math-ph</td><td align="center">Mathematical Physics</td><td align="center">数学物理</td></tr><tr><td align="center">nlin.AO</td><td align="center">Adaptation and Self-Organizing Systems</td><td align="center">适应与自组织系统</td></tr><tr><td align="center">nlin.CD</td><td align="center">Chaotic Dynamics</td><td align="center">混沌动力学</td></tr><tr><td align="center">nlin.CG</td><td align="center">Cellular Automata and Lattice Gases</td><td align="center">元胞自动机与格子气体</td></tr><tr><td align="center">nlin.PS</td><td align="center">Pattern Formation and Solitons</td><td align="center">模式形成与孤子</td></tr><tr><td align="center">nlin.SI</td><td align="center">Exactly Solvable and Integrable Systems</td><td align="center">严格可解可积系统</td></tr><tr><td align="center">nucl-ex</td><td align="center">Nuclear Experiment</td><td align="center">核试验</td></tr><tr><td align="center">nucl-th</td><td align="center">Nuclear Theory</td><td align="center">核理论</td></tr><tr><td align="center">physics.acc-ph</td><td align="center">Accelerator Physics</td><td align="center">加速器物理学</td></tr><tr><td align="center">physics.ao-ph</td><td align="center">Atmospheric and Oceanic Physics</td><td align="center">大气和海洋物理学</td></tr><tr><td align="center">physics.app-ph</td><td align="center">Applied Physics</td><td align="center">应用物理学</td></tr><tr><td align="center">physics.atm-clus</td><td align="center">Atomic and Molecular Clusters</td><td align="center">原子和分子团簇</td></tr><tr><td align="center">physics.atom-ph</td><td align="center">Atomic Physics</td><td align="center">原子物理学</td></tr><tr><td align="center">physics.bio-ph</td><td align="center">Biological Physics</td><td align="center">生物物理学</td></tr><tr><td align="center">physics.chem-ph</td><td align="center">Chemical Physics</td><td align="center">化学物理</td></tr><tr><td align="center">physics.class-ph</td><td align="center">Classical Physics</td><td align="center">经典物理学</td></tr><tr><td align="center">physics.comp-ph</td><td align="center">Computational Physics</td><td align="center">计算物理学</td></tr><tr><td align="center">physics.data-an</td><td align="center">Data Analysis, Statistics and Probability</td><td align="center">数据分析、统计和概率</td></tr><tr><td align="center">physics.ed-ph</td><td align="center">Physics Education</td><td align="center">物理教育</td></tr><tr><td align="center">physics.flu-dyn</td><td align="center">Fluid Dynamics</td><td align="center">流体动力学</td></tr><tr><td align="center">physics.gen-ph</td><td align="center">General Physics</td><td align="center">普通物理</td></tr><tr><td align="center">physics.geo-ph</td><td align="center">Geophysics</td><td align="center">地球物理学</td></tr><tr><td align="center">physics.hist-ph</td><td align="center">History and Philosophy of Physics</td><td align="center">物理学的历史与哲学</td></tr><tr><td align="center">physics.ins-det</td><td align="center">Instrumentation and Detectors</td><td align="center">仪器和探测器</td></tr><tr><td align="center">physics.med-ph</td><td align="center">Medical Physics</td><td align="center">医学物理学</td></tr><tr><td align="center">physics.optics</td><td align="center">Optics</td><td align="center">光学</td></tr><tr><td align="center">physics.plasm-ph</td><td align="center">Plasma Physics</td><td align="center">等离子体物理</td></tr><tr><td align="center">physics.pop-ph</td><td align="center">Popular Physics</td><td align="center">大众物理</td></tr><tr><td align="center">physics.soc-ph</td><td align="center">Physics and Society</td><td align="center">物理学与社会</td></tr><tr><td align="center">physics.space-ph</td><td align="center">Space Physics</td><td align="center">空间物理学</td></tr><tr><td align="center">q-bio.BM</td><td align="center">Biomolecules</td><td align="center">生物分子</td></tr><tr><td align="center">q-bio.CB</td><td align="center">Cell Behavior</td><td align="center">细胞行为</td></tr><tr><td align="center">q-bio.GN</td><td align="center">Genomics</td><td align="center">基因组学</td></tr><tr><td align="center">q-bio.MN</td><td align="center">Molecular Networks</td><td align="center">分子网络</td></tr><tr><td align="center">q-bio.NC</td><td align="center">Neurons and Cognition</td><td align="center">神经元与认知</td></tr><tr><td align="center">q-bio.OT</td><td align="center">Other Quantitative Biology</td><td align="center">其他定量生物学</td></tr><tr><td align="center">q-bio.PE</td><td align="center">Populations and Evolution</td><td align="center">种群与进化</td></tr><tr><td align="center">q-bio.QM</td><td align="center">Quantitative Methods</td><td align="center">定量方法</td></tr><tr><td align="center">q-bio.SC</td><td align="center">Subcellular Processes</td><td align="center">亚细胞突起</td></tr><tr><td align="center">q-bio.TO</td><td align="center">Tissues and Organs</td><td align="center">组织和器官</td></tr><tr><td align="center">q-fin.CP</td><td align="center">Computational Finance</td><td align="center">金融工程</td></tr><tr><td align="center">q-fin.EC</td><td align="center">Economics</td><td align="center">经济学</td></tr><tr><td align="center">q-fin.GN</td><td align="center">General Finance</td><td align="center">财务概述</td></tr><tr><td align="center">q-fin.MF</td><td align="center">Mathematical Finance</td><td align="center">数学金融</td></tr><tr><td align="center">q-fin.PM</td><td align="center">Portfolio Management</td><td align="center">投资组合管理</td></tr><tr><td align="center">q-fin.PR</td><td align="center">Pricing of Securities</td><td align="center">证券定价</td></tr><tr><td align="center">q-fin.RM</td><td align="center">Risk Management</td><td align="center">风险管理</td></tr><tr><td align="center">q-fin.ST</td><td align="center">Statistical Finance</td><td align="center">金融统计</td></tr><tr><td align="center">q-fin.TR</td><td align="center">Trading and Market Microstructure</td><td align="center">交易与市场微观结构</td></tr><tr><td align="center">quant-ph</td><td align="center">Quantum Physics</td><td align="center">量子物理学</td></tr><tr><td align="center">stat.AP</td><td align="center">Applications</td><td align="center">应用</td></tr><tr><td align="center">stat.CO</td><td align="center">Computation</td><td align="center">计算</td></tr><tr><td align="center">stat.ME</td><td align="center">Methodology</td><td align="center">方法论</td></tr><tr><td align="center">stat.ML</td><td align="center">Machine Learning</td><td align="center">机器学习</td></tr><tr><td align="center">stat.OT</td><td align="center">Other Statistics</td><td align="center">其他统计学</td></tr><tr><td align="center">stat.TH</td><td align="center">Statistics Theory</td><td align="center">统计学理论</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 便捷工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Arxiv API </tag>
            
            <tag> 论文查询 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读《Sequence to Sequence Learning with Neural Networks》</title>
      <link href="posts/d1bb6beb.html"/>
      <url>posts/d1bb6beb.html</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本篇论文提出了一种对序列结构做最小假设的端到端序列学习方法。使用一个多层的长短时记忆（LSTM）将输入序列映射到一个固定维数的向量上，然后再使用另一个深度 LSTM 从向量上解码目标序列。这种模型结构称为 Seq2seq 的模型。它可以处理任意长度的序列数据，弥补了 DNN 模型只能处理固定维度的输入输出的缺点。Seq2seq 在机器翻译，语音识别等多个领域发挥着重要作用。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>DNN 模型功能强大，但是只能处理固定输入输出的数据，这限制了它在序列数据中的功能。为了可以处理任意长度的序列问题（如语音识别，机器翻译等），论文提出了一种端到端的神经网络模型 - Seq2seq。</p><p>模型使用两个 LSTM 结构，一个将源序列映射到一个固定维数的向量上，然后使用另一个 LSTM 从该向量中提取输出序列。第二个 LSTM 相当于一个条件语言模型，取决于输入序列。LSTM 可以很好的学习到长序列数据，所以可以满足这种需求。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>Seq2seq 结构的一般形式如下图：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_paper_seq2seq_001.png" alt=""></p><p>RNN 循环神经网络模型是前馈神经网络在序列问题上的一种推广，给入一个输入序列（$x_1,\cdots,x_t$），可以得到对应的输出序列（$y_1,\cdots,y_t$）：</p><center>$\begin{align} h_t &= sigm(W^{hx}x_t + W^{hh}h_{t-1}) \\ y_t &=W^{yh}h_t  \end{align}$</center></br>如果数据是对齐的，那么 RNN 很容易将序列映射到序列，反之如果输入输出不是对齐的，那么就不确定是否可以达到要求。一般的策略是用一个 RNN 将源序列映射到一个向量，然后使用另外一个 RNN 从中提取出目标序列。但是由于 RNN 自身的局限性，很难学习到太长的序列数据，所以使用 LSTM 替换 RNN。<p>LSTM 的目标是估计条件概率 $P(y_1,\cdots,y_T | x_1,\cdots,x_T)$，其中 $x_1,\cdots,x_T$ 是输入序列，$y_1,\cdots,y_T$ 是对应的输出序列。首先，LSTM 计算输入序列（$x_1,\cdots,x_T$） 的条件概率得到最后一个隐藏状态 v，然后使用一个标准的 LSTM-LM 计算输出的条件概率（$y_1,\cdots,y_T$），初始的隐藏状态为 v ：</p><center>$P(y_1,\cdots,y_T|x_1,\cdots,x_T) = \prod^T_{t=1}P(y_t|v,y_1,\cdots,y_{t-1})$</center></br>在这个公式中，概率分布 $P(y_t|v,y_1,\cdots,y_{t-1})$ 使用 softmax 计算。同时需要注意的是，每个序列结尾需要使用 *<EOS>* 结尾，这么做的目的是以便预测输出序列的结尾。<p>在该论文中，作者使用的模型与上述的一般形式有三点不同：</p><ul><li>使用了两种不同的 LSTM 结构，一种用于输入序列，一种用于输出序列。这么做的好处是增加了模型参数数量，但是成本却可以忽略不计。</li><li>作者发现深层次的 LSTM 比浅层次的 LSTM 更有效，所以在实验中使用了 4 层 LSTM。</li><li>并且发现了一个惊人的现象，将输入句子的单词顺序颠倒，极大的提高了性能（个人认为这跟双向 LSTM 的原理是一样的）。</li></ul><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="Dataset-details"><a href="#Dataset-details" class="headerlink" title="Dataset details"></a>Dataset details</h3><p>作者使用从英语到法语的 WMT’14 数据集，模型训练在一个由 3.48 亿个法语单词和 3.04 亿个英语单词组成的 1200 万个句子的子集上。</p><p>在源语言中使用了 16 万个最常用的单词，在目标语言中使用了 8 万个最常用的单词。每一个词汇表以外的单词都被替换成一个特殊的 <em>UNK</em> 标记。</p><h3 id="Decoding-and-Rescoring"><a href="#Decoding-and-Rescoring" class="headerlink" title="Decoding and Rescoring"></a>Decoding and Rescoring</h3><p>模型的目标函数为给定输入句子 S 得到输出的翻译句子 T ，最大化 log 概率：</p><center>${1\over |S|}\sum_{(T,S) \in S}log P(T|S)$</center></br>一但训练完成就可以通过模型找到最佳的翻译：<center>$\hat{T} = argmaxP(T|S)$</center></br>并且使用 Beam search 来寻找最佳的翻译句子。<h3 id="Reversing-the-Source-Sentences"><a href="#Reversing-the-Source-Sentences" class="headerlink" title="Reversing the Source Sentences"></a>Reversing the Source Sentences</h3><p>在实验中发现，将源语句反转（目标语句没有反转）后，LSTM 可以更好的学习。通过这样做，LSTM 的困惑度（perplexity）从 5.8 下降到 4.7，并且 BLEU 分数从 25.9 提高到 30.6。</p><p>虽然这个现象没有很好的解释，但作者们认为这是由于数据集引入了许多短期依赖关系造成的。通常，当把一个源句和一个目标句连接起来时，源句中的每个单词都与目标句中的对应单词相差甚远。因此，这个问题有一个很大的 <em>最小时间延迟</em>。通过倒装原句中的词，使原语中对应词与目标语中对应词的平均距离不变。</p><h3 id="Training-details"><a href="#Training-details" class="headerlink" title="Training details"></a>Training details</h3><p>实验发现 LSTM 模型很容易训练。作者使用了 4 层深度 LSTMs，每层有 1000 个单元，1000 个多维词嵌入，输入词汇量为 16 万个，输出词汇量为 8 万个。因此深层 LSTM 使用 8000 个实数来表示一个句子。我们发现深层的LSTMs明显优于浅层的LSTMs，每一层增加的LSTMs使perplexity减少了近10%，这可能是因为它们的隐藏状态更大。我们在每个输出中使用了一个朴素的 softmax，超过 80,000 个单词。得到的 LSTM 有 384M 的参数，其中 64M 是纯循环连接（编码器LSTM 为 32M，解码器 LSTM 为 32M）。完整的训练详情如下：</p><ul><li><p>初始化 LSTM 的所有参数，使其均匀分布在-0.08和0.08之间。</p></li><li><p>使用了没有动量的随机梯度下降法 SGD，学习速率固定为 0.7。</p></li><li><p>Batch size 为 128 个。</p></li><li><p>虽然 LSTM 没有梯度消失的问题，但是可能会发生梯度爆炸，所以使用范数进行约束。对于每个 batch，计算 $s = \left| g\right|_2$ ，其中 g 是梯度除以 128 ，如果 $s &gt; 5$ ，设置 $g = {5g\over s}$。</p></li><li><p>不同的句子有不同的长度。大多数句子都很短（比如 20-30 个），但也有一些句子很长（比如&gt; 100 个），所以 128个随机选择的训练句子就会有很多短句子和很少的长句子，这样就浪费了大量的计算量。为了解决这个问题，确保在一个小批处理中的所有句子长度大致相同。</p></li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>该论文提出的 Seq2seq 结构，为后续的序列问题处理打下了不错的基础。同时实验中的发现也很有意思，将源语句的单词反转，可以提高不少的性能。</p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 机器翻译 </tag>
            
            <tag> Seq2seq </tag>
            
            <tag> LSTM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224n-lecture8 Machine Translation, Seq2Seq and Attention</title>
      <link href="posts/7f15f578.html"/>
      <url>posts/7f15f578.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>CS224n 深度学习自然语言处理 2019 版 Lecture-8 学习笔记。</p></blockquote><p>本节课的内容是 NLP 的一个重要领域 - 机器翻译（Machine Translation, MT）。同时也介绍了很重要的 Seq2seq 和 Attention。这三点内容在 NLP 中是很核心的，Seq2seq 和 Attention 不光用于机器翻译，在很多 NLP 任务中都可能遇到，同时 Attention 也不光可以用于 NLP ，也可以用于 CV 领域中。</p><h2 id="Pre-Neural-Machine-Translation"><a href="#Pre-Neural-Machine-Translation" class="headerlink" title="Pre-Neural Machine Translation"></a>Pre-Neural Machine Translation</h2><h3 id="Machine-Translation"><a href="#Machine-Translation" class="headerlink" title="Machine Translation"></a>Machine Translation</h3><p>机器翻译（MT）就是将一个句子 x 从一种语言（源语言）通过计算机转换成另外一种语言（目标语言）的句子 y 的任务。</p><p>早期的机器翻译研究是从 1950 年代开始，从俄语翻译到英语，背景原因是由于冷战的需求。主要的核心内容使用的是基于规则的方法进行翻译。</p><h3 id="Statistical-Machine-Translation"><a href="#Statistical-Machine-Translation" class="headerlink" title="Statistical Machine Translation"></a>Statistical Machine Translation</h3><p>到了 1990 - 2010 年代，统计机器翻译（SMT）占领了主导地位。其核心思想是：从数据中学习到概率模型。比如进行从法语到英语的翻译，在给定一个法语句子 x 的同时，找到最好的英语句子 y：</p><center>$argmax_y P(y | x)$</center></br>然后通过贝叶斯规则将目标函数差分成两部分分别去计算：<center>$ = argmax_y P(x|y)P(y)$</center></br>其中前一个概率 $P(x | y)$ 是翻译模型，从平行语料库中学习得到，后一个概率 $P(y)$ 是语言模型，从正常语料库中学得，以便使得到的翻译语句可以接近自然语言。<h3 id="Learning-alignment-for-SMT"><a href="#Learning-alignment-for-SMT" class="headerlink" title="Learning alignment for SMT"></a>Learning alignment for SMT</h3><p>那么如何学习到翻译模型 $P(x|y)$ 呢？首先第一点就是要加大平行语料库。</p><p>进一步分解，实际上需要考虑的是：$P(x,a|y)$，其中 a 是对齐，即法语句子 x 和英语句子 y 之间的单词级对应。</p><p>对齐（Alignment）是翻译语句中特定词语之间的对应关系。</p><blockquote><p>注意：一些词是没有对应关系的。</p></blockquote><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec8_align_001.png" alt=""></p><p>对齐可以是多对一的关系：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec8_align_002.png" alt=""></p><p>也可以是一对多的关系：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec8_align_003.png" alt=""></p><p>还可以是多对多的关系（短语对短语）:</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec8_align_004.png" alt=""></p><p>对于 $P(x,a|y)$ 需要学习多种因素的组合，比如：</p><ul><li>特定词对齐的概率（也取决于单词位置信息）。</li><li>特定词的特定频率的概率（对应词的数量）。</li></ul><h3 id="Decoding-for-SMT"><a href="#Decoding-for-SMT" class="headerlink" title="Decoding for SMT"></a>Decoding for SMT</h3><p>如何计算 argmax ？ 可以列出所有可能的 y 并计算概率，但是这样计算量太昂贵了。所以使用启发式的搜索算法来搜索最佳的翻译语句，丢弃那些概率低的结果。这个过程叫做 decoding。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec8_smt_decoding_001.png" alt=""></p><p>一个 SMT 系统是巨大的研究领域，效果最好的系统是极其复杂的，需要大量人工去研究特征工程，需要大量的人工维护。</p><p>​    </p><h2 id="Neural-Machine-Translation"><a href="#Neural-Machine-Translation" class="headerlink" title="Neural Machine Translation"></a>Neural Machine Translation</h2><h3 id="What-is-Neural-Machine-Translation"><a href="#What-is-Neural-Machine-Translation" class="headerlink" title="What is Neural Machine Translation?"></a>What is Neural Machine Translation?</h3><p>神经网络机器翻译（NMT）是一种使用单一神经网络去做机器翻译的方法。其主要的结构包括两个 RNN ，这种结构称为 Seq2seq。它的结构图如下所示：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec8_nmt_s2s_001.png" alt=""></p><p>Seq2seq 不仅仅用于 MT ，在其他 NLP 领域也发挥这很大的作用：</p><ul><li>文本摘要（长文本 -&gt; 短文本）</li><li>对话系统（上一句 -&gt; 下一句）</li><li>语法分析（输入文本 -&gt; 输出分析序列）</li><li>代码生成（自然语言 -&gt; Python 代码）</li></ul><p>Seq2seq 模型的实质是一个条件语言模型（Conditional Language Model）：</p><ul><li>Language Model：因为 decoder 是在预测目标句子 y 的下一个单词。</li><li>Conditional：因为预测也是在知道源序列 x 的条件下进行的。</li></ul><p>NMT 是直接计算概率 $P(y|x)$：</p><center>$P(y|x) = P(y_1|x)P(y_2|y_1,x)P(y_3|y_1,y_2,x)\cdots P(y_T|y_1,\cdots,y_{T-1},x)$</center></br>关于 Seq2seq 的论文：[论文阅读《Sequence to Sequence Learning with Neural Networks》](https://hiyoungai.com/posts/d1bb6beb.html)<h3 id="Training-a-Neural-Machine-Translation-system"><a href="#Training-a-Neural-Machine-Translation-system" class="headerlink" title="Training a Neural Machine Translation system"></a>Training a Neural Machine Translation system</h3><p>如何训练一个 NMT 系统呢？同样的，还是首先需要一个尽量大的语料库。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec8_nmt_s2s_002.png" alt=""></p><p>从上图看出损失函数的梯度可以一直反向传播到 encoder，模型可以整体优化，所以 Seq2Seq 也被看做是 end2end 模型。</p><h3 id="Greedy-decoding"><a href="#Greedy-decoding" class="headerlink" title="Greedy decoding"></a>Greedy decoding</h3><p>在上文说到，在计算 argmax 时需要用搜索算法，也就是 decoding 的过程，很费算力。在 Seq2seq 的 decoder 中用到的一种算法叫做 Greedy decoding。即每一步均选取概率最大的单词并将其作为下一步的 decoder input。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec8_nmt_s2s_greedy_001.png" alt=""></p><p>但是 Greedy decoding 有个问题是，每一步选取最大概率的词不一定在全局是最优的选择，并且没有办法回退重新选择。</p><p>为了解决这一个问题，主要有两种方法，第一种是穷举搜索，但是这样计算量太大。第二种就是下面要介绍的 Beam Search。</p><h3 id="Beam-search-decoding"><a href="#Beam-search-decoding" class="headerlink" title="Beam search decoding"></a>Beam search decoding</h3><p>Greedy decoding 每一步取最大概率，而穷举的方法是每一步取所有的可能。Beam search decoding 的核心思想就是介于两者之间的，每一个时间步只关注前 k 个最有可能的翻译情况。这样既保证了不会丢失其他概率的翻译，也减少了计算量。一般情况下 k 的取值在 5 - 10 之间。</p><p>对于预测出来的翻译 $y_1,\cdots,y_t$，它的得分为 log 概率：</p><center>$score(y_1,\cdots,y_t) = logP_{LM}(y_1,\cdots,y_t|x)=\sum^t_{i=1}logP_{LM}(y_i|y_1,\cdots,y_{i-1},x)$</center></br>分数总是负的，越高表示预测的翻译越好，寻找高得分的假设，跟踪每一步的top k。虽然 Beam search 不能找到全局最优解，但是相对于 Greedy 和穷举来说效果已经很好了。<h3 id="Beam-search-decoding-example"><a href="#Beam-search-decoding-example" class="headerlink" title="Beam search decoding example"></a>Beam search decoding example</h3><p>Beam search 的主要流程如下：</p><ul><li>计算下一个单词的概率分布。</li><li>取前 k 个单词并计算分数。</li><li>对于每一次的 k 个假设，找出最前面的 k 个单词并计算分数</li><li>在 $k^2$ 的假设中，保留 k 个最高的分值，如 t = 2 时，保留分数最高的 hit 和 was</li></ul><p>在下图中，beam size = k = 2，蓝色的字为分数：$score(y_1,\cdots,y_t)=\sum^t_{i=1}logP_{LM}(y_i|y_1,\cdots,y_{i-1},x)$。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec8_nmt_beam_search_001.png" alt=""></p><p>总的来说，在每一个时间步计算下一个词时，使用上述的分数计算公式计算得分，留下 k 个最高的得分，然后再分别预测下一个词，每个时间步都只留下最高的 k 个选项即可。</p><p>那么什么时候搜索结束呢？在 Greedy decoding 中，通常在模型产生一个 *<END>* token 的时候才终止解码。在 Beam search 中，不同的假设可能会产生 *<END>* token，所以当一个假设产生了 *<END>* token 后就结束该假设，而继续其它假设的搜索。通常情况下，Beam search 直到产生长度为 T 的句子，或者 至少有 n 个候选句子时结束，T 和 n 都是预先设定的超参数。</p><p>解码完成后，就有了一些候选的翻译句子，那么如何从这些候选句子中选择一条作为预测句子呢？在解码完成后，每一个假设得到的句子都会有自己的得分：</p><center>$score(y_1,\cdots,y_t)=logP_{LM}(y_1,\cdots,y_t|x)=\sum^t_{i=1}logP_{LM}(y_i|y_1,\cdots,y_{i-1},x)$</center></br>但是越长的句子对应的得分会变得越低，所以需要对得分进行归一化处理：<center>${1\over t} \sum^t_{i=1}log P_{LM}(y_i|y_1,\cdots,y_{i-1},x)$</center></br>这样就可以在候选翻译句子中选择得分最高的作为预测句子。<h3 id="Advantages-and-Disadvantages-of-NMT"><a href="#Advantages-and-Disadvantages-of-NMT" class="headerlink" title="Advantages and Disadvantages of NMT"></a>Advantages and Disadvantages of NMT</h3><p>相比较传统的 SMT，NMT 有很多优点：</p><ul><li>更好的性能<ul><li>句子更通顺</li><li>更好的利用上下文</li><li>更好的使用短语相似性</li></ul></li><li>一个单一模型可以实现端到端的优化<ul><li>没有单独的子组件</li></ul></li><li>不需要太多的人为影响<ul><li>没有特征工程</li><li>对所有的平行语料使用同一种方法即可</li></ul></li></ul><p>但是相比较 SMT，MNT 也是有缺点的：</p><ul><li><p>可解释性较差</p><ul><li>很难去 Debug</li></ul></li><li><p>很难控制</p><ul><li>不能使用特殊的规则或者方针去进行翻译</li><li>安全问题</li></ul></li></ul><h2 id="How-do-we-evaluate-Machine-Translation"><a href="#How-do-we-evaluate-Machine-Translation" class="headerlink" title="How do we evaluate Machine Translation?"></a>How do we evaluate Machine Translation?</h2><h3 id="BLEU"><a href="#BLEU" class="headerlink" title="BLEU"></a>BLEU</h3><p>BLEU(Bilingual Evaluation Understudy)，是将机器翻译出来的句子与人工翻译的句子进行比较，计算 n-gram 对应出现的概率，并且为了防止一些句子太短，还需要加上惩罚项，得出一个得分。</p><p>虽然 BLEU 很有用，但不是完美的，很多方法翻译出来的句子，虽然正确，但是却有很低的 BLEU 评分。 </p><p>​    </p><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>Attention 注意力是为了解决 Seq2seq 的信息瓶颈问题而诞生的，回顾一下上文的 Seq2seq 结构图：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec8_nmt_attention_001.png" alt=""></p><p>在 encoder 的过程中，需要将输入的所有信息都 encode 到 encoder 的最后一个 hidden state 上，这通常是不现实的，随着源句子的长度增大，hidden state 会记录的信息也就越多，其中包括一些无用的信息。而模型很难去存储更多的信息，所以需要进行取舍，也就是将有用的信息进行保留，这也就是注意力名字的由来。</p><p>Attention 的核心思想是：在解码器的每个时间步中，使用与编码器的直接连接来聚焦于源序列的特定部分。具体讲解请参考 <a href="https://www.jianshu.com/p/4868162a679b">Attention注意力机制介绍</a>。</p><p>​    </p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>NMT 是 NLP 一个重要领域，现如今已经取得了很大的成功，比如Google翻译，百度翻译等。这都归功于 Seq2seq 和 Attention 机制。虽然许多困难仍然存在，比如平行语料库的缺失，未登录词的处理，在较长文本维护上下文（Attention可以缓解该问题）等等。但是目前来看最好的机器翻译系统已经可以媲美人类的翻译。</p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> CS224n </tag>
            
            <tag> 机器翻译 </tag>
            
            <tag> Seq2seq </tag>
            
            <tag> Attention </tag>
            
            <tag> BLEU </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224n-lecture6 Language Models and Recurrent Neural Networks</title>
      <link href="posts/b91e9559.html"/>
      <url>posts/b91e9559.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>CS224n 深度学习自然语言处理 2019 版 Lecture-6 学习笔记。</p></blockquote><p>这节课的主要的内容是语言模型和循环神经网络在语言模型中的应用。语言模型是很多自然语言处理任务的基础，比如机器翻译和自然语言生成相关任务等。而循环神经网络又是 NLP 最主要最基础的神经网络，也是现在一些主流神经网络模型的基本模型。</p><h2 id="Language-Modeling"><a href="#Language-Modeling" class="headerlink" title="Language Modeling"></a>Language Modeling</h2><p>语言模型是一个预测下一个单词的任务，通常情况下是已知一个语言序列，预测下一个单词是什么。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec6_lm_001.png" alt=""></p><p>更正式的说是跟定一组词序列 $x^{(1)},x^{(2)},\cdots,x^{(t)}$，计算下一个词 $x^{(t+1)}$ 的概率分布：</p><center>$P(x^{(t+1)}|x^{(t)},\cdots,x^{(1)})$</center></br>其中词 $x^{(t+1)}$ 可以是词汇表中的任何一个词，最后得到概率最大的一个词就是预测的词。这样的系统叫做语言模型。<p>也可以认为语言模型是把概率分配给文本的一种系统，比如有一段文本 $x^{(1)},\cdots,x^{(T)}$，根据语言模型计算得到该文本的概率为：</p><center>$\begin{align} P(x^{(1)},\cdots,x^{(T)}) &= P(x^{(1)})\times P(x^{(2)}|x^{(1)})\times \cdots \times P(x^{(T)}|,\cdots,x^{(1)}) \\ &= \prod^T_{t=1} P(x^{(t)}|x^{(t-1)},\cdots,x^{(1)})\end{align}$</center></br>这样就可以求得一段文本的概率。<h3 id="Application-of-Language-Modeling"><a href="#Application-of-Language-Modeling" class="headerlink" title="Application of Language Modeling"></a>Application of Language Modeling</h3><p>语言模型的应用很广泛，除了在一些 NLP 任务中（如机器翻译，文本生成等）会使用语言模型外，我们生活中也有很多使用语言模型的例子。</p><p>输入法的文本预测：我们使用的输入法中，每当你打出一个字或者词后，输入法 app 都会提示你下一个词，这就是语言模型的一个应用。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec6_lm_002.png" alt=""></p><p>搜索引擎搜索内容提示：当我们在搜索引擎中输入想要搜索的内容时，会弹出一些向相关的搜索提示。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec6_lm_003.png" alt=""></p><p>除了这些，语言在智能客服等等的领域都有发挥作用。</p><h2 id="N-gram-Language-Models"><a href="#N-gram-Language-Models" class="headerlink" title="N-gram Language Models"></a>N-gram Language Models</h2><p>上面说完了什么是语言模型，那么怎么样才能学习一个语言模型呢？传统的做法就是 N-gram 语言模型。那么问题又来了，什么是 N-gram语言模型呢？</p><p>N-gram 是一个由 n 个连续单词组成的块，它的思想是一个单词出现的概率与它前 n 个出现的词有关。也就是每个词依赖于前 n 个词。下面是一些常见的术语以及示例，可以帮助你更好的理解 N-gram 语言模型：</p><ul><li>Unigrams：一元文法，由一个单词组成的 token，例如： “the”, “students”, “opened”, ”their”。</li><li>Bigrams：二元文法，由连续两个单词组成的 token，例如：“the students”, “students opened”, “opened their”。</li><li>Trigrams：三元文法，由连续三个单词组成的 token，例如：“the students opened”, “students opened their”。</li><li>4-grams：四元文法，由连续四个单词组成的 token，例如：“the students opened their”。</li></ul><p>更高元的 N-grams 计算概率会变得更加复杂，所以一般情况下都使用的是 unigrams，bigrams 和 trigrams。使用这些 N-gram 的统计数据，去预测文本序列的下一个词是什么。</p><p>首先做一个简单的假设，$x^{(t+1)}$ 只依赖前面的 n-1 个词：</p><center>$\begin{align}P(x^{(t+1)}|x^{(t)},\cdots,x^{(1)})&=P(x^{(t+1)}|x^{(t)},\cdots,x^{(t-n+2)}) \\ &= {P(x^{(t+1)},x^{(t)},\cdots,x^{(t-n+2)}) \over P(x^{(t)},\cdots,x^{(t-n+2)})} \end{align}$</center></br>其中第二个等号后面的式子的分子为 n-gram 的概率，分母为 (n-1)-gram 的概率。整个式子为依赖前 n-1 个词的词 $x^{(t+1)}$ 的条件概率。那么如果计算 n-gram 和 (n-1)-gram 的概率呢？<p>可以通过在大型语料库中，统计他们出现的频数来近似的计算：</p><center>$\approx {count(x^{(x+1)},x^{(t)},\cdots,x^{(t-n+2)}) \over count(x^{(t)},\cdots,x{(t-n+2)}}$</center></br>例如使用 4-gram 语言模型：<p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec6_ngram_001.png" alt=""></p><p>其中，“students opened their” 在语料库中出现了 1000 次，“students opened their books ” 出现了 400 次，那么 $P( books | students \ opened \ their) = 0.4$ ；“students opened their exams ” 出现了 100 次，那么 $P( exams | students \ opened \ their) = 0.1$。</p><p>根据之前所说的，这里下一个词是 books 的概率大于 exams ，理应选择 books。但是在文本的上下文中出现过 proctor，这个直接影响了最后的选择，所以 exams 在这里的上下文中应该是比 books 概率更大的。这就是 n-gram 的缺点，下面从几个方面来讨论 n-gram 的缺点。</p><h3 id="Sparsity-Problems"><a href="#Sparsity-Problems" class="headerlink" title="Sparsity Problems"></a>Sparsity Problems</h3><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec6_ngram_002.png" alt=""></p><p>如上图所示，关于 n-gram 的稀疏性问题：</p><ol><li>如果要求的词没有在文本中出现，也就是分子的概率为 0。解决办法是添加一个很小的值给对应的词，这种方法叫做平滑，例如拉普拉斯平滑。这使得词表中的每个单词都至少有很小的概率。</li><li>如果前 n-1 个词没有出现在文本中，也就是分母的概率无法计算。解决办法是使用 “opened their“ 替代，这种方法叫做后退。保证作为条件的分母概率值存在。</li></ol><blockquote><p>注意：提高 n 的值会使稀疏性变得更糟糕，上文已经提到过，n-gram 一般不会超过 5。</p></blockquote><h3 id="Storage-Problems"><a href="#Storage-Problems" class="headerlink" title="Storage Problems"></a>Storage Problems</h3><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec6_ngram_003.png" alt=""></p><p>第二个是存储问题，由于要统计文本在语料库中的计数，增加 n 或增加语料库都会增加模型大小。</p><blockquote><p>n-gram 实践和基于窗口的神经网络模型略过。</p></blockquote><h2 id="Recurrent-Neural-Networks-RNN"><a href="#Recurrent-Neural-Networks-RNN" class="headerlink" title="Recurrent Neural Networks (RNN)"></a>Recurrent Neural Networks (RNN)</h2><p>循环神经网络（RNN）模型是一种神经网络族，它有许多种变体，最核心最基本的思想是重复使用相同的权重矩阵 W。它的基本结构如下图所示：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec6_rnn_001.png" alt=""></p><blockquote><p>本篇文章主要以 cs224n 的课件为主，关于 RNN 的详细讲解后续会有更详细的文章。</p></blockquote><p>从上图的结构中可以看出，它具有三层网络结构，输入层是任意长度的序列，隐藏层是共用一套权重 W ，输出层的内容也是可选择的，根据不同任务可以输出不同长度。同时，由于每个时间步的输出都是结合当前时间步的输入以及上一个时间步的输出，所以它可以学习到文本之前的历史信息。</p><h3 id="A-RNN-Language-Model"><a href="#A-RNN-Language-Model" class="headerlink" title="A RNN Language Model"></a>A RNN Language Model</h3><p>下面来看一下 RNN 模型在 LM 中是如何使用的：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec6_rnn_lm_001.png" alt=""></p><p>根据上图的结构，从下到上的分层来看：</p><ul><li>输入层：词的 one-hot 编码 $x^{(t)} \in R^{|V|}$，然后经过 word embeddings 词嵌入后转为词向量表示 $e^{(t)} = Ex^{(t)}$。</li><li>隐藏层：每个时间步的输出 $h^{(t)} = \sigma (W_hh^{(t-1)} + W_ee^{(t)} + b_1)$，其中 $h^{(0)}$ 为初始化时隐藏层的状态。</li><li>输出层：由于语言模型为预测下一个词，所以在输出层只有一个节点的输出也就是上图中的 $h^{(4)}$。还需要经过一个矩阵变换 $\hat{y}^{(t)} = softmax(Uh^{(t)} + b_2) \in R^{|V|}$也就是说 $h^{(4)}$ 就是我们上文计算的概率值 $\hat{y}^{(4)} = P(x^{(5)}|the \ students \ opened \ their)$。</li></ul><p>RNN 的优点：</p><ul><li>可以处理任意长度的输入序列。</li><li>每个时间步 t 可以使用之前的时间步的历史信息（理论上）。</li><li>对于更长的输入，模型的大小没有提高。</li><li>在每个时间步上应用相同的权重，因此在处理输入时具有对称性。</li></ul><p>RNN 的缺点：</p><ul><li>递归计算速度慢。</li><li>在实践上，很难去利用更多时间步之前的信息。</li></ul><h3 id="Training-a-RNN-Language-Model"><a href="#Training-a-RNN-Language-Model" class="headerlink" title="Training a RNN Language Model"></a>Training a RNN Language Model</h3><p>现在来看一下如何训练一个 RNN 的语言模型。简单点来说，就是需要一个大的包括词序列（$x^{(1)},\cdots,x^{(T)}$）文本语料库，然后喂给 RNN-LM 模型，计算每个时间步的输出也就是每个词的概率分布 $\hat{y}^{(t)}$。然后像一般的预测问题一样，使用交叉熵损失函数，计算模型预测值和真实值之间的误差：</p><center>$J^{(t)}(\theta) = CE(y^{(t)},\hat{y}^{(t)}) = - \sum_{w\in V}y^{(t)}_wlog\hat{y}^{(t)}_w = -log\hat{y}^{(t)}_{x_t+1}$</center></br>然后对于整个训练集使用评价误差：<center>$J(\theta) = {1\over T} \sum^T_{t=1}J^{(t)}(\theta)={1\over T}\sum^T_{t=1}-log\hat{y}^{(t)}_{x_t+1}$</center></br>但是把整个语料库当作一个文本序列计算 loss 和梯度太昂贵了，所以可以把一句话或者一个文档当作文本序列输入到模型中，然后使用随机梯度下降算法进行训练。<blockquote><p>关于反向回归和梯度计算同样在其他文章中讲解。</p></blockquote><h3 id="Generating-text-with-a-RNN-Language-Model"><a href="#Generating-text-with-a-RNN-Language-Model" class="headerlink" title="Generating text with a RNN Language Model"></a>Generating text with a RNN Language Model</h3><p>像传统的 N-gram 模型一样，RNN-LM 也可以进行文本生成，相比 N-gram 更加流畅，语法正确，但总体上仍然很不连贯。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec6_rnn_lm_002.png" alt=""></p><p>有趣的是，使用 RNN-LM 模型可以学习到文本中的风格，比如使用奥巴马的演讲作为语料库，那么生成的文本则具有奥巴马演讲的风格。同样如果使用哈利波特作为语料库，那么生成的文本就像是哈利波特的风格。</p><h2 id="Evaluating-Language-Models"><a href="#Evaluating-Language-Models" class="headerlink" title="Evaluating Language Models"></a>Evaluating Language Models</h2><p>语言模型的标准评价指标叫做困惑度（perplexity）：</p><center>$perplexity = \prod^T_{t=1} ({1\over P_{LM}(x^{(t+1)}|x^{(t)},\cdots,x^{(1)})})^{1\over T}$</center></br>这等同于交叉熵损失函数的指数：<center>$ \begin{align} &= \prod^T_{t=1}({1\over \hat{y}^{(t)}_{x_{t+1}}})^{1\over T} \\  &=exp({1\over T \sum^T_{t=1}}-log\hat{y}^{(t)}_{x_{t+1}}) \\ &= exp(J(\theta))\end{align}$</center></br>困惑度越低，说明生成的语言越接近真实语言，常用于机器翻译和文本生成等 NLP 任务中。具体内容查看另外一篇博客[信息熵总结](https://hiyoungai.com/posts/686d9456.html)。<p>下图是语言模型的困惑度变化：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec6_rnn_lm_pre_001.png" alt=""></p><p>​    </p><h2 id="Why-should-we-care-about-Language-Modeling"><a href="#Why-should-we-care-about-Language-Modeling" class="headerlink" title="Why should we care about Language Modeling?"></a>Why should we care about Language Modeling?</h2><p>语言模型是一项基准测试任务，它帮助我们衡量我们在理解语言方面的进展。而且语言建模是许多NLP任务的子组件，尤其是那些涉及生成文本或估计文本概率的任务：</p><ul><li>预测性打字</li><li>语音识别</li><li>手写识别</li><li>拼写/语法纠正</li><li>作者识别</li><li>机器翻译</li><li>摘要</li><li>对话</li><li>等等</li></ul><p>所以语言模型是 NLP 任务中一个核心应用。</p><blockquote><p>注意 RNN 不等于语言模型，RNN 是一类神经网络，语言模型是 NLP 一个任务/应用。</p></blockquote><h2 id="Application-of-RNN"><a href="#Application-of-RNN" class="headerlink" title="Application of RNN"></a>Application of RNN</h2><p>RNN 除了用于语言模型之外，在 NLP 其他领域也大有用处。</p><h3 id="RNNs-can-be-used-for-tagging"><a href="#RNNs-can-be-used-for-tagging" class="headerlink" title="RNNs can be used for tagging"></a>RNNs can be used for tagging</h3><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec6_rnn_app_001.png" alt=""></p><h3 id="RNNs-can-be-used-for-sentence-classification"><a href="#RNNs-can-be-used-for-sentence-classification" class="headerlink" title="RNNs can be used for sentence classification"></a>RNNs can be used for sentence classification</h3><p>在分类任务中，可以使用 RNN 学习到句子的表示，然后进行分类。下面有两种计算句子向量的方法。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec6_rnn_app_002.png" alt=""></p><p>基本方法，用最后一个时间步的输出作为句子的向量表示。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec6_rnn_app_003.png" alt=""></p><p>第二种方法是将每个时间步的输出取平均或者最大值最为句子的向量表示。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec6_rnn_app_004.png" alt=""></p><h3 id="RNNs-can-be-used-as-an-encoder-module"><a href="#RNNs-can-be-used-as-an-encoder-module" class="headerlink" title="RNNs can be used as an encoder module"></a>RNNs can be used as an encoder module</h3><p>主要用于机器翻译/问答系统等任务中。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec6_rnn_app_006.png" alt=""></p><h3 id="RNN-LMs-can-be-used-to-generate-text"><a href="#RNN-LMs-can-be-used-to-generate-text" class="headerlink" title="RNN-LMs can be used to generate text"></a>RNN-LMs can be used to generate text</h3><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec6_rnn_app_005.png" alt=""></p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>语言模型是一个简单容易实现很有趣的任务，有精力的话会去做一下。RNN 是 NLP 和深度学习中非常重要的基本神经网络模型，它的许多变体在现在广泛使用，比如 LSTM/GRU 等，后续会继续讨论这些重要的变体。</p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> CS224n </tag>
            
            <tag> 语言模型 </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224n-lecture4 Backpropagation and computation graphs</title>
      <link href="posts/cc5eafd6.html"/>
      <url>posts/cc5eafd6.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>CS224n 深度学习自然语言处理 2019 版 Lecture-4 学习笔记。</p></blockquote><p>本节课分为三大部分：矩阵梯度计算，反向传播，神经网络模型的一些组件和超参数。前两部分以后会有专门的博客讲解，本篇主要说一下第三部分。</p><h2 id="Derivative-wrt-a-weight-matrix"><a href="#Derivative-wrt-a-weight-matrix" class="headerlink" title="Derivative wrt a weight matrix"></a>Derivative wrt a weight matrix</h2><h2 id="Computation-Graphs-and-Backpropagation"><a href="#Computation-Graphs-and-Backpropagation" class="headerlink" title="Computation Graphs and Backpropagation"></a>Computation Graphs and Backpropagation</h2><h2 id="We-have-models-with-many-params"><a href="#We-have-models-with-many-params" class="headerlink" title="We have models with many params"></a>We have models with many params</h2><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>实际上一个完整模型的损失函数还包括正则项，它是所有参数 $\theta$ 的正则化。例如使用 $L_2$ 正则：</p><center>$J(\theta) = {1\over N} \sum^N_{i=1}-log({e^{f_{y_i}} \over \sum^C_{c=1} e^{f_e}})+\lambda \sum_k \theta^2_k$</center></br>正则化的目的是防止模型过拟合，也就是在训练集上效果很好，但是在测试集上效果差。把所有参数加和算到损失里，这样可以抑制模型学习的强度。其中 $\lambda$ 是抑制因子，用于调节正则化的强度。<p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec4_reg_001.png" alt=""></p><h3 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h3><p>在深度学习里更准确的说应该叫做矩阵化。传统的机器学习中的训练方法，是通过迭代的方式，一个数据一个数据的传入模型。但是这样速度很慢，可以将数据组合成矩阵，一起喂给模型去训练，使用矩阵运算可以大幅度提高训练速度。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> numpy <span class="token keyword">import</span> randomN <span class="token operator">=</span> <span class="token number">500</span> <span class="token comment" spellcheck="true"># number of windows to classify</span>d <span class="token operator">=</span> <span class="token number">300</span> <span class="token comment" spellcheck="true"># dimensionality of each window</span>C <span class="token operator">=</span> <span class="token number">5</span> <span class="token comment" spellcheck="true"># number of classes</span>W <span class="token operator">=</span> random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>C<span class="token punctuation">,</span>d<span class="token punctuation">)</span>wordvectors_list <span class="token operator">=</span> <span class="token punctuation">[</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>d<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>N<span class="token punctuation">)</span><span class="token punctuation">]</span>wordvectors_one_matrix <span class="token operator">=</span> random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>d<span class="token punctuation">,</span>N<span class="token punctuation">)</span><span class="token operator">%</span>timeit <span class="token punctuation">[</span>W<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>wordvectors_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>N<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token operator">%</span>timeit W<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>wordvectors_one_matrix<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上面的代码对比迭代和矩阵运算的结果：</p><p>1000 loops, best of 3: 639 μs per loop<br>10000 loops, best of 3: 53.8 μs per loop</p><h3 id="非线性"><a href="#非线性" class="headerlink" title="非线性"></a>非线性</h3><p>神经网络单元中会加入非线性激活函数，使得神经网络模型可以进行非线性计算。下面是一些常用的非线性激活函数：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec4_none_linear_001.png" alt=""></p><ul><li>sigmoid，常用于逻辑回归，神经网络中偶尔会在最后一层使用。</li><li>tanh，是一个重新放缩和移动的 sigmoid (两倍陡峭，[-1,1])。</li><li>logistic 和 tanh 仍然被用于特定的用途，但不再是构建深度网络的默认值。</li></ul><p>因为 tanh 计算缓慢，所以提出了 ReLu 以及变体：</p><p>​    <img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec4_none_linear_002.png" alt=""></p><p>非零范围内只有一个斜率，这一位置梯度十分有效的传递给了输入，所以模型非常有效的训练，但是由于 ReLu 会有一定概率使单元失活，所以在零范围内进行了修改。稍后会在一篇博客中全面比较各个非线性激活函数。</p><h3 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h3><p>通常必须将权重初始化为小的随机值，这样才能在激活函数的有效范围内， 即存在梯度可以使其更新。也为了避免对称性妨碍学习。</p><p>一般情况下初始化隐藏层偏差为 0，如果权重为 0 则输出，偏差为最优值。初始化其他所有权重为 Uniform(-r, r)，选择使使数字既不会很大也不会很小的 r。</p><p>学习框架中的 Xavier，方差与 fan-in 前一层维度和 fan-out 后一层维度成反比：</p><center>$Var(W_i) = {2\over n_{in} + n_{out}}$</center></br>### 优化器<p>这是机器学习和深度学习中重要的一部分，通常情况下都使用的是我们熟知的 SGD 随机梯度下降。然而，要得到好的结果通常需要手动调整学习速度。或者在复杂的神经网络中，或者为了安全考虑，可以使用自适应优化器。</p><ul><li>Adagrad</li><li>RMSprop</li><li>Adam : 非常好用的一个优化器，大多数情况下可用。</li><li>SparseAdam</li></ul><p>后续也会单独写一篇博客对比各个优化器。</p><h3 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h3><p>学习率一般有两种使用方法，一种是固定学习率，从 r = 0.001 开始以 10 的次方数量级变化选择一个合适的学习率。</p><ul><li>太大：模型可能会发散或不收敛</li><li>太小：你的模型可能训练不出很好的效果</li></ul><p>另外一种是变化的学习率，通常可以获得更好的效果。</p><ul><li><p>每隔 k 个阶段(epoch)将学习速度减半</p></li><li><p>epoch = 遍历一次数据 (打乱或采样的)</p></li><li><p>通过一个公式：$ lr=lr_0e^{−kt},\ \ for \ \  epoch \ \ t $</p></li><li><p>还有更新奇的方法，比如循环学习率(q.v.)</p></li></ul><p>更高级的优化器仍然使用学习率，但它可能是优化器缩小的初始速度——因此可以从较高的速度开始。</p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> CS224n </tag>
            
            <tag> 反向回归 </tag>
            
            <tag> 矩阵梯度 </tag>
            
            <tag> 超参数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224n-lecture3 Word Window Classification, Neural Networks, and Matrix Calculus</title>
      <link href="posts/b38e2b22.html"/>
      <url>posts/b38e2b22.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>CS224n 深度学习自然语言处理 2019 版 Lecture-3 学习笔记。</p></blockquote><p>本节课主要内容是通过分类和实体命名识别两个任务来认识一下深度学习在 NLP 中的应用，后续又讲解了神经网络以及梯度计算。</p><h2 id="Classification-setup-and-notation"><a href="#Classification-setup-and-notation" class="headerlink" title="Classification setup and notation"></a>Classification setup and notation</h2><p>首先看一下分类任务情况下的一般符号表示，通常会有一个训练数据集，包含的数据一般形式为：</p><center>$\begin{Bmatrix}x_i,y_i\end{Bmatrix}^N_{i=1}$</center></br>其中<ul><li><p>$x_i$ 是输入，例如单词（索引或者是词向量），句子或者文档等。维度一般表示为 d。</p></li><li><p>$y_i$ 是我们需要预测的标签，是类别中的一种，例如感情色彩，命名实体或者购买/卖出的决定等。</p></li><li><p>N 是数据集的大小。</p></li></ul><h3 id="Classification-intuition"><a href="#Classification-intuition" class="headerlink" title="Classification intuition"></a>Classification intuition</h3><p>对于基本的二分类任务，通常最简单的办法是使用 softmax/logistic 回归模型，训练权重 w，找到决策边界将数据集进行正确划分。可视化表示：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec3_class_visa_001.png" alt=""></p><p>使用 softmax 回归的公式为：</p><center>$p(y|x) = {exp(W_y,x)\over \sum^C_{c=1} exp(W_Cx)}$</center></br>### Details of the softmax classifier<p>将上述的预测函数分为两个步骤介绍：</p><ul><li><p>取 W 的第 y 行与 x 相乘：</p><center>$W_y x = \sum^d_{i=1} W_{yi} x_i = f_y$</center></br>计算所有的 $f_c, c\in 1,2,\dots,C$。</li><li><p>使用 softmax 函数得到归一化的概率：</p><center>$p(y|x) = {exp(f_y)\over \sum^C_{c=1}exp(f_c)} = softmax(f_y)$</center></br></li></ul><p>通俗一点的讲，就是将要预测的数据 x 与每一个分类类别所对应的权重 $W_y$ 相乘（有几个类别就有几组权重），然后将相乘的结果送入到 softmax 中计算分类得分，这样一个输入数据 x 就有了 C 组得分，C 是分类的类别个数。而最终在这些得分里，哪组的得分最高，也就是数据 x 属于该类别的概率最高，最后就认为 x 属于该类别。</p><h3 id="Training-with-softmax-and-cross-entropy-loss"><a href="#Training-with-softmax-and-cross-entropy-loss" class="headerlink" title="Training with softmax and cross-entropy loss"></a>Training with softmax and cross-entropy loss</h3><p>对于每个训练样本 $(x,y)$，目标函数是最大化正确分类 y 的概率，等价于该类最小化负对数概率（一般优化算法中都喜欢使用最小化，凸优化理论相关的知识）。</p><center>$-logp(y|x) = -log({exp(f_y)\over\sum^C_{c=1}exp(f_c)})$</center></br>###　What is “cross entropy” loss/error?<p>交叉熵是信息论中的概念，用于衡量两个分布 p 和 q 之间的差异。在这里使用 p 代表真实值，使用 q 代表模型预测值，那么交叉熵损失函数的公式为：</p><center>$H(p,q) = -\sum^C_{c=1} p(c)log q(c)$</center></br>具体交叉熵的介绍可以看我另外一篇博客[信息熵总结](https://hiyoungai.com/posts/686d9456.html)。<h3 id="Classification-over-a-full-dataset"><a href="#Classification-over-a-full-dataset" class="headerlink" title="Classification over a full dataset"></a>Classification over a full dataset</h3><p>交叉熵损失函数在整个数据集上的表达形式：</p><center>$J(\theta) = {1\over N} \sum^N_{i=1} -log({e^{f_{yi}}\over\sum_{c=1}^Ce^{f_c}})$</center></br>其中：<center>$f_y = f_y(x) = W_yx = \sum^d_{j=1}W_{yj}x_j$</center></br>使用矩阵的方式表示 f：<center>$f = Wx$</center></br>### Traditional ML optimization<p>通常基本的机器学习模型参数 $\theta$ 只包含一列 W：</p><center>$\theta = \left[ \begin{matrix} W_{.1} \\ \vdots \\ W_{.d} \end{matrix} \right]$</center></br>所以只需要根据参数 $\theta$ 的梯度来更新决策边界：<center>$\nabla_\theta J(\theta) = \left[ \begin{matrix} \nabla W_{.1} \\ \vdots \\ \nabla W_{.d} \end{matrix} \right] \in R^{Cd}$</center></br>## Neural Network Classifiers<p>一般使用简单传统的 softmax 或者 logistic 回归能进行线性分类，有局限性。而现在的神经网络模型不仅可以学习到线性决策边界还可以学习到非线性的决策边界（依赖于非线性激活单元）。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec3_compare_001.png" alt=""></p><p>上图左边是传统机器学习的分类决策边界，右边是神经网络学习到的分类决策边界，可以看出神经网络模型是非线性的，更好的处理一些不易区分的数据。</p><p>剩下的内容主要是前馈神经网络的介绍，这里不详细介绍了，后续会在深度学习中讲解。</p><h2 id="Named-Entity-Recognition-NER"><a href="#Named-Entity-Recognition-NER" class="headerlink" title="Named Entity Recognition (NER)"></a>Named Entity Recognition (NER)</h2><p>命名实体识别，自然语言处理基本任务之一。该技术的主要工作就是从文本中找出命名实体，比如人名，地址名，组织名等。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec3_ner_example.png" alt=""></p><p>NER 的用途有：</p><ul><li>跟踪文档中提到的特定实体（组织、个人、地点、歌曲名、电影名等）</li><li>对于问题回答，答案通常是命名实体</li><li>许多需要的信息实际上是命名实体之间的关联</li><li>同样的技术可以扩展到其他 slot-filling 槽填充分类</li></ul><p>通常后面是命名实体链接/规范化到知识库，做知识图谱时 NER 是一项不可缺少的技术。</p><h3 id="Named-Entity-Recognition-on-word-sequences"><a href="#Named-Entity-Recognition-on-word-sequences" class="headerlink" title="Named Entity Recognition on word sequences"></a>Named Entity Recognition on word sequences</h3><p>NER 的实现有多种方法，课程中提出了一种可以通过上下文对词进行分类，然后提取词的子序列来预测实体。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec3_ner_example_001.png" alt=""></p><h3 id="Why-might-NER-be-hard"><a href="#Why-might-NER-be-hard" class="headerlink" title="Why might NER be hard?"></a>Why might NER be hard?</h3><p>NER 的难点是有的时候很难区分Named Entity的边界，有的时候很难判断一个词是不是 Named Entity，而且 Named Entity 依赖于上下文，同一个名词可能在某些语境中是机构名，在其他语境中又是人名。</p><h2 id="Binary-word-window-classification"><a href="#Binary-word-window-classification" class="headerlink" title="Binary word window classification"></a>Binary word window classification</h2><p>通常情况下很少对一个词进行分类，鉴于同一个词在不同上下文可能是不同的 Named Entity，一个思路是通过对该词在某一窗口内附近的词来对其进行分类（这里的类别是人名，地点，机构名等等）。对窗口中的单词向量进行平均，并对平均向量进行分类。但是这种方法的缺点是会丢失词的位置信息。</p><h3 id="Window-classification-Softmax"><a href="#Window-classification-Softmax" class="headerlink" title="Window classification: Softmax"></a>Window classification: Softmax</h3><p>为了不丢失位置信息，可以将取平均向量改成将窗口内的各个词的向量串联起来，组成一个大的向量。</p><p>例如对于 museums in Paris are amazing, 我们希望探测到地点名 Paris。假设窗口大小为2，并且通过词向量方法如word2vec 得到窗口内 5 个单词的词向量（前两个 + 后两个 + 中心词），则我们可以将这 5 个向量连在一起得到更大的向量，再对该向量进行分类。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec3_class_softmax_001.png" alt=""></p><p>其中列向量 $x_{window} = x \in R^{5d}$。</p><p>得到了输入向量 $x_{windows}$ 后，就可以使用 softmax 分类器进行分类了，softmax 已经说过很多次就不重复说了：</p><center>$\widehat{y}_y = p(y|x) = {exp(W_y \cdot x) \over \sum ^C_{c=1}exp(W_c \cdot x)}$</center>其中 $\widehat{y}_y$ 是模型的预测值，C 为类别数量。对应的交叉熵损失函数表达式：<center>$J(\theta) = {1\over N} \sum^N_{i=1} -log({e^{f_{yi}}\over \sum^C_{c=1}e^{f_c}})$</center></br>这里说明一下，上面的式子不像交叉熵是因为这里把真实值得概率当作 1，所以这个式子只有一个概率分布。然后通过求导、梯度下降优化算法去更新参数。<h3 id="Binary-classification-with-unnormalized-scores"><a href="#Binary-classification-with-unnormalized-scores" class="headerlink" title="Binary classification with unnormalized scores"></a>Binary classification with unnormalized scores</h3><p>Binary classification with unnormalized scores 是 Collobert &amp; Weston 在（2008, 2011）年提出的一个方法，并且在 2018 年获得了 ICML 2018 Test of time award。</p><p>主要思路就是通过有监督的学习，跟 word2vec 一样，遍历语料库中的所有位置信息，当中心词是一个实际真实的位置实体时，它会得到一个很高的 score。</p><h3 id="Neural-Network-Feed-forward-Computation"><a href="#Neural-Network-Feed-forward-Computation" class="headerlink" title="Neural Network Feed-forward Computation"></a>Neural Network Feed-forward Computation</h3><p>使用一个三层的神经网络来计算窗口的得分：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec3_class_score_nn_001.png" alt=""></p><h3 id="The-max-margin-loss"><a href="#The-max-margin-loss" class="headerlink" title="The max-margin loss"></a>The max-margin loss</h3><p>对于目标函数的一些想法，就是让正确窗口的得分 $s$ 越大，而错误窗口的得分 $s_c$ 越小。则朴素的思路是最大化 $s−s_c$ 或最小化 $s_c−s$。让正确窗口的得分和错误窗口的得分差距最大化。</p><p>$s $　= score(museums in Paris are amazing)<br>$s_c$  = score(Not all museums in Paris)</p><blockquote><p>注意只有命名实体在窗口的中心位置才认为该窗口为正确的。</p></blockquote><p>目标函数为最大间隔目标函数，它通常作为 SVM 的目标函数：</p><center>$J = max(0,1-s+s_c)$</center></br>同样优化算法使用的是随机梯度下降算法，梯度计算时用到的技术叫做反向传播，以后会在深度学习中介绍。同时需要提醒的是，使用矩阵向量计算，会比迭代的方式计算梯度快的多很多。<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>最后剩下时间都是在练习矩阵梯度的计算、雅可比矩阵等其他矩阵知识，重复的内容这里就不再多说，后续会有专门的文章记录一下。</p><p>本节课的内容，对于我个人来说感觉没有特别深入的内容，不像之前讲解 word2vec 的时候。提到的分类和 NER 都是很潜的说一下，更像是引出神经网络一样。看来具体的技术内容还需要去阅读其他论文才可以。</p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> CS224n </tag>
            
            <tag> 分类任务 </tag>
            
            <tag> NER </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性代数概要</title>
      <link href="posts/58c9c51f.html"/>
      <url>posts/58c9c51f.html</url>
      
        <content type="html"><![CDATA[<h2 id="Basic-Concepts-and-Notation"><a href="#Basic-Concepts-and-Notation" class="headerlink" title="Basic Concepts and Notation"></a>Basic Concepts and Notation</h2><p>线性代数提供了一种紧凑表示和操作线性方程式集合的方法。如下面的方程组：</p><center>$\begin {aligned}4x_1 - 5x_2 &= -13 \\\ -2x_1 + 3x_2 &= 9\end {aligned}$</center></br><p>除了传统的解方程组求解之外，还可以使用更简洁的矩阵方法表示：</p><center>$Ax = b$</center></br>其中：<center>$A = \left[ \begin{matrix}4 &-5 \\ -2 &3 \end{matrix} \right], b = \left[\begin{matrix} -13\\ 9 \end{matrix} \right]. $</center></br>从上面的例子可以看出矩阵形式更加简洁。下面来认识一下常用的基本符号：<h3 id="基本符号"><a href="#基本符号" class="headerlink" title="基本符号"></a>基本符号</h3><ul><li><p>使用 $A \in R^{m \times n}$ 表示一个 m 行 n 列的矩阵，其中矩阵 A 中的每个元素都是实数。</p></li><li><p>使用 $x \in R^n$ 表示包含 n 个元素的向量。传统意义上，一个 n 维向量通常认为是一个 n 行 1 列的矩阵，叫做列向量。如果想表示行向量 - 一个 1 行 n 列的矩阵，那么可以写作 $x^T$，叫做 x 的转置。</p><center>$x = \left[ \begin{matrix}x_1\\x_2\\ \vdots \\ x_n \end{matrix} \right]$</center></br></li><li><p>使用 $x_i$ 表示 x 向量的第 i 个元素。</p></li><li><p>使用 $a_{ij}$（或者 $A_{ij},A_{i,j}$）表示矩阵 A 中的第 i 行第 j 列的元素。</p><center>$A = \left[\begin{matrix}a_{11}\ &a_{12}\ &\dots \ &a_{1n} \\ a_{21} \ &a_{22} \ &\dots \ &a_{2n} \\ \vdots \ &\vdots \ &\ddots \ &\vdots \\ a_{m1} \ &a_{m2} \ &\dots &a_{mn} \end{matrix}\right]$</center></br></li></ul><p>需要注意的是，这些符号并不是一成不变的，具体使用还是要在具体的应用当中去抉择。</p><h2 id="Matrix-Multiplication"><a href="#Matrix-Multiplication" class="headerlink" title="Matrix Multiplication"></a>Matrix Multiplication</h2><p>两个矩阵相乘，结果还是一个矩阵。例如矩阵 $A \in R^{m \times n}$ 和矩阵 $B \in R^{n \times p}$相乘：</p><center>$C = AB \in R^{m\times p}$</center></br>其中：<center>$C_{ij} = \sum^n_{k=1} A_{ik}B_{ik}$</center></br>需要注意的是两个矩阵相乘的前提条件：前一个矩阵 A 的列数必须等于后一个矩阵 B 的行数。<p>下面从几个特殊的例子来研究一下矩阵乘法。</p><h3 id="向量与向量"><a href="#向量与向量" class="headerlink" title="向量与向量"></a>向量与向量</h3><p>两个向量 $x,y \in R^n$ 的乘积 $x^Ty$ 也叫作内积、点积或者点乘。符号表示为：</p><center>$x^Ty\in R = \left[ \begin{matrix} x_1\ x_2\ \dots \ x_n \end{matrix}\right] \left[ \begin{matrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{matrix}\right] = \sum^n_{i=1}x_i y_i$</center></br>内积是矩阵乘法的特例，并且它总是符合交换律：$x^Ty = y^Tx$。<p>另外一种两个向量相乘的例子，向量 $x \in R^m$ 和向量 $y \in R^n$，维度大小不是必须一致的。这个两个向量的乘积 $xy^T\in R^{m\times n}$ 称为外积或者叫做叉乘。相乘后得到的矩阵每个元素表示为：$(xy^T)_{ij} = x_i y_j$：</p><center>$xy^T \in R^{m \times n} = \left[\begin{matrix}x_1 \\ x_2 \\ \vdots \\ x_m \end{matrix}\right] \left[\begin{matrix}y_1 \ y_2 \ \dots \ y_n \end{matrix} \right] = \left[\begin{matrix}x_1y_1  &x_1y_2 &\dots  &x_1y_n \\ x_2y_1 &x_2y_2 &\dots &x_2y_n \\ \vdots &\vdots &\ddots &\vdots \\ x_my_1 &x_my_2 &\dots &x_my_n \end{matrix} \right]$</center></br>外积一个有用的例子是，定义一个 n 维且每个元素都是 1 的单位向量 $I\in R^n$ ，并且定义矩阵 $A\in R^{m \times n}$ 每一列都是相同的向量 $x \in R^m$。这样可以用如下式子来表示 A：<center>$A = \left[\begin{matrix}\mid &\mid &\mid \\ x &x &x \\ \mid &\mid &\mid \end{matrix} \right] = \left[\begin{matrix}x_1 &x_1 &\dots &x_1 \\ x_2 &x_2 &\dots &x_2 \\ \vdots & \vdots &\ddots &\vdots \\ x_m &x_m &\dots &x_m \end{matrix} \right] = \left[\begin{matrix}x_1 \\ x_2 \\ \vdots \\x_m \end{matrix} \right]\left[\begin{matrix}1 \ 1 \dots \ 1 \end{matrix} \right] = xI^T$</center></br>### 矩阵与向量<p>矩阵 $A\in R^{m\times n}$ 和向量 $x\in R^n$ 相乘，它们的乘积是 $y = Ax \in R^m$ 。矩阵与向量相乘有几种不同的形式，下面一个个来说：</p><p>首先按行写 A ，然后 Ax 可以表示为：</p><center>$y = Ax = \left[\begin{matrix}— &a_1^T &— \\ — &a_2^T &— \\ &\vdots \\ — &a_m^T &—\end{matrix} \right]x = \left[\begin{matrix}a_1^Tx\\a_2^Tx\\ \vdots \\ a_m^Tx \end{matrix} \right]$</center></br>换句话说，y 的第 i 个元素等于矩阵 A 的第 i 行和 x 的内积 $y_i = a^T_ix$。<p>如果我们按列去写 A，那么公式可以写成如下形式：</p><center>$y = Ax = \left[\begin{matrix}\mid &\mid & &\mid \\ a_1 &a_2 &\dots &a_n \\ \mid &\mid & &\mid \end{matrix} \right]\left[\begin{matrix}x_1 \\ x_2 \\ \vdots \\ x_n \end{matrix} \right] = [a_1]x_1 + [a_2]x_2 + \dots + [a_n]x_n$</center></br>自己可以举个小例子验证一下，按行按列去计算，两种方式的结果是一样的。从上可以看出，y 是 矩阵 A 的列向量的线性组合，线性组合的系数由 x 决定。<p>上面讨论的两种乘法都是向量在右边，下面讨论一下向量在左边的情况。</p><p>首先是按列写矩阵 A：</p><center>$y^T = x^TA = x^T\left[\begin{matrix}\mid &\mid & &\mid \\ a_1 &a_2 &\dots &a_n \\ \mid &\mid & &\mid \end{matrix} \right] = \left[\begin{matrix}x^Ta_1 \ x^Ta_2 \ \dots \ x^Ta_n\end{matrix} \right]$</center></br>这表明 $y^T$ 的第 i 项等于 x 和矩阵 A 的第 i 列的内积。<p>下面看一下按行写矩阵 A：</p><center>$y^T=x^TA=\left[x_1 \ x_2 \ \dots \ x_n \right]\left[\begin{matrix}— &a_1&— \\ — &a^T_2 &— \\ \ &\vdots & \\ — &a_m^T &—\end{matrix} \right]=x_1[—\ a^T_1\ —]+x_2[— \ a_2^T \ —]+\dots + x_n[— \ a_n^T \ —]$</center></br>可以看出 $y^T$ 是矩阵 A 行向量的线性组合，线性组合的系数由 x 决定。<h3 id="矩阵与矩阵"><a href="#矩阵与矩阵" class="headerlink" title="矩阵与矩阵"></a>矩阵与矩阵</h3><p>有了上面的基础，我们就知道矩阵和矩阵相乘 $C = AB$ 有四种不同的表现形式，当然这些都是等价的。</p><p>首先把矩阵乘法看做是向量－向量的集合乘法，那么这样的情况有两种方式，一是矩阵 A 是行向量集合，矩阵 B 是列向量集合；另外一种是矩阵 A 是列向量集合，矩阵 B 是行向量集合。下面是两种表现形式的公式：</p><center>$C = AB = \left[\begin{matrix}— &a_1^T &— \\— &a_2^T &— \\ \ &\vdots & \\ — &a_m^T & — \end{matrix} \right]\left[\begin{matrix}\mid &\mid & &\mid \\ b_1 &b_2 &\dots &b_p \\ \mid &\mid & &\mid  \end{matrix} \right] = \left[\begin{matrix}a_1^Tb_1 &a_1^Tb_2 &\dots &a_1^Tb_p \\ a^T_2b_1 &a_2^T b_2 &\dots &a_2^Tb_p \\ \vdots &\vdots &\ddots  &\vdots \\ a_m^Tb_1 &a_m^Tb_2 &\dots &a_m^Tb_p \end{matrix} \right]$</center></br>从第一种形式可以看出，矩阵 C 的第 i 行第 j 列的元素等于矩阵 A 的第 i 行和矩阵 B 的第 j 列向量的内积。其中 $A\in R^{m\times n},B\in R^{n\times p},a_i\in R^n,b_j\in R^n$，所以内积计算是成立的。<p>第二种形式的表达式：</p><center>$C = AB = \left[\begin{matrix}\mid &\mid & &\mid \\ a_1 &a_2 &\dots &a_n \\ \mid &\mid & &\mid  \end{matrix} \right]\left[\begin{matrix}— &b^T_1 &— \\ — &b_2^T &— \\ \ &\vdots &\\ — &b^T_n &— \end{matrix} \right]=\sum^n_{i=1}a_ib^T_i$</center></br>很明显，矩阵 C 等于 矩阵 A 的行向量和矩阵 B 的列向量外积之和。<p>其次也可以把矩阵和矩阵相乘看做成矩阵和向量之间的乘法：</p><center>$C = AB = A\left[ \begin{matrix}\mid &\mid & &\mid \\ b_1 &b_2 &\dots &b_p \\ \mid &\mid & &\mid  \end{matrix}\right] = \left[\begin{matrix}\mid &\mid & &\mid \\ Ab_1 &Ab_2 &\dots &Ab_p \\ \mid &\mid & &\mid \end{matrix} \right]$</center></br>其中 $c_i = Ab_i$。<center>$C = AB = \left[\begin{matrix}— &a^T_1 &— \\— &a_2^T &— \\ \ &\vdots \ \\ — &a_m^T &—\end{matrix}\right]B = \left[\begin{matrix}— &a^T_1B &— \\— &a_2^TB &— \\ \ &\vdots \ \\ — &a_m^TB &—\end{matrix}\right]$</center></br>其中 $c^T_i = a^T_iB$。<p>矩阵乘法的基本性质：</p><ul><li>结合律：$(AB)C = A(BC)$；</li><li>分配律：$A(B+C) = AB + AC$；</li><li>不可交换：$AB \neq BA$</li></ul><p>总结一下，其实只要记住矩阵乘法存在的前提就是前一个矩阵的列数必须等于后一个矩阵的行数，最后相乘的结果矩阵行列数分别为前一个矩阵的行数和后一个矩阵的列数，记住这一点就可以解决大多数矩阵乘法。</p><h2 id="Operations-and-Properties"><a href="#Operations-and-Properties" class="headerlink" title="Operations and Properties"></a>Operations and Properties</h2><p>这一节主要介绍矩阵和向量的一些运算和性质，记住这些内容可以帮助在矩阵运算时更方便一些。</p><h3 id="单位矩阵和对角矩阵"><a href="#单位矩阵和对角矩阵" class="headerlink" title="单位矩阵和对角矩阵"></a>单位矩阵和对角矩阵</h3><p>单位矩阵（Identity Matrix） $I\in R^{n\times n}$ 是一个方阵（行数和列数相等），它的对角线上元素全是 1，其余位置元素全是 0：</p><center>$I_{ij} = \begin{cases}1 & i=j \\ 0 & i \neq j \end{cases}$</center></br>对于任意矩阵 $A \in R^{m\times n}$ 有：<center>$AI = A = IA$</center></br>需要注意的是，单位矩阵 $I$ 的表示方法是模糊的，因为没有指定维度大小，它是随着矩阵运算的变化而变化。在上面的公式 $AI = A$ 中，$I$ 的维度是 $n \times n$；在公式 $A = IA$ 中，$I$ 的维度是 $m \times m$。<p>对角矩阵（Diagonal Matrices）是一个非对角线上的元素都为 0 的矩阵，通常用 $D = diag(d_1,d_2,\dots,d_n)$ 表示，它的每个元素表达式：</p><center>$D_{ij} = \begin{cases}d_i &i=j \\ 0 &i\neq j \end{cases}$</center></br>由上可以看出单位矩阵 $I = diag(1,1,\dots,1)$ 是特别的对角矩阵。<h3 id="转置"><a href="#转置" class="headerlink" title="转置"></a>转置</h3><p>矩阵的转置（Transpose）是由一个矩阵的行和列翻转得到的。比如一个矩阵 $A \in R^{m\times n}$，它的转置为 $A^T \in R^{n \times m}$。它的每个元素为：</p><center>$A^T_{ij} = A_{ji}$</center></br>其实在上一节内容中就已经提出了转置的概念，列向量的转置就是行向量。<p>下面一些性质可以很容易得到验证：</p><ul><li>$(A^T)^T = A$</li><li>$(AB)^T = B^TA^T$</li><li>$(A+B)^T = A^T + B^T$</li></ul><h3 id="对称矩阵"><a href="#对称矩阵" class="headerlink" title="对称矩阵"></a>对称矩阵</h3><p>对称矩阵（Symmetric Matrices），也就是以对角线为界，右上角的元素和左下角的元素成对称。如果 $A = A^T$ 那么矩阵 $A \in R^{m\times  n}$ 就是对称矩阵。如果 $A = -A^T$ 那么矩阵 A 就是反对称矩阵。</p><p>给定一个矩阵 $A \in R^{n \times n}$，有 $A + A^T$ 是对称矩阵，$A - A^T$ 是反对称矩阵。由此可以得出，任何一个方阵都可以由一个对称矩阵和反对称矩阵的和得到：</p><center>$A = {1\over2}(A+A^T)+{1\over 2}(A-A^T)$</center></br>对称矩阵有很好的性质，在实际应用中经常遇到，所以将所有维度为 n 的对称矩阵用符号 $S^n$ 表示。如果一个矩阵 $A \in S^n$ 那么说明 A 是一个 $n\times n$ 的对称矩阵。<h3 id="矩阵的迹"><a href="#矩阵的迹" class="headerlink" title="矩阵的迹"></a>矩阵的迹</h3><p>一个方阵 $A\in R^{n\times n}$ 的迹记作 tr(A)，它表示矩阵对角线的和：</p><center>$trA = \sum _{i=1}^n A_{ii}$</center></br>迹具有下面这些性质：<ul><li>对于 $A \in R^{n\times n},trA = trA^T$</li><li>对于 $A,B \in R^{n\times n},tr(A+B) = trA + trB$</li><li>对于 $A\in R^{n\times n},t\in R, tr(t\cdot A) = t\cdot trA$</li><li>对于 $A,B$ 如果 $AB$ 是方阵，那么 $trAB = trBA$</li><li>对于 $A,B,C$ 如果 $ABC$ 是方阵，那么 $trABC = trBCA = trCAB$ 等等，这同样适用于更多的矩阵。</li></ul><p>下面来证明一下性质四：</p><center>$\begin{align}trAB &= \sum ^m_{i=1} (AB)_{ii} = \sum^m_{i=1}(\sum ^n_{j=1}A_{ij}B_{ji})\\ &=\sum_{i=1}^m\sum_{j=1}^nA_{ij}B_{ji}=\sum_{j=1}^n\sum_{i=1}^m B_{ji}A_{ij} \\ &=\sum ^n_{j=1}(\sum^m_{i=1}B_{ji}A_{ij})=\sum^n_{j=1}(BA)_{jj} = trBA \end{align}$</center></br>该性质的主要用途是利用可换性来交换公式中的乘项，并利用结合性重新排序乘积的顺序。<h3 id="范式"><a href="#范式" class="headerlink" title="范式"></a>范式</h3><p>一个向量的范数是向量长度的非规范表示，比如通常使用欧几里得（也叫 $L_2$ 范数）：</p><center>$\left\|x\right\|_2 = \sqrt{\sum^n_{i=1}x_i^2}$</center></br>> 注意的是 $\left\|x \right\| ^2_2 = x^Tx$.<p>更规范的说，范数是满足下面四个条件的函数（$f:R^n \rightarrow R$）:</p><ul><li>非负性：所有 $x\in R^n,f(x) \ge 0$</li><li>确定性：有且只有当 $x=0,f(x) = 0$</li><li>齐次性：对于所有 $x\in R^n,t\in R,f(tx)=|t|f(x)$</li><li>三角形公理：对于所有 $x,y\in R^n,f(x+y)\le f(x)+f(y)$</li></ul><p>还有一些其他的范数，如 $L_1$ 范数：</p><center>$\left\|x \right\|_1 = \sum^n_{i=1}|x_i|$</center></br>还有 $L_{\infty}$：<center>$\left\|x \right\|_{\infty} = max_i|x_i|$</center></br>事实上，上面的三个范数都是属于 $l_p$ 范数家族的例子（当 $p \ge 1$ 时）：<center>$\left\|x \right\|_p = (\sum^n_{i=1}|x_i|^p)^{1\over p}$</center></br>此外矩阵的范数也可以定义，比如 Frobenius：<center>$\left\|A \right\|_F = \sqrt{\sum^m_{i=1}\sum^n_{j=1}A^2_{ij}}=\sqrt{tr(A^TA)}$</center></br>此外还有许多其他范数这里不再多说。<h3 id="线性无关与秩"><a href="#线性无关与秩" class="headerlink" title="线性无关与秩"></a>线性无关与秩</h3><p>在一个向量集合 $\lbrace x_1,x_2,\cdots,x_n \rbrace \in R^m$ 里，如果没有向量可以通过其他向量经过线性组合表示，那么就称为这个集合的向量线性无关。相反的，如果一个集合里的向量可以通过其他向量线性组合表示，那么就称为线性相关：</p><center>$x_n = \sum^{n-1}_{i=1} a_ix_i$</center>在上面的式子中，对于有些标量值 $a_1,a_2,\cdots,a_{n-1}\in R$ ，可以说 $x_1,x_2,\cdots,x_n$ 线性相关，反之线性无关。<p>一个矩阵 $A \in R^{m\times n}$ 的列秩是该矩阵中组成线性无关列的最大子集个数。同样的，行秩则是该矩阵中线性无关行的最大子集个数。</p><p>对于任意的矩阵 $A \in R^{m\times n}$ ，它的列秩等于行秩（不做验证）。所以通常统称为矩阵的秩，记作 $rank(A)$。下面来看一些秩的性质：</p><ul><li>对于任意矩阵 $A \in R^{m\times n},rank(A)\leq min(m,n)$，当 $rank(A) = min(m,n)$时，称矩阵 A 为满秩。</li><li>对于任意矩阵 $A \in R^{m\times n}$，$rank(A) = rank(A^T)$.</li><li>对于矩阵 $A \in R^{m\times n},B\in R^{n\times p}，rank(AB)\leq min(rank(A),rank(B))$.</li><li>对于矩阵 $A,B\in R^{m\times n},rank(A+B)\leq rank(A)+rank(B).$</li></ul><h3 id="逆矩阵"><a href="#逆矩阵" class="headerlink" title="逆矩阵"></a>逆矩阵</h3><p>一个方阵 $A\in R^{n\times n}$ 的逆矩阵记作 $A^{-1}$，这个矩阵是唯一的。</p><center>$A^{-1}A = I = AA^{-1}$ </center></br>注意不是所有的矩阵都有逆矩阵，根据定义非方阵没有逆矩阵，同时有些方阵也是没有逆矩阵的。如果矩阵的逆矩阵存在，那么就称为这个矩阵为可逆的或者非奇异的，反之称为不可逆的或者奇异矩阵。<p>一个方阵如果有逆矩阵，那么必须是满秩的。除了满秩之外，下面还有一些其他的可逆性质：</p><ul><li>$(A^{-1})^{-1} = A$</li><li>$(AB)^{-1} = B^{-1}A^{-1}$</li><li>$(A^{-1})^T = (A^T)^{-1}$，这个矩阵通常用 $A^{-T}$ 表示。</li></ul><p>用一个例子来看矩阵的逆矩阵是如何使用的，对于线性方程 $Ax = b$，当 $A \in R^{n \times n}，x,b \in R^{n}$，如果 A 是非奇异的，则有 $x = A^{-1}b$.</p><h3 id="正交矩阵"><a href="#正交矩阵" class="headerlink" title="正交矩阵"></a>正交矩阵</h3><p>如果两个向量 $x,y \in R^n,x^Ty=0$，则这两个向量为正交的（Orthogonal）。如果一个向量 $x\in R^n，\left| x\right|_2 = 1$，则称这些向量是标准化的。如果一个方阵 $U \in R^{n\times n}$ 它的所有列都是相互正交并且是标准化的，那么这个方阵就是正交的。</p><center>$U^TU = I = UU^T$ </center></br>换句话说，正交矩阵的逆矩阵就是它的转置。<blockquote><p>注意的是，如果一个矩阵 $U$ 不是方阵，而是 $U\in R^{m \times n},n&lt;m$，但是它的列向量也符合正交，$U^TU=I$，但是 $UU^T \neq I$，这里只使用正交的来描述它。没看懂、、、</p></blockquote><p>正交矩阵有一个很好的性质是：作用于一个有正交矩阵的向量，不会改变它的欧式范数：</p><center>$\left\| U_x\right\|_2 = \left\| x\right\|_2$</center></br>对于任意的正交 $x\in R^n, U\in R^{n\times n}$ 都适用。<h3 id="矩阵的值域和零空间"><a href="#矩阵的值域和零空间" class="headerlink" title="矩阵的值域和零空间"></a>矩阵的值域和零空间</h3><p>向量集合的范围通常是指能用线性组合表示的所有向量的集合：</p><center>$span(\lbrace x_1,x_2,\cdots,x_n\rbrace) = \lbrace v:v=\sum ^n_{i=1} a_ix_i ,a_i \in R \rbrace$</center></br>可以证明，如果 $\lbrace x_1,\cdots,x_n \rbrace$ 是当 $x_i \in R^n$ 时，n 个线性无关向量的集合，那么 $span(\lbrace x_1,\cdots,x_n\rbrace)=R^n$。换句话说任何向量 $v\in R^n$ 都可以被写成是从 $x_1$ 到 $x_n$ 的线性组合。<p>向量 $y\in R^m$ 在空间 $\lbrace x_1,\cdots,x_n \rbrace,x_i \in R^m$ 的投影记作 $v \in span(\lbrace x_1,\cdots,x_n \rbrace)$。使用欧式范数 $\left| v-y\right|_2$ 去衡量，让 y 尽可能的接近 v。用 $Proj(y;\lbrace x_1,\cdots,x_n \rbrace)$ 表示投影（projection），有如下定义：</p><center>$Proj(y;\lbrace x_1,\cdots,x_n \rbrace) = argmin_{v \in sapn(\lbrace x_1,\cdots, x_n \rbrace)}\left\|y - v \right\|_2$</center></br>矩阵 $A \in R^{m\times n}$ 的值域（也叫列空间）被记作 $R(A)$，是矩阵 A 的列范围（span）。换句话说：<center>$R(A) = \lbrace v\in R^m : v = Ax,x\in R^n\rbrace$</center></br>可以做一些技术性假设（即设 A 是满秩的，并且 n < m），向量 $y\in R^m$ 在矩阵 A 的值域上的投影为：<center>$Proj(y;A) = argmin_{v\in R(A)}\left\|v-y \right\|_2 = A(A^TA)^{-1}A^Ty$</center></br>最后这个公式与最小二乘推导的公式差不多相同，结合投影的定义和最小二乘，可以很轻松的推导出上面的式子。<p>当矩阵 A 只包含一个列向量 $a\in R^m$ 时，给出投影的特例：</p><center>$Proj(y;a) = {aa^T\over a^Ta}y$</center></br>矩阵 $A\in R^{m\times n}$ 的零空间（nullspace）是所有和矩阵 A 相乘后等于 0 的向量集合，记作 $N(A)$：<center>$N(A) = \lbrace x\in R^n : Ax =0 \rbrace$</center></br>注意的是，$R(A)$ 中向量的大小是 m，而 $N(A)$ 中向量的大小是 n ，所以 $R(A^T)$ 和 $N(A)$ 中的向量都属于 $R^n$：<center>$\lbrace w: w = u+v, u\in R(A^T), v\in N(A)\rbrace = R^n \ and \ R(A^T) \and N(A) = {0}$</center></br>换句话说，$R(A^T)$ 和 $N(A)$ 是不相交的子集，它俩的并集跨越了 $R^n$ 。这种集合称为正交补集，记作 $R(A^T) = N(A)^\bot$。<h3 id="行列式"><a href="#行列式" class="headerlink" title="行列式"></a>行列式</h3><p>一个方阵 $A\in R^{n\times n}$ 的行列式（determinant）是一个函数 $det:R^{n\times n} \rightarrow R$，记作 $|A|$ 或者 $det A$。从代数方面来看，虽然可以写出它的显示代数公式，但是很难理解含义。所以从几何的角度来分析行列式的代数性质。</p><p>给定一个矩阵</p><center>$\left[\begin{matrix} — &a^T_1 &— \\ — & a^T_2 &— \\ &\vdots \\ — & a^T_n &— \end{matrix} \right]$</center></br>考虑所有由矩阵 A 中的行向量 $a_1,\cdots,a_n \in R^n$ 的线性组合形成的点集合 $S \subset R^n$，其中线性组合系数在 0 到 1 之间。也就是说：<center>$S = \lbrace v\in R^n:v = \sum^n_{i=1} \alpha_i a_i \ where \ 0 \leq \alpha_i \leq 1 , i=1,\dots,n \rbrace$ </center></br>而 $detA$ 的绝对值是集合 S 的值的度量。例如考虑 $2 \times 2$ 的矩阵：<center>$A = \left[\begin{matrix} 1 &3 \\ 3 &2 \end{matrix}\right]$</center></br>矩阵的行：<center>$a_1 = \left[\begin{matrix} 1 \\ 3  \end{matrix}\right] , a_2 = \left[\begin{matrix} 3 \\ 2 \end{matrix} \right]$</center></br>这些行对应的集合 S 的图形如下图所示：<p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/CDN/img/img_liang_001.png" alt=""></p><p>对于二维矩阵，对应的集合 S 的图形一般是平行四边形。在上述的例子中，行列式 $|A| = －7$，所以平行四边形的面积就等于 7。</p><p>在三维矩阵中，集合 S 对应的图形是一个平行六面体（每个面都是平行四边形），对应的行列式的值是该平行六面体的体积。在更高维度上，称之为 n 维平行四边体。</p><p>从代数的角度看，行列式具有如下性质：</p><ol><li><p>单位矩阵的行列式是 1，$|I| = 1$，通常对应的几何体的体积为 1 。</p></li><li><p>给定一个矩阵 $A \in R^{n \times n}$ ，如果将矩阵的任意一行乘以一个标量 $t\in R$，新矩阵的行列式为 $t|A|$。</p><center>$\left| \left[\begin{matrix}— &ta_1^T &— \\ — &a_2^T & — \\ & \vdots \\ — & a_m^T & — \end{matrix} \right]\right| = t|A|$</center></br></li><li><p>任意交换 A 的两行 $a_i^T, a_j^T$，新矩阵的行列式为 $-|A|$：</p><center>$\left|\left[\begin{matrix}— &a_2^T &— \\ — &a_1^T &— \\ &\vdots \\ — &a_m^T &— \end{matrix} \right] \right|=-|A|$</center></br></li></ol><p>满足上面三个性质的函数是真实存在的，且唯一。下面看一下其他的性质：</p><ul><li>对于 $A \in R^{n \times n},|A| = |A^T|$</li><li>对于 $A,B\in R^{n\times n},|AB|=|A||B|$</li><li>对于 $A \in R^{n\times n}$，如果 A 是奇异的（也就是不可逆的），那么$|A| = 0$.因为如果 A 是奇异的，那么它就不是满秩的，因此它的列是线性相关的。在这种情况下，集合 S 在 n 维空间的图像是一个平面，所以它的体积为 0 。</li><li>对于 $A \in R^{n\times n}$ 和 A 是非奇异的，那么 $|A^{-1}| = 1/|A|$。 </li></ul><p>给出行列式的一般定义之前，先来定义几个符号。$A \in R^{n \times n},A_{\setminus i,\setminus j} \in R^{(n-1)\times (n-1)}$表示从矩阵 A 中删除第 i 行和第 j 列后的矩阵。行列式的一般表达式为：</p><center>$\begin{aligned}|A| &= \sum^n_{i=1}(-1)^{i+j}a_{ij}|A_{\setminus i,\setminus j}|  \ (for \ any \ j \in 1,\cdots , n) \\ & = \sum^n_{j=1}(-1)^{i+j} a_{ij}|A_{\setminus i,\setminus j}| \ (for \ any \ i \in 1,\cdots,n)\end{aligned}$</center></br>在初始化的情况下，$|A|=a_{11} \ for  \ A \in R^{1 \times 1}$，如果把 $A \in R^{n\times n}$展开，就会得到 $n!$ 个不同的项。对于这个结果，很难写出维度大于$3\times 3$ 的项，但是仍然可以学习一下 $3 \times 3$的展开：<center>$\begin{aligned}|[a_{11}]| &= a_{11} \\  \left|\left[\begin{matrix} a_{11} &a_{12} \\ a_{21} & a_{22}\end{matrix}\right]\right| &= a_{11}a_{22} - a_{12}a_{21} \\ \left|\left[ \begin{matrix}a_{11} &a_{12} &a_{13} \\ a_{21}&a_{22} &a_{23} \\ a_{31} &a_{32} &a_{33} \end{matrix}\right]\right| &= a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31}+a_{13}a_{21}a_{32} - a_{11}a_{23}a_{32}-a_{12}a_{21}a_{33}-a_{13}a_{22}a_{31}\end{aligned}$</center></br>矩阵 $A \in R^{n\times n}$ 的伴随矩阵记作 $adj(A)$：<center>$adj(A)\in R^{n\times n}, (adj(A)_{ij}) = (-1)^{i+j}|A_{\setminus j,\setminus i}|$</center></br>> 注意这里的 i 和 j 的顺序。<p>可以证明对于任意非奇异矩阵 $A\in R^{n\times n}$：</p><center>$A^{-1} = {1\over |A|}adj(A)$</center></br>这虽然是一个很明显的矩阵逆的表达式，但是有更好的计算方式去表示矩阵逆。<h3 id="二次型和半正定矩阵"><a href="#二次型和半正定矩阵" class="headerlink" title="二次型和半正定矩阵"></a>二次型和半正定矩阵</h3><p>给定一个方阵 $A \in R^{n \times n}$ 和向量 $x \in R^n$，标量值 $x^TAx$ 被称为二次型，明确的写：</p><center>$x^T A x = \sum_{i=1}^nx_i(Ax)_i  = \sum_{i=1}^n (\sum_{j=1}^n A_{ij}x_j) = \sum_{i=1}^n \sum_{j=1}^n A_{ij} x_ix_j $</center></br>注意<center>$x^TAx = (x^TAx)^T = x^TA^Tx = x^T({1\over 2} A + {1\over 2}A^T)x$</center></br>其中第一个等号是因为标量的转置等于自身，第二个等号源于两个相等的量求平均。由此可以得出结论：只有 $A$ 的对称部分有助于二次型，因此，通常隐含地假设二次型矩阵是对称的。<p>给出了如下定义：</p><ul><li>如果对于所有非零向量有 $x\in R^n, x^TAx &gt; 0$，则称对称矩阵 $A\in S^n$ 是正定的（PD）。通常将其表示为 $A\succ0 $或者 $A&gt;0$，所有的正定矩阵表示为 $S^n_{++}$。</li><li>如果对于所有向量有 $x\in R^n, x^TAx \geq 0$，则称对称矩阵 $A \in S^n$ 是半正定的（PSD）。通常将其表示为 $A \succeq 0$，所有的半正定矩阵表示为 $S^n_{+}$.。</li><li>同样的，对于所有非零向量 $x\in R^n,x^TAx &lt; 0$，则称对称矩阵 $A \in S^m$ 是负定的（ND），通常将其表示为 $A \prec 0$ 或者 $A &lt; 0$。</li><li>相似的，对于所有 $x \in R^m, x^TAx \leq 0$，则称对称矩阵 $A \in S^m$ 是半负定的（NSD），表示为 $A \preceq 0$ 或者. $A \leq 0$。</li><li>最后，如果存在 $x_1,x_2 \in R^n$ 使得 $x_1^TAx_1 &gt; 0,x_2^TAx_2 &lt; 0$，则对称矩阵 $A \in S^m$ 既不是正半定也不是负半定。</li></ul><p>显然，如果 $A$ 是正定的，那么 $-A$ 则是负定的；同样，如果 $A$ 是正半定的，那么. $-A$ 是负半定的。如果 $A$ 是不确定的，那么 $-A$ 也是不确定的。</p><p>正定和负定矩阵一个重要的性质是他们总是满秩的，因此总是可逆的。为了证明这一性质，先假设一些矩阵 $A\in R^{n \times n}$ 不是满秩的。然后假设 $A$ 的第 $j$ 列可以表示为其他 $n-1$ 列的线性组合：</p><center>$a_j = \sum_{i\neq j}x_i a_i$</center></br>其中 $x_1,\cdots,x_{j-1},x_{j+1},\cdots,x_n \in R$，设 $x_j = -1$，则有：<center>$Ax = \sum_{i=1}^n x_ia_i =0$</center></br>但是这意味着一些非零向量 $x$ 有 $x^TAx =0$，因此 $A$ 既不是正定也不是负定。所以如果 $A$ 是正定或者负定，那么必须是满秩的。 <blockquote><p>非满秩 -&gt; 列向量可能线性相关 -&gt; Ax = 0 -&gt; 不是正定也不是负定。</p><p>满秩的方阵  -&gt;  列向量都线性无关  </p></blockquote><p>最后，值得一提的是经常出现的一种正定矩阵类型。给定任意矩阵 $A \in R^{m \times n}$（不是必须对称或者是方阵），矩阵 $G = A^T A$ （通常称为 Gram 矩阵）总是半正定的。此外，如果 $m \geq n$（为了方便起见，假设 $A$ 是满秩的），则 $G = A^T A$ 是正定的。</p><h3 id="特征值和特征向量"><a href="#特征值和特征向量" class="headerlink" title="特征值和特征向量"></a>特征值和特征向量</h3><p>给定一个方阵 $A \in R^{n\times n}$，如果有如下表达式，那么我们说 $\lambda \in C$ 是矩阵 $A$ 的特征值，$x \in C^n$ 是对应的特征向量：</p><center>$Ax = \lambda x,  x\neq 0$</center></br>直观上说，这个定理表示将一个矩阵 $A$ 乘以向量 $x$，会得到一个与 $x$ 方向相同的新的向量，但是缩放比例为 $\lambda$．还要注意，对于任何特征向量 $x \in C^n$ 和标量 $c \in C$，有 $A(cx) = cAx = c\lambda x = \lambda(cx)$，所以 $cx$ 也是特征向量．因此，当我们谈论 $\lambda$ 相关的特征向量时，通常认为特征向量是标准化为长度为 1 的（这仍然会有歧义，因为 $x$ 和 $-x$ 都是特征向量，但是必须这么做）．<p>可以重写上面的等式，来声明 $(\lambda,x)$ 是一对特征值和特征向量：</p><center>$(\lambda I - A)x = 0,x \neq 0$</center></br>但是，只有 $(\lambda I - A)$ 具有非空的零空间时，$(\lambda I -A)x=0$ 才有关于 $x$ 的非零解，这仅在 $(\lambda I -A)$ 是奇数的情况下：<center>$|(\lambda I -A)| = 0$</center></br>我们可以使用前面讲的行列式，将该表达式扩展到 $\lambda$ 的多项式（很大）中，其中 $\lambda$ 的最大次数为 $n$。然后只要找到该多项式的 $n$ 个根 $\lambda_1,\cdots,\lambda_n$，就找了 $n$ 个特征值。为了找到特征值 $\lambda_i$ 对于的特征向量，只需要求解线性方程组 $(\lambda_i I - A)x=0$。<blockquote><p>在实践中，这不是求特征值和特征向量的方法，因为行列式的展开有 $n!$ 项。</p></blockquote><p>下面列举一下特征值和特征向量的性质（下面所有情况，假设 $A\in R^{n\times n}$，特征值 $\lambda_i$ 和特征向量 $x_i$）：</p><ul><li><p>矩阵 $A$ 的迹等于它的特征值的和：</p><center>$tr(A) = \sum_{i=1}^n \lambda_i$</center></br></li><li><p>矩阵 $A$ 的行列式等于它的特征值的乘积：</p><center>$|A| = \prod_{i=1}^n \lambda_i$</center></br></li><li><p>矩阵 $A$ 的秩等于它的非零特征值的数量。</p></li><li><p>如果 $A$ 是非奇异的，那么 ${1\over \lambda_i}$ 是 $A^{-1}$ 的一个特征值，其对应的特征向量为 $x_i$。例如：$A^{-1}x_i = ({1\over \lambda_i})x_i$，为了证明这一点，采用特征向量方程 $Ax_i = \lambda_ix_i$，两边同时乘以 $A^{-1}$。</p></li><li><p>对角矩阵 $D = diag(d_1,\cdots,d_n)$ 的特征值就是对角线上的元素 $d_1,\cdots,d_n$。</p></li></ul><p>我们可以写出所有特征向量方程：</p><center>$AX = X\Lambda$</center></br>其中 $X\in R^{n\times n}$ 的列是 $A$ 的特征向量，$\Lambda$ 是对角矩阵，其中的值是 $A$ 的特征值：<center>$X \in R^{n\times n} = \begin{bmatrix} | & | & & | \\ x_1 & x_2 & \cdots & x_n  \\ | & | & & | \end{bmatrix}, \Lambda = diag(\lambda_1,\cdots , \lambda_n)$</center></br>如果 $A$ 的特征向量是线性独立的，那么矩阵 $X$ 是可逆的，因此 $A = X \Lambda X^{-1}$ 。一个矩阵如果可以写成这种形式，那么称为可以对角化。<h3 id="对称矩阵的特征值和特征向量"><a href="#对称矩阵的特征值和特征向量" class="headerlink" title="对称矩阵的特征值和特征向量"></a>对称矩阵的特征值和特征向量</h3><p>当我们谈论对称矩阵 $A \in S^n$ 的特征值和特征向量时，有两个明显的性质。首先，可以证明 $A$ 的特征值都是真实的。其次，$A$ 的特征向量上正交的，即上面定义的矩阵 $X$ 是正交的（因此，在这种情况下，我们可以将特征向量矩阵使用 $U$ 表示）。因此，我们可以将 $A$ 表示为 $A = U\Lambda U^T$，并且记住的是，正交矩阵的逆只是其转置。</p><p>使用此方法，可以证明矩阵的确定性完全取决于其特征值的符号。假设 $A \in S^n = U \Lambda U^T$，然后：</p><center>$x^TAx = x^TU\Lambda U^Tx = y^T\Lambda y = \sum_{i=1}^n \lambda_i y_i^2$</center></br>s其中 $y=U^Tx$ （并且由于 $U$ 是满秩的，所以任何矢量 $y\in R^n$ 都可以以这种形式表示）。由于 $y_i^2$ 始终为正，因此该表达式的符号完全取决于 $\lambda_i$。如果所有 $\lambda_i > 0$，则矩阵为正定矩阵；如果所有 $\lambda_i \geq 0$，则为半正定矩阵；反之亦然。最后，如果 $A$ 具有正特征值和负特征值，那么它是不确定的。<p>特征值和特征向量频繁出现的应用是最大化矩阵的某些功能．特别是，对于矩阵 $A\in S^n$，请考虑以下最大化问题：</p><center>$max_{x\in R^n} x^TA x,\ subject \ to\  ||x||^2_2 = 1 $</center></br>例如，我们想找到最大化二次型的向量（范数为１）．假设特征值被排序为 $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n $，这样，这个优化问题$x$ 的最优解 $x_1$ 就是 $\lambda_1$ 对应的特征向量．此时，该二次型的最大值是 $\lambda_1$．同样，求最小化问题也是如此．<center>$min_{x\in R^n} x^TA x, \ subject \ to \ ||x||^2_2 =1$</center></br>最优解是 $\lambda_n$ 对应的特征向量 $x_n$，最小值是 $\lambda_n$．可以结合 $A$ 的特征向量-特征值对的形式以及正交矩阵的性质来证明这一点．但是在下一节，可以看到一种使用矩阵计算直接显示它． <h2 id="Matrix-Calculus"><a href="#Matrix-Calculus" class="headerlink" title="Matrix Calculus"></a>Matrix Calculus</h2><p>尽管前面几节的主题通常在线性代数的课程中很常见，但是很少涉及到（并且广泛使用）的一个主题是将微积分扩展到矢量集中．尽管我们使用的所有实际演算都是相对琐碎的，但这种表示法通常会使事情看起来比实际困难得多。 在本节中，我们介绍矩阵演算的一些基本定义，并提供一些示例．</p><h3 id="矩阵梯度"><a href="#矩阵梯度" class="headerlink" title="矩阵梯度"></a>矩阵梯度</h3><p>假设 $f:R^{m\times n}\rightarrow R$ 是一个函数，它将大小为 $m \times n$ 的矩阵作为输入，并返回一个实数值．则 $f$ （相对于 $A \in R^{m\times n}$） 的梯度是偏导数矩阵，定义为：</p><center>$\nabla_A f(A) \in R^{m\times n } = \begin{bmatrix} {\partial f(A)\over \partial A_{11}} & {\partial f(A)\over \partial A_{12}} & \cdots & {\partial f(A) \over \partial A_{1n}} \\ \vdots & \vdots & \vdots & \vdots \\ {\partial f(A) \over \partial A_{m1}} & {\partial f(A) \over \partial A_{m2}} & \cdots & {\partial f(A) \over \partial A_{mn}} \end{bmatrix}$</center></br><p>例如，一个 $m \times n$ 的矩阵：</p><center>$(\nabla_A f(A))_{ij} = {\partial f(A) \over \partial A_{ij}}$</center></br><p>注意的是，$\nabla_A f(A)$ 的大小始终与 $A$ 的大小相同．因此，特别是如果 $A$ 只是一个向量 $x \in R^n$：</p><center>$\nabla_x f(x) = \begin{bmatrix} {\partial f(x) \over \partial x_1} \\ {\partial f(x) \over \partial x_2} \\ \vdots \\ {\partial f(x) \over \partial x_n} \end{bmatrix}$</center></br><p>重要的是要记住，仅当函数是实数值时，即函数返回标量值时，才定义函数的梯度。 例如，我们不能采用相对于 $x$ 的$Ax$，$A\in R^{n\times n}$的梯度，因为此量是矢量值的．</p><blockquote><p>翻译没搞懂．</p></blockquote><p>它直接从偏导数的等效属性得出：</p><ul><li>$\nabla _x (f(x)+g(x)) = \nabla_x f(x) + \nabla_x g(x)$</li><li>$For \ t \in R,\ \nabla_x (tf(x)) = t\nabla_x f(x)$</li></ul><p>原则上，梯度是对多元函数偏导数的自然扩展．但是实际上，由于符号的原因，使用梯度是很困难的．例如：假设 $A \in R^{m \times n}$ 是固定系数的矩阵，并且假设 $b \in R^n$ 是固定系数的向量．令 $f: R^m \rightarrow R$ 定义为是 $f(z) = z^Tz$ 的函数，使得 $\nabla _z f(z) = 2z$ ．但是现在考虑一下表达式：</p><center>$\nabla f(Ax)$</center></br>该表达式如何解释？至少有两种解释方式：<ul><li><p>首先，请记住 $\nabla_z f(z) =2z$，在这里，我们将 $\nabla f(Ax)$ 等价为点 $Ax$ 处的梯度值．因此：</p><p>$\nabla f(Ax) = 2(Ax) = 2Ax \in R^m$</p></li><li><p>第二种，我们将 $f(Ax)$ 当做输入是 $x$ 的函数，更正式的说，让 $g(x) = f(Ax)$ ，然后在这种解释：</p><p>$\nabla f(Ax) = \nabla_x g(x) \in R^n$</p></li></ul><p>在这里，我们可以看到这两种解释的确不同．一种解释结果产生一个 $m$ 维向量，而另一种解释结果产生一个 $n$ 维向量．</p><p>关键是要明确我们要区分的变量．在第一种情况下，我们针对函数 $f$ 的自变量 $z$ 对其进行微分，然后替换自变量 $Ax$．在第二种情况下，我们直接针对 $x$ 区分了复合函数 $g(x)= f(Ax)$ ．我们将第一种情况称为 $\nabla_z f(Ax)$，将第二种情况称为  $\nabla_x f(Ax)$．</p><blockquote><p>保持符号清晰非常重要！！！</p></blockquote><h3 id="海森矩阵"><a href="#海森矩阵" class="headerlink" title="海森矩阵"></a>海森矩阵</h3><p>假设 $f:R^n \rightarrow R$ 是在 $R^n$ 上采用向量并且返回实数的函数，那么关于 $x$ 的 $Hessian$ 矩阵写作 $\nabla_x^2f(x)$ 或者简单的以 $H$ 为偏导数的 $n \times n$ 矩阵：</p><center>$\nabla_x^2 f(x)\in R^{n\times n} = \begin{bmatrix} {\partial^2 f(x) \over \partial x_1^2} & {\partial ^2f(x) \over \partial x_1\partial x_2} & \cdots & {\partial^2 f(x) \over \partial x_1 \partial x_n} \\ \vdots  & \vdots & \ddots & \vdots \\ {\partial^2 f(x) \over \partial x_n \partial x_1} & \partial ^2 f(x) \over \partial x_n \partial x_2  & \cdots  & {\partial^2 f(x)\over \partial x_n^2}\end{bmatrix}$</center></br><p>换句话说，$\nabla_x^2 f(x) \in R^{n \times n}$：</p><center>$(\nabla_x^2 f(x))_{i,j}={\partial^2 f(x) \over \partial x_i \partial x_j}$</center></br><p>需要记住的是 $Hessian$ 总是对称的，因为：</p><center>${\partial^2 f(x)\over \partial x_i \partial x_j}  ={ \partial^2 f(x) \over \partial x_j \partial x_i }$</center></br><p>与梯度相似，只有 $f(x)$ 是真实值时，$Hessian$ 才存在．</p><p>很自然的将梯度视为向量函数的一阶导数，而 $Hessian$ 则是二阶导数．这种直觉是正确的，但是有一些事项需要注意：</p><p>首先，对于一个变量的实值函数  $f:R \rightarrow R$ ，这是一个基本定义，即二阶导数是一阶导数的导数：</p><center>${\partial ^2 f(x)\over \partial x^2 } = {\partial \over \partial x} {\partial \over \partial x} f(x)$</center></br><p>但是，对于向量的函数，函数的梯度是一个向量，因此我们不能取向量的梯度，即：</p><center>$\nabla_x \nabla_x f(x) = \nabla_x \begin{bmatrix} {\partial f(x) \over \partial x_1} \\ {\partial f(x) \over \partial x_2} \\ \vdots  \\ {\partial f(x) \over \partial x_n} \end{bmatrix}$</center></br><p>这种情况是不允许的，因此不能说 $Hessian$ 是梯度的梯度．但是在以下定义上是正确的：可以看一下梯度的第 $i$ 项 $(\nabla _x f(x))_i = {\partial f(x) \over \partial x_i}$，计算关于 $x$ 的梯度：</p><center>$\nabla_x {\partial f(x)\over \partial x_i} = \begin{bmatrix} {\partial ^2 f(x) \over \partial x_i \partial x_1} \\ {\partial^2 f(x) \over \partial x_i \partial x_2} \\ \vdots \\ {\partial ^2 f(x) \over \partial x_i \partial x_n} \end{bmatrix}$</center></br><p>它是 $Hessian$ 的第 $i$ 列（行），因此</p><center>$\nabla_x ^2 f(x) = \begin{bmatrix}\nabla _x (\nabla_xf(x))_1  & \nabla_x(\nabla_x f(x))_2 & \cdots \nabla_x(\nabla_x f(x))_n \end{bmatrix}$</center></br><p>可以说 $\nabla_x^2 f(x) = \nabla_x(\nabla_x f(x))^T$，这就意味着取 $\nabla_x(\nabla_x f(x))^T$ 的每一项的梯度，而不是向量整体的梯度．</p><p>最后，尽管我们可以对矩阵 $A \in R^n$ 取梯度，但是出于此类的目的，我们仅考虑针对向量 $x \in R^n$ 采取 $Hessian$．</p><p>这仅仅是一个方便的问题（事实上，我们所做的任何计算都不需要我们找到关于矩阵的 $Hessian$），因为关于矩阵的 $Hessian$ 必须表示所有偏导数 $\partial^2 f(A)\over \partial A_{ij} \partial A_{kl}$，其表示矩阵非常麻烦．</p><h3 id="二次函数和线性函数的梯度和海森矩阵"><a href="#二次函数和线性函数的梯度和海森矩阵" class="headerlink" title="二次函数和线性函数的梯度和海森矩阵"></a>二次函数和线性函数的梯度和海森矩阵</h3><p>现在，尝试去确定一些简单函数的梯度和 $Hessian$ 矩阵．</p><p>对于 $x\in R^n$ ，令 $f(x) = b^Tx$ 对于某些已知向量 $b \in R^n$．然后有：</p><center>$f(x) = \sum_{i=1}^n b_i x_i$</center></br><p>所以</p><center>${\partial f(x)\over \partial x_k} = {\partial \over \partial x_k} \sum_{i=1}^n b_ix_i = b_k$</center></br><p>从这很容易看出 $\nabla_x b^Tx = b$，这与单变量计算的类似情况进行比较，其中 ${\partial \over (\partial x) ax} = a$</p><p>现在考虑关于 $A \in S^n$ 的二次函数 $f(x)=x^TAx$，记住：</p><center>$f(x) = \sum_{i=1}^n \sum_{j=1}^n A_{ij}x_ix_j$</center></br><p>为了计算偏导数，我们考虑 $x_k$ 和 $x^2_k$ 因子项：</p><center>$\begin{align}{\partial f(x) \over \partial x_k} &= {\partial \over \partial x_k} \sum_{i=1}^n \sum_{j=1}^n A_{ij}x_i x_j  \\ & = {\partial \over \partial x_k} [\sum_{i\neq k}^n \sum_{j \neq k}^n A_{ij}x_ix_j + \sum_{i\neq k}^nA_{ik}x_i x_k + \sum_{j\neq k}^n A_{kj}x_k x_j + A_{kk}x_k^2] \\ & = \sum_{i\neq k} A_{ik}x_i + \sum_{j\neq k} A_{kj}x_j + 2A_{kk}x_k \\ & = \sum_{i=1}^n A_{ik}x_i + \sum_{j=1}^n A_{kj}x_j \\ & = 2 \sum_{i=1}^n A_{ki}x_i \end{align}$ </center></br><p>由于 $A$ 是对称的，所以最后一个等式成立（可以放心假设，因为二次型）．注意的是 $\nabla_xf(x)$ 的第 $k$ 的元素是 $A$ 的第 $k$ 行和 $x$ 的内积．因此 $\nabla_x x^TAx = 2Ax$ ，同样，这让人想起单变量计算的类似情况：${\partial \over \partial x ax^2} = 2ax$．</p><p>最后，来看一下二次函数 $f(x) = x^TAx$（显而易见的，线性函数 $b^Tx$ 的 $Hessian$ 是 0）．</p><center>${\partial ^2 f(x) \over \partial x_k \partial x_l} = {\partial \over \partial x_k}[{\partial f(x)\over \partial x_l}] = {\partial \over \partial x_k} [2\sum _{i=1}^n A_{li}x_i] = 2 A_{lk} = 2A_{kl}$</center></br><p>因此，这很清楚的看出 $\nabla_x^2 x^TAx = 2A$ ，这是完全可以预见的，并且再次类似与单变量的 ${\partial^2 \over (\partial x^2)ax^2 }= 2a$  ．</p><p>回顾一下：</p><ul><li>$\nabla_x b^Tx = b$</li><li>$\nabla_x x^TAx = 2Ax$</li><li>$\nabla_x^2 x^TAx = 2A$</li></ul><h3 id="最小二乘"><a href="#最小二乘" class="headerlink" title="最小二乘"></a>最小二乘</h3><p>使用上一节得出的方程式来推导出最小二乘方程式．假设矩阵 $A \in R^{m\times n}$ (简单起见，设 $A$ 是满秩的)，和向量 $b \in R ^m$，使得 $b \notin R(A)$．在这种情况下，我们无法找到向量 $x \in R^n$，使得 $Ax = b$，所以我们想要找到一个向量 $x$ ，使得 $Ax$ 尽可能的接近 $b$，这是使用了欧几里得范式 $||Ax -b||_2^2$的平方来衡量的．</p><p>利用 $||x||^2_2 = x^Tx$ 有：</p><center>$||Ax - b||^2_2 = (Ax-b)^T (Ax - b) = x^TA^T Ax - 2b^TAx + b^Tb$</center></br><p>取相对于 $x$ 的梯度，利用上一节的知识：</p><center>$\begin{align}\nabla_x (x^TA^TAx - 2b^TAx +b^Tb) &= \nabla_x x^TA^TAx - \nabla_x 2b^TAx +\nabla_x b^Tb \\ & = 2A^TAx - 2b^TA \end{align}$</center></br><p>将最后的式子等于 $0$，并求解 $x$ 的值：</p><center>$x = (A^TA)^{-1}A^Tb$</center></br><h3 id="行列式的梯度"><a href="#行列式的梯度" class="headerlink" title="行列式的梯度"></a>行列式的梯度</h3><p>略</p><h3 id="特征值优化"><a href="#特征值优化" class="headerlink" title="特征值优化"></a>特征值优化</h3><p>拉格朗日，略</p>]]></content>
      
      
      <categories>
          
          <category> 线性代数 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线性代数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读《GloVe: Global Vectors for Word Representation》</title>
      <link href="posts/fb292938.html"/>
      <url>posts/fb292938.html</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本篇论文是 CS224n 课程教师 Manning 大神所在的小组发布的，主要是介绍一种新的词向量表示模型 - GloVe。该模型结合了 Word2vec 框架和共现计数两种方法的优点：上下文窗口和全局矩阵分解，可以学习到全局的词向量表示。主要特点有计算速度快，对于大型和小型语料库都有不错的性能表现。这种模型能在词语类比任务的准确率能够达到 75%，并且在词相似度计算和命名实体识别（named entity recognition）中的表现也能比其他模型要好。下面我们来看一下这篇论文的主要内容吧。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>词向量学习的主要两个模型：</p><ul><li>全局矩阵分解方法，如隐语义分析（LSA）。</li><li>上下文窗口方法，如 Skip-gram 模型。</li></ul><p>目前这两种方法都存在这缺陷，比如 LSA 可以很好的利用全局统计信息，但是在类比任务上没有好的表现。而 Skip-gram 正好相反，在词类比任务中有更好的表现，却无法利用全局统计信息。</p><p>论文作者提出了一种特殊的加权最小二乘模型，在训练全局 word - word 共现矩阵时可以有效的利用全局统计信息。该模型在词类比任务中可以达到 75% 的最优性能指标。并且在相似度任务和命名实体识别（NER）任务中也比其他模型要好。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="矩阵分解"><a href="#矩阵分解" class="headerlink" title="矩阵分解"></a>矩阵分解</h3><p>矩阵分解方法最早可以追溯到 LSA，但是原本是 word - document 共现矩阵，在 GloVe 中作者改为 word - word 共现矩阵。word - word 共现矩阵比起 word - document 矩阵更加稠密，虽然 word - word 中也会有一些稀疏，但后续会有方法进行处理。这种方法的优点是可以学习到全局统计信息，并且通过降维技术，可以将稀疏矩阵压缩到 8 或者 9 个数量级的稠密矩阵。</p><h3 id="基于窗口方法"><a href="#基于窗口方法" class="headerlink" title="基于窗口方法"></a>基于窗口方法</h3><p>这类是以 word2vec 为代表的方法，主要通过滑动窗口学习词上下文关系，可以很好的学习到语义关系，但是不能有效的使用统计信息。</p><h2 id="GloVe-模型"><a href="#GloVe-模型" class="headerlink" title="GloVe 模型"></a>GloVe 模型</h2><p>作者认为词与词之间共现的统计数据是作为词向量的重要依据，因此 Glove 词向量的本质也是意图利用这种共现的次数来构造。首先介绍一些符号：</p><ul><li>$X$：word - word 共现矩阵</li><li>$X_{ij}$：单词 $j$ 在单词 $i$ 的上下文出现的次数</li><li>$X_i=\sum_k X_{ik}$：表示任何单词出现在单词 $i$ 的上下文总次数</li><li>$P_{ij}=P(j|i)=X_{ij}/X_i$：表示单词 $j$ 出现在单词 $i$ 的上下文的概率</li></ul><p>下面通过一个例子来说明一下上面各个符号的具体含义：</p><p>考虑两个在某些方面比较类似的词：$i$ 代表 ice，$j$ 代表 steam。这两个词的关系可以通过研究它们与某个词 $k$ 的共现概率之比来得到。例如，$k$ 是某个和 ice 相关但是和 steam 无关的词，比如 k = solid，那么 $P_{ik}/P_{jk}$ 将会很大；而当 $k$ 和steam 相关但是和 ice 无关时，比如 k = gas，这个比值将会很小。还有 $k$ 和两个词都相关（k=water）或者和两个词都不相关（k=fashion），这个比值将接近于1。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_paper_glove_example_001.png" alt=""></p><p>上面的例子表明，词向量的学习应该从共现概率的比值开始而不是概率本身。由于 $P_{ik}/P_{jk}$ 依赖于三个单词 $i,j$ 和 $k$，因此模型的一般形式如下：</p><center>$F(w_i,w_j,\widetilde{w}_k) = {P_{ik}\over P_{jk}}$</center></br>其中 $w∈R^d$ 表示词向量，$\widetilde{w}∈R^d$ 表示上下文词向量。<p>我们希望 F 能在向量空间中编码 $P_{ik}/P_{jk}$这个信息。由于向量空间是线性的，最自然的方法就是对向量做差。因此 F 变成如下形式：</p><center>$F((w_i - w_j),\widetilde{w}_k) = {P_{ik} \over P_{jk}}$</center></br>由于公式右边是一个实数，而左边的参数是向量，尽管 F 可以代表像神经网络一样的复杂结构，但这样的结构会打乱我们希望获得的线性结构，因此为了避免这种情况，首先对参数做点积：<center>$F((w_i - w_j)^T \widetilde{w}_k) = {P_{ik} \over P_{jk}}$</center></br>我们注意到 $w_i$ 到 $w_j$ 的距离与 $w_j$ 到 $w_i$ 的距离是相等的，并且共现矩阵是一个对称的矩阵，即 $X^T==X$，我们把 $w_i$ 称为主单词，$\widetilde{w}_k$ 称为 $w_i$ 的上下文的某个单词，从某种角度看，$w_i$ 与 $\widetilde{w}_k$ 的角色是可以互换的，它们的地位是相等的，那么我们就希望模型 F 能隐含这种特性。再看看上式：<center>$F((w_i - w_j)^T \widetilde{w}_k) = {P_{ik} \over P_{jk}} \neq {P_{ki} \over P_{kj}}$ </center></br>于是作者在外面嵌套了一层指数运算（将差形式转换为商形式），因此：<center>$F((w_i - w_j)^T \widetilde{w}_k) = {P_{ik} \over P_{jk}} = {F(w_i^T \widetilde{w}_k) \over F(w_j^T \widetilde{w}_k)}$ </center></br>从而如下表达式成立：<center>$F(w^T_i \widetilde{w}_k) = P_{ik} = {X_{ik} \over X_i}$</center></br>可以从上述表达式看出 $F=exp$，进而在上述式子两边加上 $log$：<center>$w_i^T \widetilde{w}_k = log(P_{ik}) = log(X_{ik}) - log(X_i)$</center></br>这个时候仔细观察上式，会发现一个对称性的问题，即：<center>$w_i^T \widetilde{w}^k = w_k^T \widetilde{w}_i$</center></br>但是右边的式子交换并不相等，而此时我们也发现 $log(X_i)$ 也独立于 k，因此我们将其吸纳进的 $w_i$ 偏置项 $b_i$，然后同时引入 $\widetilde{w}_k$ 的偏置项 $\widetilde{b}_k$，最终得到：<center>$w_i^T \widetilde{w}_k+b_i+\widetilde{b}_i = log(X_{ik})$</center></br>作者认为这样的处理存在一个弊端，即对于一个词，他的每一个共现词都享有相同的权重来决定该词的词向量，而这在常理上的理解是不合理的，因此，作者引入了一种带权的最小二乘法来解决这种问题，最终的损失函数就为：<center>$J = \sum_{i,j=1}^V f(X_{ij})(w_i^T\widetilde{w}_j+b_i+\widetilde{b}_j - log(X_{ij}))^2$</center></br>其中，权重方程 $f(X_{ij})$ 应该具有一下特点：<ul><li>f(0) = 0</li><li>f(x) 是增函数，这样低频词不会被 over weight。</li><li>当 x 很大时，f(x) 相对小一些，这样高频词也不会被 over weight。</li></ul><p>定义与图像如下：</p><center>$f(x) =\begin{cases}(x/x_{max})^\alpha \qquad & x \lt x_{max} \\1 \qquad & otherwise\end{cases}$</center></br><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_paper_glove_f_visa_001.png" alt=""></p><p>作者经过实验得出，$\alpha = 0.75$ 能得到最好的模型效果。接下来的论文部分还讨论了 Glove 词向量与其他词向量的关系以及复杂度，这里不继续展开说明，后续会在另一篇各种词向量模型比较中详细说明，有兴趣的读者可以仔细阅读一下原论文。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h3><p>将 GloVe 模型得到的词向量分别用于 Word analogies, Word similarity, Named entity recognition，在相同的数据集上和CBOW,SVD 等方法进行比较：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_paper_glove_compare_001.png" alt=""></p><h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><ul><li>语料库中的词汇都符号化和并变为小写，建立一个含有400,000个常用词的词汇表。</li><li>利用上下文窗口来计数得到共现矩阵 X。在利用上下文窗口时需要设定窗口的大小（论文采用了上下文各10个单词的窗口长度）和是否需要区分上文和下文等。</li><li>乘以一个随距离 d 递减的权重项，即与单词 i 距离为 d 的单词在计数时要乘上权重 1/d，表示距离越远的词可能相关性越小。</li><li>采用 AdaGrad 的方法迭代 50 次（非监督学习，没有用神经网络）。</li><li>模型产生两个词向量集 $W$ 和 $ \widetilde{W}$，如果 X 是对称矩阵，则 $W$ 和 $\widetilde{W}$ 在除了初始化的不同以外其他部分应该相同，二者表现应该接近。将$W$ 和 $ \widetilde{W}$的加和作为最终词向量，并且能够使得这些词向量在某些任务上的表现变好。 </li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>虽然论文中 GloVe 有着指标上的领先，但在实际使用中 word2vec 的使用率相对来说更多一些，可能的原因是 word2vec 可以更快地提供一个相对来说不错的 word embedding 层的初始值。毕竟词向量的主要作用还是为下游任务提供良好的词表示，所以在实际应用过程中，哪个词向量模型在对应的任务中效果好我们就使用哪个模型。</p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> GloVe </tag>
            
            <tag> 全局词向量 </tag>
            
            <tag> 共现矩阵 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224n-lecture2 Word Vectors and Word Senses</title>
      <link href="posts/ad770643.html"/>
      <url>posts/ad770643.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>CS224n 深度学习自然语言处理 2019 版 Lecture-2 学习笔记。 </p></blockquote><p>在这节课程中，首先回顾了上一节的内容：word2vec，以及两个主要模型 Skip-gram 和 CBOW。还有就是一些技术优化，如负采样等。我们知道，word2vec 的主要思想是通过词的上下文窗口去进行训练，是一种预测模型通过中词预测上下文，或者通过上下文预测中心词。那为什么不可以直接通过共现计数来统计共现词呢？这就是本节课的主要内容，一部分讲了基于共现矩阵的模型，最后又提出了一个基于两者的新模型 - GloVe.</p><h2 id="Why-not-capture-co-occurrence-counts-directly"><a href="#Why-not-capture-co-occurrence-counts-directly" class="headerlink" title="Why not capture co-occurrence counts directly?"></a>Why not capture co-occurrence counts directly?</h2><h3 id="Co-occurrence-Matrix"><a href="#Co-occurrence-Matrix" class="headerlink" title="Co-occurrence Matrix"></a>Co-occurrence Matrix</h3><p>在 NLP 中构建共现矩阵有两种方法：</p><ul><li>Windows：与 Word2vec 类似，通过指定一个窗口，每个单词周围 window 内出现的单词都认为是共现的，对应计数增加，可以捕捉到位置（POS）信息和语义（semantic）信息。</li><li>Word-document：我们假设在同一篇文章中出现的单词关联性更大。假设单词 $i$ 出现在文章 $j$ 中，则矩阵元素 $X_{i j}$ 计数加一，当处理完所有文档后，就会得到一个 $|V| \times M$ 的矩阵。其中 $|V|$ 为词汇表大小，M 为文档数量。这一构建共现矩阵的方法也是 Latent Semantic Analysis (LSA) 浅语义分析中使用的经典方法。</li></ul><p>下面我们来举例说明一下 windows 方法，假设我们的数据包含以下几个句子：</p><ol><li><p>I like deep learning.</p></li><li><p>I like NLP.</p></li><li><p>I enjoy flying。</p></li></ol><p>则我们可以得到如下的word-word co-occurrence matrix：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec2_window_matrix_001.png" alt=""></p><p>在这里设置的 window 为 1，所以只取中心词周围一个词的距离计数。比如单词 “I” 与 “like” 在上面的三句话中在 window 为 1的距离内共同出现了 2 次，所以矩阵中对应位置是 2。统计完所有的词后就得到了一个 co-occurrence matrix。通过共现矩阵的共现计数来衡量两个单词之间的相关性。</p><blockquote><p>一般情况下 window 取 5 ~ 10。而且从上图可以看出，矩阵是对称的，与左右上下文无关。</p></blockquote><p>共现矩阵的缺点也很明显，随着词汇量增加，矩阵也在不断地变大，而且变得更稀疏，需要更多的存储空间。后续的分类模型也会由于矩阵的稀疏性而存在稀疏性问题，使得效果不佳。我们需要对这一矩阵进行降维，获得像 word2vec 那样的低维 (25-1000) 的稠密向量。</p><h3 id="SVD-奇异值分解"><a href="#SVD-奇异值分解" class="headerlink" title="SVD 奇异值分解"></a>SVD 奇异值分解</h3><p>奇异值分解 SVD (Single Value Decomposition) 就是一种常用的降维模型。通过 SVD 可以将共现矩阵 X 分解成 $UΣV^⊤$ 的形式，其中 $Σ$ 是对角线矩阵，对角线上的值是矩阵的奇异值。$U$ 和 $V$ 是对应行和列的正交基。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec2_svd_001.png" alt=""></p><p>为了减少维度同时尽量保存有效信息，可保留对角矩阵的最大 k 个值，并将矩阵 $U$,$V$ 的相应的行列保留。这是经典的线性代数算法，对于大型矩阵而言，计算代价比较高。</p><p>使用代码演示一下上面例子：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> npla <span class="token operator">=</span> np<span class="token punctuation">.</span>linalgwords <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"I"</span><span class="token punctuation">,</span> <span class="token string">"like"</span><span class="token punctuation">,</span> <span class="token string">"enjoy"</span><span class="token punctuation">,</span> <span class="token string">"deep"</span><span class="token punctuation">,</span> <span class="token string">"learning"</span><span class="token punctuation">,</span> <span class="token string">"NLP"</span><span class="token punctuation">,</span> <span class="token string">"flying"</span><span class="token punctuation">,</span> <span class="token string">"."</span><span class="token punctuation">]</span>X <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>              <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>              <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>              <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>              <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>              <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>              <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>              <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>U<span class="token punctuation">,</span> s<span class="token punctuation">,</span> Vh <span class="token operator">=</span> la<span class="token punctuation">.</span>svd<span class="token punctuation">(</span>X<span class="token punctuation">,</span> full_matrices<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以通过下图看一下，降维到 2 个维度的词向量：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec2_svd_002.png" alt=""></p><h3 id="Hacks-to-X-several-used-in-Rohde-et-al-2005"><a href="#Hacks-to-X-several-used-in-Rohde-et-al-2005" class="headerlink" title="Hacks to X (several used in Rohde et al. 2005)"></a>Hacks to X (several used in Rohde et al. 2005)</h3><p>不管是什么模型，一些高频词对模型的结果影响很大，比如一些虚词（the, he, she）等等。一般有如下几个方法对高频词进行处理：</p><ul><li>使用 log 进行缩放</li><li>$min(X, t), t\approx 100$</li><li>直接舍去</li><li>在基于window的计数中，提高更加接近的单词的计数</li><li>使用 Person 相关系数替代共现计数，如果值为复数则用 0 代替。</li></ul><p>在论文《An Improved Model of Semantic Similarity Based on Lexical Co-Occurrence Rohde et al. ms., 2005》中的COALS模型，通过改善计数，取得了不错的效果：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec2_hacktoX_001.png" alt=""></p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec2_hacktoX_002.png" alt=""></p><p>在向量中出现的有趣的句法模式：语义向量基本上是线性组件，虽然有一些摆动，但是基本是存在动词和动词实施者的方向。</p><h3 id="计数模型和-Word2vec-对比"><a href="#计数模型和-Word2vec-对比" class="headerlink" title="计数模型和 Word2vec 对比"></a>计数模型和 Word2vec 对比</h3><p>基于计数（LSA，HAL等模型）：使用整个矩阵的全局统计数据来直接估计。</p><ul><li>优点<ul><li>训练快速</li><li>统计数据高效利用</li></ul></li><li>缺点<ul><li>主要用于捕捉单词相似性</li><li>对大量数据给予比例失调的重视</li></ul></li></ul><p>直接预测：定义概率分布并试图预测单词</p><ul><li>优点<ul><li>提高其他任务的性能</li><li>能捕获除了单词相似性以外的复杂的模式</li></ul></li><li>缺点<ul><li>与语料库大小有关的量表</li><li>统计数据的低效使用</li></ul></li></ul><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec2_compare_001.png" alt=""></p><h2 id="GloVe-模型"><a href="#GloVe-模型" class="headerlink" title="GloVe 模型"></a>GloVe 模型</h2><p>将两个流派的想法结合起来，在神经网络中使用计数矩阵 - GloVe (Global Vectors的缩写)。表示可以有效的利用全局的统计信息。那么如何利用 word-word co-occurrence count 并能学习到词语背后的含义呢？</p><p>首先我们在上文说到的共现矩阵符号基础上加入几个符号，$X_i = \sum <em>k X</em>{i k}$ 代表所有出现在单词 $i$ 的上下文中的单词次数，用$P_{i j} = P{j|i} = X_{i j} / X_i$ 来表示单词 $j$ 出现在单词 $i$ 上下文中的概率。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec2_glove_001.png" alt=""> </p><blockquote><p>重点不是单一的概率大小，重点是他们之间的比值，其中蕴含着meaning component。</p></blockquote><p>例如我们想区分热力学上两种不同状态ice冰与蒸汽 steam，它们之间的关系可通过与不同的单词 x 的 co-occurrence probability 的比值来描述，例如对于 solid 固态，虽然 $P(solid | icce)$ 与 $P(solid|steam)$ 本身很小，没有什么有效信息。但是他们的比值 $P(solid|ice) \over P(solid|steam)$ 却较大，因为solid更常用来描述 ice​ 的状态而不是 ​steam​ 的状态，所以在 ice 的上下文中出现几率较大。</p><p>对于 gas 则恰恰相反，而对于 water 这种描述 ice 与 steam 均可或者 fashion 这种与两者都没什么联系的单词，则比值接近于1。所以相较于单纯的 co-occurrence probability，实际上 co-occurrence probability 的相对比值更有意义。</p><p>那么问题来了，如何在词向量空间中以线性 meaning component 的形式捕获共现概率的比值？</p><ul><li>Log-bilinear model：$w_i \cdot w_j = logP(i|j)$</li><li>Vector differences：$w_x \cdot (w_a - w_b) = log{P(x|a)\over P(x|b)}$</li></ul><p>基于这些直接给出了 GloVe 的损失函数：</p><center>$J = \sum_{i,j=1}^V f(X_{ij})(w_i^T \widetilde{w}_j+b_i+\widetilde{b}_j -logX_{ij})^2$</center></br>GloVe 模型的优点有：<ul><li>训练快速</li><li>可扩展到大型语料库</li><li>即使使用小的语料库和小的向量，也能获得良好的性能</li></ul><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec2_glove_002.png" alt=""></p><h2 id="How-to-evaluate-word-vectors"><a href="#How-to-evaluate-word-vectors" class="headerlink" title="How to evaluate word vectors?"></a>How to evaluate word vectors?</h2><p>和一般的 NLP 的评估一样，非为内在和外在：</p><ul><li>内在<ul><li>在特定子任务中的表现</li><li>计算速度快</li><li>有助于理解系统</li><li>不清楚是否真的有帮助，除非与真正的任务建立相关性</li></ul></li><li>外在<ul><li>对实际任务的评估</li><li>计算精确度可能需要很长时间</li><li>不清楚子系统是问题所在，是交互问题，还是其他子系统</li><li>如果用另一个子系统替换一个子系统可以提高精确度</li></ul></li></ul><h3 id="Intrinsic-word-vector-evaluation"><a href="#Intrinsic-word-vector-evaluation" class="headerlink" title="Intrinsic word vector evaluation"></a>Intrinsic word vector evaluation</h3><p>可以通过类比的形式评估词向量，比如 man 和 woman 之间的关系是男女性别的差异，那么 king 和什么词也有这种关系呢？下图表示了这种类比评估的方法：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec2_glove_eval.png" alt=""></p><p>整体思想在第一节课中已经提到过，并且还有用于测试的测试集合，这里不多说了。</p><h3 id="Glove-Visualizations"><a href="#Glove-Visualizations" class="headerlink" title="Glove Visualizations"></a>Glove Visualizations</h3><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec2_glove_vs_001.png" alt=""></p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec2_glove_vs_002.png" alt=""></p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec2_glove_vs_003.png" alt=""></p><p>从图中可以看出，具有相同类比含义的几组词都是平行的。</p><h3 id="Analogy-evaluation-and-hyperparameters"><a href="#Analogy-evaluation-and-hyperparameters" class="headerlink" title="Analogy evaluation and hyperparameters"></a>Analogy evaluation and hyperparameters</h3><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec2_glove_compare_001.png" alt=""></p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec2_glove_compare_002.png" alt=""></p><p>从上面两个对比数据可以看出：</p><ul><li>300 是一个很好的词向量维度</li><li>不对称上下文（只使用单侧的单词）不是很好，但是这在下游任务重可能不同</li><li>window size 设为 8 对 Glove 来说比较好</li></ul><p>与 Skip-gram + Neg 对比，可以看出训练时间越长效果越好：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_css224n_19_lec2_glove_compare_003.png" alt=""></p><p>从下面这组对比数据可知，数据集越大越好，并且维基百科数据集比新闻文本数据集要好。这是因为 Wiki 百科是解释性文本语料库，里面包含了文本本身的含义与相关语句。而新闻类的文本只是在胡说八道（@_@ 哈哈教授原话！）。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec2_glove_compare_004.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总的来说，各种词向量模型各有好坏，每篇论文选取的数据肯定都是对自己的模型有利的，所以不能只靠论文中的数据就去一味选取模型，自己使用的时候要根据实际任务以及各个模型的优缺点去选取模型。</p><h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><p><a href="https://hiyoungai.com/posts/fb292938.html">论文阅读《GloVe: Global Vectors for Word Representation》</a></p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> CS224n </tag>
            
            <tag> 词向量 </tag>
            
            <tag> GloVe </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读《Distributed Representations of Words and Phrases and their Compositionality》</title>
      <link href="posts/28911bbb.html"/>
      <url>posts/28911bbb.html</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Tomas Mikolov 大神的另一篇论文，本篇讲解 skip-gram 模型以及优化和扩展。主要包括层次 Softmax，负采样等内容。还有的就是使用 skip-gram 模型学习短语的表示。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>最近提出的 skip-gram 模型是一种高效的学习高质量分布式向量表示的方法，可以捕获到大量精确的词之间的关系。在本篇论文中，提出了几个提高质量和训练速度的方法。词表示的一个缺陷是对词序不受影响（基于词袋模型，所以不考虑词序），所以有些短语无法得到正确的表示。为了解决这个问题，作者想出来一个办法，可以使模型正确的学习到数百万短语的词向量。</p><p>Skip-gram 模型的训练不涉及密集的矩阵乘法，这使得训练非常快速，大概一天内可以训练超过 1000 亿个单词。经过良好学习后的词向量甚至可以进行线性运算，比如：</p><center>$\vec{Madrid} - \vec{Spain} + \vec{France} = \vec{Paris}$</center></br>注意，这里的等于指代的是运算后得到的词向量与$\vec{Paris}$的距离（比如余弦距离）最接近。<p>Skip-gram 的模型结构：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_paper_002_sg_001.png" alt=""></p><p>该论文提出了几点优化扩展，比如对高频率词进行重采样，可以提高训练速度（大约 2倍 - 10倍），并且提高了低频词的向量表示。此外还提出了一种简化的噪声对比估计（Noise Contrastive Estimation, NCE），与之前使用层次 Softmax 相比，能够更快的训练和更好的表示频繁单词。</p><p>从基于单词的模型扩展到基于短语的模型相对简单，文中使用数据驱动的方法识别大量的短语，然后将短语作为单独的标记来处理。为了评估短语向量的质量，作者开发了一套包含单词和短语的类比推理任务测试集，效果如下：</p><center>$\vec{Montreal Canadiens} - \vec{Montreal} + \vec{Toronto} = \vec{TorontoMaple Leafs}$</center></br>最后作者惊奇的发现（作者总会惊奇的发现点什么。。。），对词向量进行简单的运算可以得到一些有意思的结果，比如：<center>$\vec{俄罗斯} + \vec{河} = \vec{伏尔加河}$</center><center>$\vec{德国} + \vec{首都} = \vec{柏林}$</center></br>## Skip-gram 模型<p>Skip-gram 的训练目标是能够预测文本中某个词周围可能出现的词。比如，现在有一份文档（去掉标点符号）由 T 个词组成，$w_1,w_2,w_3,\dots,w_T$， skip-gram 的目标函数就是最大化它们的平均对数概率：</p><center>${1\over T}\sum^T_{t=1} \sum_{-c\leq j \leq c,j\neq 0}logP(w_{t+j}|w_t)$</center></br>其中 c 是上下文窗口大小，c 的值越大，对于中心词来说上下文词越多，会产生更多的训练实例，从而导致更高的准确度。但是代价是花费更多的训练时间。其中概率 $P(w_{t+j}|w_t)$使用 Softmax 函数计算：<center>$P(w_O|w_I) = {exp(v_{w_O}^T v_{w_I})\over \sum^W_{w=1} exp(v_w^T v_{w_I})}$</center></br>其中，$v_{w_i}$和$v_{w_O}$是输入输出向量，W 是词汇表中单词个数。这个公式是不切实际的，因为计算$log P(w_O|w_I)$的代价很高，通常与 W 成正比，而 W 一般比较大（$10^5–10^7$）。所以为了优化这个计算量，推出了下面的方法。<h3 id="层次-Softmax"><a href="#层次-Softmax" class="headerlink" title="层次 Softmax"></a>层次 Softmax</h3><p>Hierarchical Softmax 层次 Softmax，使用霍夫曼二叉树来表示输出层，W 个词分别作为叶子节点，每个节点都表示其子节点的相对概率。总词汇中的每个词都有一条从二叉树根部到自身的路径。用 n(w,j) 来表示从根节点到 w 词这条路径上的第 j 个节点，用 ch(n) 来表示 n 的任意一个子节点，设如果 x 为真则$[x]=1$，否则$[x]=-1$。那么 Hierarchical Softmax 可以表示为：</p><center>$P(w|w_I) = \prod^{L(w)-1}_{j=1}\sigma ([[n(w,j+1)=ch(n(w,j))]]\cdot v_{n(w,j)}^T v_{w_I})$ </center></br>其中：<center>$\sigma (x) = sigmoid(x) = {1 \over 1 + exp(-x)} $</center></br>$\sum^W_{w=1}P(w|w_I)=1$ 是可以验证的，这样一来 $logP(w_O|w_I)$ 和 $∇logP(w_O|w_I)$ 的计算开销就和 $L(w_O)$ 成比例。而 $L(w_O)$ 不会超过 $log⁡W$。<p>最大的优点：</p><ul><li>由于是二叉树，在输出层不需要计算 W 个节点，而只需要计算 log(W) 即可。</li><li>由于使用霍夫曼树是高频的词靠近树根，这样高频词需要更少的时间会被找到。这对于神经网络语言模型是简单且高效的加速训练技术。</li></ul><h3 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h3><p>Noise Contrastive Estimation (NCE) 噪声对比估计，假设一个好的模型可以通过逻辑回归（logistic regression）来区分正常数据和噪声。这与 Collobert 和 Weston 通过将数据排列在噪声之上来训练的合页损失函数（hinge loss）比较相似（论文原话，没搞懂什么意思，没具体研究过）。</p><p>因为 skip-gram 更关注于学习高质量的词向量表达，所以可以在保证词向量质量的前提下对 NCE 进行简化。于是定义了 NEG(Negative Sampling)：</p><center>$log\sigma(v_{w_O}^T v_{w_I}) + \sum^k_{i=1}E_{w_i - P_n(w)}[log\sigma(-v_{w_i}^Tv_{w_I})]$</center></br>这个公式用来替代 skip-gram 目标函数中的 $logP(w_O|w_I)$。 其中$P_n(w)$ 是词 n 周围的噪声词分布（NCE 和 NEG 都有这个参数）。这个公式是用逻辑回归从 k 个负例中区别出目标词汇 $w_O$。对于小的训练集 k 的最佳取值是 5-20， 对于大的训练集 k 的取值会更小，大概 2-5。NCE 和 NEG 的区别在于 NCE 在计算时需要样本和噪音分布的数值概率，而 NEG 只需要样本。<h3 id="高频词的二次采样"><a href="#高频词的二次采样" class="headerlink" title="高频词的二次采样"></a>高频词的二次采样</h3><p>在非常大的语料库中，最频繁的单词很容易出现数亿次（例如<strong>in</strong>、<strong>the</strong>和<strong>a</strong>），这样的词通常比罕见的词提供的信息价值更少。为了解决低频词和高频词之间的不平衡，论文提出了一种简单的二次采样方法，即每个词都有一定的概率被丢弃，这个概率计算方式为：</p><center>$P(w_i) = 1 - \sqrt{t\over f(w_i)}$</center></br>其中$f(w_i)$是词$w_i$的频率，t 是一个阈值，一般在 $10^{-5}$ 左右。当某个词的频率高于 t 就有一定概率被淘汰掉，所以高频率的词越容易被淘汰。<p>这个方法是启发式的，但是在实践中很有效（个人认为通过重采样平衡数据，可以更好的学习到低频词。同时随机性丢弃又增加了数据多样性，提高了泛化能力，符合深度学习理论）。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>为了训练 Skip-gram 模型，作者们使用了一个由各种新闻文章组成的大型数据集（一个包含 10 亿字的内部 Google 数据集）。从词汇中剔除了所有在训练数据中发生的少于 5 次的单词，从而产生了 692K 的词汇量。对比了各种 Skip-gram 模型在单词模拟测试集上的性能。NCE 代表噪声对比估计，NEG 代表了负采样，HS-Huffman 代表霍夫曼层次 Softmax。测试集使用的依然是之前构造的推理测试集。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_paper_002_subsampling_001.png" alt=""></p><p>从图中可以看出，效果最好的是 NEG。</p><h2 id="学习短语"><a href="#学习短语" class="headerlink" title="学习短语"></a>学习短语</h2><p>正如前面所讨论的，许多短语的含义并不是由单个单词的含义组成的。要学习短语的向量表示，首先要找到经常出现在一起的单词，并且组成一个 Token 作为一个词处理。简单地使用 n-gram, 会大大增大单词表, 并不是一种恰当的做法。文章使用了一个基于 unigram, bigram 的数据驱动方法，对于两个词 $w_i,w_j$：</p><center>$score(w_i,w_j) = {count(w_iw_j) - \delta \over    count(w_i) \times count(w_j)}$</center></br>其中$\delta$是惩罚项，用来避免太多无关的词被组合到一起。同时文章提出了一个数据集，用于评价该模型得到的短语质量（文章结尾给出地址）。<h3 id="短语-Skip-gram-结果"><a href="#短语-Skip-gram-结果" class="headerlink" title="短语 Skip-gram 结果"></a>短语 Skip-gram 结果</h3><p>与训练词向量相同，超参数使用 300 维度的词向量和大小为 5 的 window。对比结果如下：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_paper_002_resul_phrase_001.png" alt=""></p><p>为了最大限度地提高短语类任务的准确性，作者增加了训练数据量，使用了大约 330 亿个单词的数据集。使用层次结构的 softmax, 1000的维度，以及整个上下文。这使得模型的准确率达到了 72%。当将训练数据集的大小减少到 6B 个单词时，准确率降低了 66%，这表明大量的训练数据是至关重要的。</p><p>下图展示了不同模型的实际效果：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_paper_002_example_phrase_001.png" alt=""></p><p>通过比较，作者们发现，似乎最好的短语表示是通过一个层次 softmax 和 Subsampling 结合的模型来学习的。</p><h2 id="可加性-合成性"><a href="#可加性-合成性" class="headerlink" title="可加性/合成性"></a>可加性/合成性</h2><p>通过对训练目标的考察，可以解释向量的可加性。词向量与 Softmax 非线性的输入呈线性关系。通过训练单词向量来预测句子中周围的单词，向量可以被看作是单词出现时上下文的分布。这些值与输出层计算的概率呈对数关系，因此两个词向量的和与两个上下文分布的乘积有关。乘积在这里充当 AND 函数：两个词向量都赋予高概率的词将具有高概率，而其他词将具有低概率。因此，如果“伏尔加河”与“俄语”和“河流”这两个词频繁出现在同一句话中，那么这两个词的向量之和就会形成一个与“伏尔加河”向量相近的特征向量。</p><h2 id="词向量比较"><a href="#词向量比较" class="headerlink" title="词向量比较"></a>词向量比较</h2><p>本文使用 skip-gram 模型与传统词向量模型做了对比：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_paper_002_result_word_vec_001.png" alt=""></p><p>从对比结果可以看出，skip-gram 有明显的优势，可能这是由于 skip-gram 使用了大量级的数据集训练得到的词向量。尽管如此，skip-gram 的训练时间依然比其他模型要少的很多。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>论文指出, 在他们的实验中, 发现不同的问题具有不同的最优超参数配置。影响性能的最关键的决策是：</p><ul><li>架构的选择</li><li>词向量的维度</li><li>采样率</li><li>训练窗口的大小</li></ul><p>以上介绍的用于优化 skip-gram 的技术同样适用于 CBOW。除此之外，论文中提出的用于获取短语的模型算法也值得研究研究。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>官方数据集与代码：<a href="https://code.google.com/archive/p/word2vec/source/default/source">https://code.google.com/archive/p/word2vec/source/default/source</a></p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 分布式词表示 </tag>
            
            <tag> Hierarchical Softmax </tag>
            
            <tag> 负采样 </tag>
            
            <tag> Skip-gram </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读《Efficient Estimation of Word Representations in Vector Space》</title>
      <link href="posts/fcba888f.html"/>
      <url>posts/fcba888f.html</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这篇论文发表于 2013 年，作者是 Tomas Mikolov，也就是提出 Word2vec（Google 时期）和 Fasttext（Facebook 时期）的大佬。本篇文章主要讲的就是 Word2vec 框架，也就是从这开始，Mikolov 将大家从语言模型时代带入了词嵌入的时代。</p><p>论文提出了两种可以从大规模语料库中学习到词向量表示的模型，可以用于计算词相似度的任务。并且相比以往的模型取得了不错的性能和准确度。在 1.6 亿规模的数据集训练只花了一天的时间。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>传统的 NLP 系统和技术都将单词作为基本计算单位，词之间没有相关性，只是认为是词库的索引。这样的选择虽然简单，鲁棒性好和可观察，比如语言模型 N-Gram。虽然可以用到大多数任务中，但是还是具有局限性。</p><p>随着计算机算力水平的提高，现在可以实现在大数据集进行复杂模型的计算。最成功的可能就是分布式词表示，广泛用于神经网络语言模型等任务，并且性能表现优于 N-Gram 模型。</p><h3 id="论文目标"><a href="#论文目标" class="headerlink" title="论文目标"></a>论文目标</h3><p>本篇论文的目的是可以从数十亿级别的语料库中训练出高质量的词向量，并且词向量之间具有相似度关系，如意思相近的词挨得比较近，而且每个单词可以具有多个相似度。</p><p>该论文还发现了单词的向量表示，不仅可以简单的表示相似性，还可以通过词偏移技术进行代数运算：</p><center> $\vec{King} - \vec{Man} + \vec{Woman} = \vec{Queen}$</center> </br></br>## 模型结构<p>已经有许多种类的模型用来表示连续的词向量，比较出名的有 LSA 和 LDA。而本篇论文着力于使用神经网络模型来学习词向量的表示。实验证明，在保持词向量的线性规律方面，神经网络模型比 LSA 等模型有更好的性能。而且 LDA 在大数据量情况下计算会很吃力。</p><p>训练模型的复杂度跟如下公式成正比，试图最大化精度并且最小化训练复杂度。</p><center>$O = E \times T \times Q$</center> </br></br>其中 O 代表训练模型复杂度，E 代表迭代（Epochs）次数，T 代表词汇表中单词数量，Q 表示模型本身结构复杂度（根据参数决定）。通常情况下 E = 3 - 50，T 的值也就是单词数会达到上亿。本文所有模型使用随机梯度下降法和反向传播。<h3 id="前向神经网络语言模型（NNLM）"><a href="#前向神经网络语言模型（NNLM）" class="headerlink" title="前向神经网络语言模型（NNLM）"></a>前向神经网络语言模型（NNLM）</h3><p>概率前向神经网络语言模型已经提出了，它包括输入层 Input，投影层 Projection，隐藏层 Hidden 和输出层 Output。输入层中前 N 个词编码为 1-of-V 的向量，V 是词汇表的大小。Input 和 Projection 之间是一个 N$\times$D 的权重矩阵，经过矩阵变换后，在 Projection 中任意时刻只有 N 个输入是激活的。而隐藏层用来计算整个词汇表的概率分布，所以输出层的维度是词汇表的大小 V。如果设隐藏层的节点个数为 H，那么每个训练实例的计算复杂度为：</p><center>$Q = N\times D + N\times D\times H + H\times V$</center></br></br>主要的计算量是在H $\times$ V，但是可以通过层次 Softmax，避免数据归一化或者对词汇表使用二叉树存储等操作减少计算量至 log(v)。这样主要的复杂度就落在了 N$\times$ D$\times$ H的头上。<p>该论文提出的模型使用的就是层次 Softmax，使用霍夫曼编码树，根据词出现的频率进行建树，实验证明词频对神经网络语言模型有很大影响。使用基于霍夫曼二叉树的层次 Softmax 可以将计算量减少至 log(unigram_perplexity(V))。举个例子来说，如果词汇表有 100 万个单词，那么可以减少两倍的计算量。</p><h3 id="循环神经网络语言模型（RNNLM）"><a href="#循环神经网络语言模型（RNNLM）" class="headerlink" title="循环神经网络语言模型（RNNLM）"></a>循环神经网络语言模型（RNNLM）</h3><p>循环神经网络语言模型已经被提出，以克服前向神经网络语言模型的局限性，例如需要指定上下文长度。RNNs 比浅层神经网络更能表达复杂的模式，它没有投影层 Projection，只有输入层 Input 隐藏层 Hidden和输出层 Output。RNN 的不同之处在于可以保存短暂的记忆或者说是上文的信息可以用于下文中。每个训练实例的计算复杂度为：</p><center>$Q = H\times H + H\times V$</center></br></br>同样的，H $\times$ V 可以通过层次 Softmax 进行优化减少计算量至 log(v)，所以主要计算量在于H $\times$ H。<h3 id="并行训练"><a href="#并行训练" class="headerlink" title="并行训练"></a>并行训练</h3><p>为了在大数据集上进行实验，作者在一个分布式框架（DistBelief）上实现了论文中的几个模型。这个框架允许并行运行一个模型的多个副本，每个副本的梯度更新同步通过中央服务器来保持所有参数的一致。对于这种并行训练方式，作者采用mini-batch异步梯度以及自适应的学习速率，整个过程称为Adagrad。采用这种框架，使用100多个模型副本，多个机器的多个CPU核。</p><h2 id="新的对数线性模型"><a href="#新的对数线性模型" class="headerlink" title="新的对数线性模型"></a>新的对数线性模型</h2><p>在本节，作者提出了两个新的模型用于训练分布式词向量。根据前面的介绍，模型训练的主要复杂度是在非线性隐藏层，所以提出的新模型致力于减少或者去掉该部分的计算量。</p><h3 id="连续词袋模型（CBOW）"><a href="#连续词袋模型（CBOW）" class="headerlink" title="连续词袋模型（CBOW）"></a>连续词袋模型（CBOW）</h3><p>第一个模型与 NNML 有些相似，但是去掉了非线性隐藏层，只有输入层，投影层和输出层。区别于 NNML 的投影层，这里的投影层是对所有单词共享的，即所有的单词都投影到同一个位置（所有向量取平均值）。这样不考虑单词的位置顺序信息，叫做词袋模型。同时也会用到将来的词，例如如果窗口 windows 为 2，这样训练中心词的词向量时，会选取中心词附近的 4 个上下文词（前面 2 个后面 2 个）。然后输出层是一个 log-linear 分类器，也就是上面所说的加上了霍夫曼二叉树的 Softmax，所以整体的复杂度是：</p><center>$Q = N\times D + D \times log(V)$</center></br></br>这个模型就称之为 Continuous Bag-of-Words Model 连续词袋模型，它区别于传统的词袋模型是因为使用了上下文的连续词向量表示。模型结构如下（图片中的 windows 为 2）：<p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_paper_001_cbow.png" alt=""></p><h3 id="跳词模型（Skip-gram）"><a href="#跳词模型（Skip-gram）" class="headerlink" title="跳词模型（Skip-gram）"></a>跳词模型（Skip-gram）</h3><p>第二个模型跟 CBOW 相似，但是它不是像 CBOW 那样根据上下文词学习中心词，而相反的根据中心词去学习上下文中的一个词。更准确地说，使用每个当前单词作为一个具有连续投影层的对数线性分类器的输入，并在当前单词前后的一定范围内（windows）预测单词。通过实验发现，增加 windows 可以提高结果词向量的质量，但同时也增加了计算的复杂性。由于距离较远的单词通常与当前单词的关联性比与当前单词的关联性小，因此我们通过从训练示例中的单词中抽取较少的样本来减少对距离较远的单词的权重。模型整体的复杂度为：</p><center>$Q = C \times (D + D \times log(V))$</center></br></br>其中 C 为 windows 的 2 倍，也是中心词前后上下文词的个数。模型结构为：<p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_paper_001_skipgram.png" alt=""></p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>根据以往的实验和观察，单词之间可以有许多不同类型的相似性，例如 big 和 bigger 类似于 small 和 smaller 具有相同的含义。另一种关系类型的例子可以是单词对 big - biggest 和 small - smallest。令人惊讶的是，这些关系都可以通过词向量之间的线性运算得到正确的结果。就像big-biggest，我们可以通过下面的公式计算：</p><center>$\vec{X} = \vec{biggest} - \vec{big} + \vec{small} $</center></br></br>然后通过余弦距离在向量空间中找到里$\vec{X}$最近的一个词向量，经过良好的训练后，可以正确的找到这个词向量 smallest。<p>还有的就是，通过加大词向量的维度并且在大数据集上训练的话，可以得到更微妙的语义关系。例如城市和它所属的国家，例如法国对巴黎，德国对柏林。具有这种语义关系的词向量可用于改进现有的许多NLP应用程序，如机器翻译、信息检索和问答系统等。下图是语义-句法词关系测试集中五类语义和九类句法问题的实例：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_paper_001_relation_table.png" alt=""></p><h3 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h3><p>为了衡量词向量的质量，作者定义了一个综合测试集，包含五种语义问题和九种句法问题。上图显示了每类中的两个示例。总的来说，有8869个语义问题和10675个句法问题。每个类别中的问题都是通过两个步骤创建的：首先，手动创建类似单词对的列表。然后，通过连接两个词对形成一个大的问题列表。例如，列出了68个美国大城市及其所属的州，并通过随机选择两个词对，形成了大约 2.5k 的问题。测试集中只包含单个标记词，因此不存在多词实体（such as New York）。</p><p>评价模型结果好坏的标准就是上述的词向量线性运算，如果通过线性运算得到的单词与正确的单词是完全一致的，那么就代表该词向量是正确的。所以同义词很难被计算出来，因为没有对同义词的输入，所以模型不可能达到 100% 的准确率。但是该模型的准确率与某些任务是正相关的，所以还是有一些用处的。</p><h3 id="精度最大化"><a href="#精度最大化" class="headerlink" title="精度最大化"></a>精度最大化</h3><p>论文使用了谷歌新闻语料库来训练词汇向量，这个语料库包含大约 6B 个 token，并且将词汇量限制在一百万个最常见的词。为了能够快速的评估模型结构体系和参数的最佳结果，作者首先在数据集的子集上进行评估，词汇限制在最频繁的 30K 单词。下图是使用 CBOW 在不同向量维度和不同数据量下的结果：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_paper_001_result_cbow_01.png" alt=""></p><p>可以看出达到某个点之后，增大向量维度并没有增加精度反而减少。而增加数据量也只是微小的增加精度，但是根据之前所说的训练复杂度公式可知，当增大单词数量时，训练复杂度也在增加。</p><h3 id="模型比较"><a href="#模型比较" class="headerlink" title="模型比较"></a>模型比较</h3><p>作者也做了和传统模型的比较工作，使用相同的数据集，相同 640 维度的词向量，也不仅限使用 30k 的单词，并且使用了全部的测试集数据。以下是训练结果的比较：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_paper_001_result_002.png" alt="使用在相同数据上训练的模型与640维字向量对模型进行比较。本文语义-句法关系测试集和[20]的句法关系测试集报告精确度。"></p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_paper_001_result_003.png" alt="比较语义句法词汇关系测试集上的公共可用词汇向量和我们模型中的词汇向量。使用完整词汇表。"></p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_paper_001_result_004.png" alt="在相同数据上三个epoch训练的模型与为一个epoch训练的模型的比较,使用完整的语义-句法数据集。"></p><h3 id="大规模并行训练模型"><a href="#大规模并行训练模型" class="headerlink" title="大规模并行训练模型"></a>大规模并行训练模型</h3><p>在 DistBelief 上使用谷歌 News 6B 数据集训练的几个模型的结果对比：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_paper_001_result_005.png" alt="使用DistBelief分布式框架训练的模型的比较，请注意使用1000维向量训练NNLM将花费很长时间才能完成。"></p><h3 id="Microsoft-Research-Sentence-Completion-Challenge"><a href="#Microsoft-Research-Sentence-Completion-Challenge" class="headerlink" title="Microsoft Research Sentence Completion Challenge"></a>Microsoft Research Sentence Completion Challenge</h3><p>微软的句子完成挑战最近被作为一项推进语言建模和其他NLP技术的任务引入，这个任务由1040个句子组成，每个句子中有一个单词丢失，目标从五个合理的选择列表中选择与句子其余部分最连贯的单词。本论文的模型与传统模型在该比赛中的分数对比：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_paper_001_result_006.png" alt="Microsoft Sentence完成挑战中模型的比较和组合。"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>词向量已经成为现在 NLP 深度学习中不可缺少的一部分，从 Word2vec 到 Bert 都是促进 NLP 得到飞跃式提升的重要技术。训练出质量高覆盖率广的词向量，可以有效的提高下游任务的性能准确率，如机器翻译，情感分类等。而词向量训练的难点主要就是词汇表和语料库的规模，目前 Bert 使用的语料量级已经很难超越，不知道这是不是词向量的顶点？</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>测试集：<a href="http://www.fit.vutbr.cz/~imikolov/rnnlm/word-test.v1.txt">http://www.fit.vutbr.cz/~imikolov/rnnlm/word-test.v1.txt</a><br>官方源码：<a href="https://code.google.com/archive/p/word2vec/">https://code.google.com/archive/p/word2vec/</a></p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> Skip-gram </tag>
            
            <tag> 分布式词向量 </tag>
            
            <tag> CBOW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu下划词翻译工具-GoldenDict</title>
      <link href="posts/7e57c477.html"/>
      <url>posts/7e57c477.html</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><a href="http://goldendict.org/">Goldendict</a> 是一款跨平台的翻译软件，支持划词翻译等功能。它更像一种词典或者词典的API工具，通过外挂好本地词典或者网站词典进行翻译，返回的样式就是设置词典的样式。</p><p>Golendict 有以下几大特点：</p><ul><li>自定义词本地典库，可加载外挂词典，可自定义分组；</li><li>自定义在线词典和百科；</li><li>支持屏幕取词（虽然有时不是很灵），但搭配上 Autohotkey 无敌；</li><li>支持全文搜索，有生词本且可导出。</li></ul><p>官方效果图：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_tool_goldendict_heron-single.png" alt=""></p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>这里主要介绍一下 Ubuntu 下的安装，其他平台的安装进入<a href="http://goldendict.org/">官网</a>自行查看。</p><p>在 Ubuntu 上安装 Goldendict 比较简单，直接在命令行输入如下命令即可：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> goldendict<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="添加在线词典"><a href="#添加在线词典" class="headerlink" title="添加在线词典"></a>添加在线词典</h3><p>我们可以通过如下步骤添加在线词典，左上角菜单栏找到“编辑“选项,然后 编辑-&gt;词典-&gt;网站-&gt;添加：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_tool_glodendict_add_net_dict.png" alt=""></p><p>如何获取到在线词典呢？百度搜索必应词典，随便搜索一个单词，例如 welcome，会得到一个网址：<br><code>http://cn.bing.com/dict/search?q=welcome&amp;;qs=n&amp;;form=Z9LH5&amp;;pq=welcome&amp;;sc=0-7&amp;;sp=-1&amp;;sk=&amp;;cvid=127D88B2AD4E4842A79BCB32B430FC33</code><br>然后将其中的 welcome 全部替换成 %GDWORD% 得到：<br><code>http://cn.bing.com/dict/search?q=%GDWORD%&amp;;qs=n&amp;;form=Z9LH5&amp;;pq=%GDWORD%&amp;;sc=0-7&amp;;sp=-1&amp;;sk=&amp;;cvid=127D88B2AD4E4842A79BCB32B430FC33</code></p><ul><li>有道： <code>http://dict.youdao.com/search?q=%GDWORD%&amp;ue=utf8</code></li><li>必应： <code>http://cn.bing.com/dict/search?q=%GDWORD%</code></li><li>海词：<code>http://dict.cn/%GDWORD%</code></li></ul><p>需要注意的是，添加完在线词典之后需要勾选上才可以使用。</p><h3 id="添加本地词典"><a href="#添加本地词典" class="headerlink" title="添加本地词典"></a>添加本地词典</h3><p>在编辑里选择词典&gt;词典来源&gt;文件，点击添加，我们可以新建一个文件夹来存放我们的字典文件。然后我们将下载好的字典文件解压后，放到这个文件夹中，点击重新扫描就可以识别出本地词典了。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_tool_glodendict_add_local_dict_1.png" alt=""></p><p>这样我们就可以在词典中查看已经添加进来的词典了。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_tool_glodendict_add_local_dict_2.png" alt=""></p><h2 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h2><p>推荐几部精选的词典并附上图片以及下载地址：</p><h3 id="牛津高阶-8-简体-spx-（带发音）"><a href="#牛津高阶-8-简体-spx-（带发音）" class="headerlink" title="牛津高阶 8 简体 spx （带发音）"></a>牛津高阶 8 简体 spx （带发音）</h3><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_tool_glodendict_niujin_dict.png" alt=""></p><p><a href="https://pan.baidu.com/s/1MupVJbBl4KxGjQ-PYo2pTQ">百度网盘</a></p><p>提取码：enp6</p><h3 id="Vocabulary-com-Dictionary-英文版（联网发音）"><a href="#Vocabulary-com-Dictionary-英文版（联网发音）" class="headerlink" title="Vocabulary.com Dictionary 英文版（联网发音）"></a>Vocabulary.com Dictionary 英文版（联网发音）</h3><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_tool_glodendict_vocabulary.png" alt=""></p><p><a href="https://pan.baidu.com/s/1d2pb-myWwXMZslwwQ1Hg2g">百度网盘</a></p><p>提取码：fsug</p><h3 id="简明英汉必应版（增强版升级）-432万词条"><a href="#简明英汉必应版（增强版升级）-432万词条" class="headerlink" title="简明英汉必应版（增强版升级） 432万词条"></a>简明英汉必应版（增强版升级） 432万词条</h3><p><a href="https://github.com/skywind3000/ECDICT-ultimate/releases">Github</a></p><h3 id="星际译王"><a href="#星际译王" class="headerlink" title="星际译王"></a>星际译王</h3><p>向大家推荐星际译王的词典<a href="http://download.huzheng.org/">下载网站</a>，这个网站几乎包含了所有的字典，我们可以选择下载。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_tool_glodendict_xingji.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 便捷工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 划词翻译 </tag>
            
            <tag> GoldenDict </tag>
            
            <tag> 词典 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Windows下命令行工具-Cmder</title>
      <link href="posts/7d402318.html"/>
      <url>posts/7d402318.html</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在Linux操作系统下工作久的程序员，在Windows上开发很难适应windows自带的cmd命令行。在此推荐一款开发神器 - Cmder，让你可以在Windows下也可以像Linux中那样使用命令行。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cmder_main.png" alt=""></p><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p><a href="https://cmder.net/">官网地址</a></p><p>进入官网以后，有Mini版和完整版，建议完整版，完整版功能更齐全，还可以使用<code>git</code>，下载好解压文件包以后就可以使用。 </p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cmder_download.png" alt=""></p><p>解压位置随意，但是个人建议解压到C盘下。如果你解压到了C盘，打开cmder.exe时可能会失败，因为需要使用管理员权限才可以打开。此时我们只需要右键点击Cmder.exe，选择属性 - &gt; 兼容性 - &gt; 勾选以管理员身份运行此程序即可（如下图），这样以后每次打开都不需要使用管理员身份运行了，同时在其他文件夹下使用右键开启也不会报错了。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cmder_add_auth.png" alt=""></p><h2 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h2><p>在系统变量PATH添加cmder.exe的路径，使可以在任何位置都可以执行Cmder。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cmder_add_path.png" alt=""></p><h2 id="添加到右键菜单"><a href="#添加到右键菜单" class="headerlink" title="添加到右键菜单"></a>添加到右键菜单</h2><p>对开始菜单按钮右击，选择打开windows powershell的管理员模式，执行以下命令即可： </p><pre class="line-numbers language-bash"><code class="language-bash">Cmder.exe /REGISTER ALL<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>执行完该命令后，在任何文件夹下点击鼠标右键就可以执行Cmder了。</p><h2 id="如何从右键菜单删除"><a href="#如何从右键菜单删除" class="headerlink" title="如何从右键菜单删除"></a>如何从右键菜单删除</h2><p>我们可以通过修改注册表的方式，删除掉右键菜单中不想要的选项。</p><ul><li><p>点击左下角开始菜单 -&gt; 运行（输入regedit）-&gt;  确定或者回车。</p></li><li><p>在打开的注册表中找到：HKEY_CLASSES_ROOT，并点HKEY_CLASSES_ROOT前面的小三角；找到Directory，点击前面的小三角；找到Background，点击前面的小三角；打开shell，可以看到Cmder，看清楚哦，不是cmd是Cmder。右键点击它然后选择删除即可。</p></li><li><p>接下来关闭注册表，在桌面上右击鼠标就能看到Cmder选项被删除啦！</p></li></ul><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cmder_delte_reg.png" alt=""></p><h2 id="界面设置"><a href="#界面设置" class="headerlink" title="界面设置"></a>界面设置</h2><p>首先使用<code>windows+alt+p</code>进入界面设置<br><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cmder_setting.png" alt=""></p><h2 id="快捷键"><a href="#快捷键" class="headerlink" title="快捷键"></a>快捷键</h2><pre class="line-numbers language-bash"><code class="language-bash">Tab       自动路径补全Ctrl+T    建立新页签Ctrl+W    关闭页签Ctrl+Tab  切换页签Alt+F4    关闭所有页签Alt+Shift+1 开启cmd.exeAlt+Shift+2 开启powershell.exeAlt+Shift+3 开启powershell.exe <span class="token punctuation">(</span>系统管理员权限<span class="token punctuation">)</span>Ctrl+1      快速切换到第1个页签Ctrl+n      快速切换到第n个页签<span class="token punctuation">(</span> n值无上限<span class="token punctuation">)</span>Alt + enter 切换到全屏状态Ctr+r       历史命令搜索Win+Alt+P   开启工具选项视窗<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 便捷工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cmder </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>信息熵总结</title>
      <link href="posts/686d9456.html"/>
      <url>posts/686d9456.html</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>学习决策树时会接触到一些信息熵,条件熵和信息增益的知识,此外还有互信息,相对熵,交叉熵和互信息,KL散度等等乱七八糟的知识和名字,我本人已经记得大脑混乱了,还没有全部记住,所以在这里记录一下.</p><h2 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h2><p>信息的度量，信息的不确定程度，是乱七八糟熵的基础。吴军大大的数学之美中用了猜球队冠军的方式引出了信息熵的概念。我觉得这种方法印象很深刻，所以在这里提出一下。如果有32支球队，使用二分查找法去猜哪支球队是冠军，如：冠军在1-16号球队内。这样一共需要猜5次就可以找到结果，也就是$log32=5$。但是某些球队的获胜率大一些，所以它的准确信息量的表示应该如下:</p><center>$H(X) = - \sum_{x\in X}P(x)logP(x)$ </br> </br> </center>香农称它为信息熵，表示信息的不确定程度。不确定性越大，信息熵也就越大。图1中的$P(x)$表示随机变量$x$的概率，信息熵$H(X)$的取值范围：$0<=H(X)<=logn$，其中$n$是随机变量$X$取值的种类数。<h2 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h2><p>有两个随机变量$X$和$Y$，在已知$Y$的情况下，求$X$的信息熵称之为条件熵：</p><center>$H(X|Y) = -\sum_{x\in X,y\in Y}P(x,y)logP(x|y)$ </br> </br> </center>其中$P(x|y)$是已知$y$求$x$的条件概率，$P(x,y)$是联合概率。<h2 id="联合熵"><a href="#联合熵" class="headerlink" title="联合熵"></a>联合熵</h2><p>联合熵可以表示为两个事件$X$、$Y$的熵的并集：</p><center> $H(X,Y) = -\sum_{i=1}^n \sum_{j=1}^n P(x_i,y_i)logP(x_i,y_i)$</center><center>$= \sum_{i=1}^n \sum_{j=1}^n P(x_i,y_i)log{1\over P(x_i,y_i    )}$ </br></br>  </center> <p>它的取值范围是：$max(H(x),H(y)) &lt;= H(x,y) &lt;= H(x)+H(y)$</p><h2 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h2><p>表示在确定某条件Y后，随机变量$X$的信息不确定性减少的程度，也称为互信息($Mutual Information$).</p><center> $I(X;Y) = H(X) - H(X|Y)$</br> </br> </center>它的取值是$0$到$min(H(X),H(Y))$之间的数值，取值为$0$时表示两个事件$X$和$Y$完全不相关。在决策树算法中$ID3$算法就是使用信息增益来划分特征的。在某个特征条件下，求数据的信息增益，信息增益大的特征说明对数据划分帮助很大。优先选择该特征进行决策树的划分，这就是$ID3$算法。<h2 id="信息增益比（率）"><a href="#信息增益比（率）" class="headerlink" title="信息增益比（率）"></a>信息增益比（率）</h2><p>信息增益比是信息增益的进化版，用于解决信息增益对属性选择取值较多的问题。信息增益率为信息增益与该特征的信息熵之比。在决策树中算法中，$C4.5$算法就是使用信息增益比来划分特征。公式如下：</p><center> $g_R(D,A) = {g(D,A)\over H(D)} $</br> </br> </center>信息熵，条件熵和互信息的关系：<p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_xinxishang_relation.png" alt=""></p><h2 id="基尼系数（Gini）"><a href="#基尼系数（Gini）" class="headerlink" title="基尼系数（Gini）"></a>基尼系数（Gini）</h2><p>在决策树的$CART$(分类回归树)中有两类树，一是回归树，划分特征使用的是平方误差最小化的方法。二是分类树，采用的就是$Gini$系数最小化进行划分数据集。</p><center> $Gini(P) = \sum_{k=1}^K p_k(1-p_k) = 1 - \sum_{k=1}^K p_k^2$ </br> </br> </center>其中$k$为$label$的种类数。基尼指数越大，信息的不确定性越大，这与信息熵相同。（$CART$树是如何使用$Gini$指数的这里就不详细介绍了，以后会在决策树中详细介绍的。）<h2 id="相对熵（KL散度）"><a href="#相对熵（KL散度）" class="headerlink" title="相对熵（KL散度）"></a>相对熵（KL散度）</h2><p>用来描述两个概率分布$P$、$Q$之间的差异，数学之美中介绍是用来衡量两个取值为正数函数的相似性：</p><center> $KL（P||Q）= \sum_{i=1}^n P(x_i)log{P(x_i)\over Q(x_i)}$ </br></br> </center>如果两个函数(分布)完全相同，那么它们的相对熵为0。同理如果相对熵越大，说明它们之间的差异越大，反之相对熵越小，说明它们之间的差异越小。需要注意的是相对熵不是对称的，也就是：<center> $KL(P||Q)\neq KL(Q||P)$ </br> </br> </center>但是这样计算很不方便，所以香农和杰森（不是郭达斯坦森）提出了一个新的对称的相对熵公式：<center> $JS(P||Q) = {1\over 2}[KL(P||Q) + KL(Q||P)]$ </br></br> </center>上面的相对熵公式可以用于计算两个文本的相似度。吴军大大在《数学之美》中介绍，$Google$的问答系统就是用这个公式计算答案相似性的（现在还是不是就不清楚了）。<h2 id="交叉熵（Cross-Entropy）"><a href="#交叉熵（Cross-Entropy）" class="headerlink" title="交叉熵（Cross-Entropy）"></a>交叉熵（Cross-Entropy）</h2><p>我们知道通常深度学习模型最后一般都会使用交叉熵作为模型的损失函数。那是为什么呢？首先我们先将相对熵$KL$公式进行变换（$log$中除法可以拆分为两个$log$相减）:</p><center> $D_{KL}(P||Q) = \sum_{i=1}^n P(x_i)log(P(x_i)) - \sum_{i=1}^n P(x_i)log(Q(x_i))$ </center><center> $= -H(P(x)) + [-\sum_{i=1}^n P(x_i)log(Q(x_i))]$</br></br> </center>其中前一部分的$-H(P(x))$是$P$的信息熵，后一部分就是我们所说的交叉熵。<center> $H(P,Q) = -\sum_{i=1}^n P(x_i)log(Q(x_i))$ </br></br> </center>损失函数是计算模型预测值和数据真实值之间的相关性，所以可以使用相对熵（$KL$散度）计算。根据上述公式可以看出：$-H(P(x))$是不变的，所以我们可以通过计算后一部分的交叉熵来求得$Loss$。所以通常会使用交叉熵来作为$Loss$函数。同理交叉熵越小，预测值和真实值之间相似度越高，模型越好。<blockquote><p>LR的损失函数就是交叉熵。</p></blockquote><h2 id="困惑度（Perplexity，PPL）"><a href="#困惑度（Perplexity，PPL）" class="headerlink" title="困惑度（Perplexity，PPL）"></a>困惑度（Perplexity，PPL）</h2><p>在$NLP$中，通常使用困惑度作为衡量语言模型好坏的指标。</p><center> $PP(S) = P(w_1w_2\dots w_n)^{-{1\over N}}$</center><center> $= \sqrt[N]{1\over{P(w_1w_2\dots w_n)}}$</center><center> $= \sqrt[N]{\prod_{i=1}^n {1\over P(w_i|w_1w_2\dots w_{i-1})}}$ </br></br> </center>其中$S$为句子，$N$是句子中单词的个数，$P(w_i)$代表第$i$个单词的概率。所以$PPL$越小$P(w_i)$的概率越高，则这句话$S$属于自然语言的概率也就越高。]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信息熵 </tag>
            
            <tag> 交叉熵 </tag>
            
            <tag> 信息增益 </tag>
            
            <tag> 基尼系数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人工智能名校课程汇总</title>
      <link href="posts/f17e61a2.html"/>
      <url>posts/f17e61a2.html</url>
      
        <content type="html"><![CDATA[<h2 id="斯坦福大学"><a href="#斯坦福大学" class="headerlink" title="斯坦福大学"></a>斯坦福大学</h2><p><a href="https://www.stanford.edu/">大学课程官网</a></p><h3 id="CS229-机器学习-by-吴恩达"><a href="#CS229-机器学习-by-吴恩达" class="headerlink" title="CS229 机器学习 by 吴恩达"></a>CS229 机器学习 by 吴恩达</h3><p>课程主页：<a href="http://cs229.stanford.edu/">http://cs229.stanford.edu/</a><br>相关链接：</p><ul><li><a href="https://stanford.edu/~shervine/teaching/cs-229/">https://stanford.edu/~shervine/teaching/cs-229/</a></li><li><a href="https://github.com/afshinea/stanford-cs-229-machine-learning">https://github.com/afshinea/stanford-cs-229-machine-learning</a></li><li><a href="https://github.com/Kivy-CN/Stanford-CS-229-CN">https://github.com/Kivy-CN/Stanford-CS-229-CN</a></li></ul><h3 id="CS231n-深度视觉识别-by-李飞飞"><a href="#CS231n-深度视觉识别-by-李飞飞" class="headerlink" title="CS231n 深度视觉识别 by 李飞飞"></a>CS231n 深度视觉识别 by 李飞飞</h3><p>课程主页：<a href="http://cs231n.stanford.edu/">http://cs231n.stanford.edu/</a><br>视频地址：<a href="https://www.bilibili.com/video/av13260183/">https://www.bilibili.com/video/av13260183/</a><br>课件地址：<a href="http://cs231n.stanford.edu/syllabus.html">http://cs231n.stanford.edu/syllabus.html</a><br>相关链接：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/76514213">https://zhuanlan.zhihu.com/p/76514213</a></li><li><a href="https://github.com/Burton2000/CS231n-2017">https://github.com/Burton2000/CS231n-2017</a></li><li><a href="https://github.com/mbadry1/CS231n-2017-Summary">https://github.com/mbadry1/CS231n-2017-Summary</a></li><li><a href="https://zhuanlan.zhihu.com/p/21353567">https://zhuanlan.zhihu.com/p/21353567</a></li><li><a href="https://zhuanlan.zhihu.com/p/21941485">https://zhuanlan.zhihu.com/p/21941485</a></li></ul><h3 id="CS224n-深度学习自然语言处理-by-Manning"><a href="#CS224n-深度学习自然语言处理-by-Manning" class="headerlink" title="CS224n 深度学习自然语言处理 by Manning"></a>CS224n 深度学习自然语言处理 by Manning</h3><p>课程主页：<a href="http://web.stanford.edu/class/cs224n">http://web.stanford.edu/class/cs224n</a><br>相关链接：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/59011576">https://zhuanlan.zhihu.com/p/59011576</a> 2019版川陀学者学习笔记</li><li><a href="https://zhuanlan.zhihu.com/p/38387843">https://zhuanlan.zhihu.com/p/38387843</a> 知乎推荐</li><li><a href="https://zhuanlan.zhihu.com/p/60992466">https://zhuanlan.zhihu.com/p/60992466</a> 2019版资料汇总</li><li><a href="https://zhuanlan.zhihu.com/p/68502016">https://zhuanlan.zhihu.com/p/68502016</a> 2019版LooperXX学习笔记</li><li><a href="https://www.hankcs.com/nlp/cs224n-introduction-to-nlp-and-deep-learning.html">https://www.hankcs.com/nlp/cs224n-introduction-to-nlp-and-deep-learning.html</a> 2017版中文翻译+笔记</li><li><a href="https://blog.csdn.net/weixin_37251044/article/details/83473874">https://blog.csdn.net/weixin_37251044/article/details/83473874</a> 2017版学习笔记+资料+训练营</li><li><a href="https://github.com/learning511/cs224n-learning-camp">https://github.com/learning511/cs224n-learning-camp</a> 2017版训练营</li></ul><h3 id="CS230-深度学习-by-吴恩达"><a href="#CS230-深度学习-by-吴恩达" class="headerlink" title="CS230 深度学习 by 吴恩达"></a>CS230 深度学习 by 吴恩达</h3><p>课程主页：<a href="https://web.stanford.edu/class/cs230/">https://web.stanford.edu/class/cs230/</a><br>视频地址：<a href="https://www.bilibili.com/video/av47055599">https://www.bilibili.com/video/av47055599</a><br>相关链接：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/38426219">https://zhuanlan.zhihu.com/p/38426219</a></li><li><a href="https://stanford.edu/~shervine/teaching/cs-230/">https://stanford.edu/~shervine/teaching/cs-230/</a></li><li><a href="https://github.com/afshinea/stanford-cs-230-deep-learning">https://github.com/afshinea/stanford-cs-230-deep-learning</a></li><li><a href="https://zhuanlan.zhihu.com/p/61062475">https://zhuanlan.zhihu.com/p/61062475</a></li></ul><h3 id="CS236-深度生成模型"><a href="#CS236-深度生成模型" class="headerlink" title="CS236 深度生成模型"></a>CS236 深度生成模型</h3><p>相关链接：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/80638685">https://zhuanlan.zhihu.com/p/80638685</a></li><li><a href="https://www.bilibili.com/video/av71884912">https://www.bilibili.com/video/av71884912</a></li></ul><h3 id="CS224u-自然语言理解"><a href="#CS224u-自然语言理解" class="headerlink" title="CS224u 自然语言理解"></a>CS224u 自然语言理解</h3><p>课程主页：<a href="http://web.stanford.edu/class/cs224u/">http://web.stanford.edu/class/cs224u/</a><br>视频地址：<a href="https://www.bilibili.com/video/av56067156/">https://www.bilibili.com/video/av56067156/</a><br>课件地址：<a href="http://web.stanford.edu/class/cs224u/index.html">http://web.stanford.edu/class/cs224u/index.html</a><br>相关链接：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/70087847">https://zhuanlan.zhihu.com/p/70087847</a></li><li><a href="https://zhuanlan.zhihu.com/p/74471770">https://zhuanlan.zhihu.com/p/74471770</a></li><li><a href="https://github.com/cgpotts/cs224u/">https://github.com/cgpotts/cs224u/</a></li></ul><h3 id="CS234-强化学习"><a href="#CS234-强化学习" class="headerlink" title="CS234 强化学习"></a>CS234 强化学习</h3><p>课程主页：<a href="http://web.stanford.edu/class/cs234/">http://web.stanford.edu/class/cs234/</a><br>课件地址：<a href="http://web.stanford.edu/class/cs234/schedule.html">http://web.stanford.edu/class/cs234/schedule.html</a><br>相关链接：</p><ul><li><a href="https://blog.csdn.net/yH0VLDe8VG8ep9VGe/article/details/89077964">https://blog.csdn.net/yH0VLDe8VG8ep9VGe/article/details/89077964</a></li><li><a href="https://www.zhihu.com/question/265091571/answer/293362064">https://www.zhihu.com/question/265091571/answer/293362064</a></li><li><a href="https://github.com/Observerspy/CS234">https://github.com/Observerspy/CS234</a></li><li><a href="https://github.com/dennybritz/reinforcement-learning">https://github.com/dennybritz/reinforcement-learning</a></li></ul><h3 id="CS224s-语音识别"><a href="#CS224s-语音识别" class="headerlink" title="CS224s 语音识别"></a>CS224s 语音识别</h3><p>课程主页：<a href="http://web.stanford.edu/class/cs224s/">http://web.stanford.edu/class/cs224s/</a><br>课件地址：<a href="http://web.stanford.edu/class/cs224s/syllabus.html">http://web.stanford.edu/class/cs224s/syllabus.html</a><br>相关链接：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/39387424">https://zhuanlan.zhihu.com/p/39387424</a></li><li><a href="https://zhuanlan.zhihu.com/p/92812997">https://zhuanlan.zhihu.com/p/92812997</a></li></ul><h3 id="CS20-深度学习平台Tensorflow"><a href="#CS20-深度学习平台Tensorflow" class="headerlink" title="CS20 深度学习平台Tensorflow"></a>CS20 深度学习平台Tensorflow</h3><p>课程主页：<a href="http://web.stanford.edu/class/cs20si/">http://web.stanford.edu/class/cs20si/</a><br>相关连接：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/42422376">https://zhuanlan.zhihu.com/p/42422376</a></li><li><a href="https://zhuanlan.zhihu.com/p/33412526">https://zhuanlan.zhihu.com/p/33412526</a></li><li><a href="http://txshi-mt.com/2018/02/15/CS20-8-Style-Transfer/">http://txshi-mt.com/2018/02/15/CS20-8-Style-Transfer/</a></li><li><a href="https://zhuanlan.zhihu.com/p/34471982">https://zhuanlan.zhihu.com/p/34471982</a></li><li><a href="https://github.com/tensorflow/tensor2tensor">https://github.com/tensorflow/tensor2tensor</a></li></ul><h2 id="卡内基·梅隆大学"><a href="#卡内基·梅隆大学" class="headerlink" title="卡内基·梅隆大学"></a>卡内基·梅隆大学</h2><h3 id="CMU10701-机器学习"><a href="#CMU10701-机器学习" class="headerlink" title="CMU10701 机器学习"></a>CMU10701 机器学习</h3><p>课程主页：<a href="http://www.cs.cmu.edu/~tom/10701_sp11/">http://www.cs.cmu.edu/~tom/10701_sp11/</a><br>相关链接：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/36707683">https://zhuanlan.zhihu.com/p/36707683</a></li></ul><h3 id="CMU16720-计算机视觉"><a href="#CMU16720-计算机视觉" class="headerlink" title="CMU16720 计算机视觉"></a>CMU16720 计算机视觉</h3><p>相关链接：</p><ul><li><a href="http://16720.courses.cs.cmu.edu/">http://16720.courses.cs.cmu.edu/</a></li><li><a href="http://www.andrew.cmu.edu/course/16-720/">http://www.andrew.cmu.edu/course/16-720/</a></li></ul><h3 id="CMU11785-深度学习介绍"><a href="#CMU11785-深度学习介绍" class="headerlink" title="CMU11785 深度学习介绍"></a>CMU11785 深度学习介绍</h3><p>课程主页：<a href="http://deeplearning.cs.cmu.edu/">http://deeplearning.cs.cmu.edu/</a><br>相关链接：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/79142469">https://zhuanlan.zhihu.com/p/79142469</a></li></ul><h3 id="CMU11642-搜索引擎"><a href="#CMU11642-搜索引擎" class="headerlink" title="CMU11642 搜索引擎"></a>CMU11642 搜索引擎</h3><p>课程主页：<a href="https://boston.lti.cs.cmu.edu/classes/11-642/">https://boston.lti.cs.cmu.edu/classes/11-642/</a><br>相关链接：</p><ul><li><a href="http://shuang0420.com/works/">http://shuang0420.com/works/</a></li></ul><h3 id="CMU10605-大数据中的机器学习"><a href="#CMU10605-大数据中的机器学习" class="headerlink" title="CMU10605 大数据中的机器学习"></a>CMU10605 大数据中的机器学习</h3><p>视频地址<br>课件地址</p><h3 id="CMU11611-自然语言处理"><a href="#CMU11611-自然语言处理" class="headerlink" title="CMU11611 自然语言处理"></a>CMU11611 自然语言处理</h3><p>课程主页：<a href="http://demo.clab.cs.cmu.edu/NLP/">http://demo.clab.cs.cmu.edu/NLP/</a><br>相关链接：</p><ul><li><a href="http://shuang0420.com/works/">http://shuang0420.com/works/</a></li></ul><h3 id="CMU36705-中级统计"><a href="#CMU36705-中级统计" class="headerlink" title="CMU36705 中级统计"></a>CMU36705 中级统计</h3><p>视频地址<br>课件地址</p><h3 id="CMU10703-深度强化学习和控制"><a href="#CMU10703-深度强化学习和控制" class="headerlink" title="CMU10703 深度强化学习和控制"></a>CMU10703 深度强化学习和控制</h3><p>视频地址<br>课件地址</p><h3 id="CMU10708-概率图模型"><a href="#CMU10708-概率图模型" class="headerlink" title="CMU10708 概率图模型"></a>CMU10708 概率图模型</h3><p>课程主页：<a href="http://www.cs.cmu.edu/~epxing/Class/10708-14/lecture.html">http://www.cs.cmu.edu/~epxing/Class/10708-14/lecture.html</a><br>视频地址：<a href="https://www.kanbilibili.com/video/av30770453/">https://www.kanbilibili.com/video/av30770453/</a></p><h2 id="伯克利大学"><a href="#伯克利大学" class="headerlink" title="伯克利大学"></a>伯克利大学</h2><h3 id="CS189-机器学习"><a href="#CS189-机器学习" class="headerlink" title="CS189 机器学习"></a>CS189 机器学习</h3><p>视频地址<br>课件地址</p><h3 id="EECS126-概率的应用"><a href="#EECS126-概率的应用" class="headerlink" title="EECS126 概率的应用"></a>EECS126 概率的应用</h3><p>视频地址<br>课件地址</p><h3 id="EECS127-优化模型"><a href="#EECS127-优化模型" class="headerlink" title="EECS127 优化模型"></a>EECS127 优化模型</h3><p>视频地址<br>课件地址</p><h3 id="CS182-深度学习"><a href="#CS182-深度学习" class="headerlink" title="CS182 深度学习"></a>CS182 深度学习</h3><p>视频地址<br>课件地址</p><h3 id="CS294-112-深度强化学习"><a href="#CS294-112-深度强化学习" class="headerlink" title="CS294-112 深度强化学习"></a>CS294-112 深度强化学习</h3><p>视频地址：<a href="https://www.kanbilibili.com/video/av20957290/">https://www.kanbilibili.com/video/av20957290/</a><br>相关链接：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/33855389">https://zhuanlan.zhihu.com/p/33855389</a></li><li><a href="https://zhuanlan.zhihu.com/p/32530166">https://zhuanlan.zhihu.com/p/32530166</a></li><li><a href="https://zhuanlan.zhihu.com/p/46684216">https://zhuanlan.zhihu.com/p/46684216</a></li><li><a href="https://zhuanlan.zhihu.com/p/76947371">https://zhuanlan.zhihu.com/p/76947371</a></li></ul><h3 id="CS188-人工智能基础"><a href="#CS188-人工智能基础" class="headerlink" title="CS188 人工智能基础"></a>CS188 人工智能基础</h3><p>课程主页：<a href="http://inst.eecs.berkeley.edu/~cs188/fa19/">http://inst.eecs.berkeley.edu/~cs188/fa19/</a><br>相关链接：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/61895500">https://zhuanlan.zhihu.com/p/61895500</a></li><li><a href="https://zhuanlan.zhihu.com/p/53745278">https://zhuanlan.zhihu.com/p/53745278</a></li><li><a href="https://zhuanlan.zhihu.com/p/55066572">https://zhuanlan.zhihu.com/p/55066572</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 资源 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 学习资料 </tag>
            
            <tag> 课程资源 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224n-lecture1 Introduction and Word Vectors</title>
      <link href="posts/2878d2b0.html"/>
      <url>posts/2878d2b0.html</url>
      
        <content type="html"><![CDATA[<blockquote><p> CS224n 深度学习自然语言处理 2019 版 Lecture-1 学习笔记。 </p></blockquote><p>这次的课程相比以往没有那么多的介绍，而是简短的介绍了人类语言的作用和特殊之外就开始讲主要课程。所以这里也不说多余的废话，直接进入主题。</p><h2 id="Human-language-and-word-meaning"><a href="#Human-language-and-word-meaning" class="headerlink" title="Human language and word meaning"></a>Human language and word meaning</h2><h3 id="How-do-we-represent-the-meaning-of-a-word"><a href="#How-do-we-represent-the-meaning-of-a-word" class="headerlink" title="How do we represent the meaning of a word?"></a>How do we represent the meaning of a word?</h3><p>词是自然语言组成的基本单位（汉语体系是以字为基本单位），它表达了最基本的意思，通过不同词的不同排列组合才形成了我们丰富的语言世界。所以想让机器了解自然语言，首先要解决最基本的问题就是要让机器明白词的含义<strong>meaning of a word</strong>。</p><h3 id="WordNet"><a href="#WordNet" class="headerlink" title="WordNet"></a>WordNet</h3><blockquote><p>WordNet是由Princeton 大学的心理学家，语言学家和计算机工程师联合设计的一种基于认知语言学的英语词典。它不是光把单词以字母顺序排列，而且按照单词的意义组成一个“单词的网络”。– 百度百科</p></blockquote><p>说白了$WordNet$是一个网络词典，包含同义词集和上位词(“is a”关系) <strong>synonym sets and hypernyms</strong>。在$WordNet$中，用一个词的同义词和上位词来表示这个词的意思。</p><p>我们可以通过Python库来访问WordNet，看看它具体是什么样子的。</p><blockquote><p>让jupter-notebook使用conda的虚拟环境：conda install nb_conda</p><p>通过nltk访问wordnet之前，需要执行nltk.download(‘wordnet’)去下载对应的数据。</p></blockquote><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> nltk<span class="token punctuation">.</span>corpus <span class="token keyword">import</span> wordnet <span class="token keyword">as</span> wnposes <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; 'n':'noun', 'v':'verb', 's':'adj (s)', 'a':'adj', 'r':'adv'&amp;#125;</span><span class="token keyword">for</span> synset <span class="token keyword">in</span> wn<span class="token punctuation">.</span>synsets<span class="token punctuation">(</span><span class="token string">"good"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"&amp;#123;&amp;#125;: &amp;#123;&amp;#125;"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>poses<span class="token punctuation">[</span>synset<span class="token punctuation">.</span>pos<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            <span class="token string">", "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span>l<span class="token punctuation">.</span>name<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> l <span class="token keyword">in</span> synset<span class="token punctuation">.</span>lemmas<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>同义词集效果如下：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/CDN/img/img_cs224n_19_lec1_wn_1.png" alt=""></p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> nltk<span class="token punctuation">.</span>corpus <span class="token keyword">import</span> wordnet <span class="token keyword">as</span> wnpanda <span class="token operator">=</span> wn<span class="token punctuation">.</span>synset<span class="token punctuation">(</span><span class="token string">"panda.n.01"</span><span class="token punctuation">)</span>hyper <span class="token operator">=</span> <span class="token keyword">lambda</span> s<span class="token punctuation">:</span> s<span class="token punctuation">.</span>hypernyms<span class="token punctuation">(</span><span class="token punctuation">)</span>list<span class="token punctuation">(</span>panda<span class="token punctuation">.</span>closure<span class="token punctuation">(</span>hyper<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>上位词效果如下：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n-19-lec1-wn-2.png" alt=""></p><p>NLTK：Natural Language Toolkit，自然语言处理工具包，在NLP领域中，最常使用的一个Python库。</p><p>$WordNet$作为资源库很好，但是有一些缺点：</p><ul><li>缺少语义差别，如上面实验的”proficient”被列为“good”的同义词，只是在某些场景下是可行的。</li><li>缺少新词或者词的新含义，需要不断的去更新。</li><li>主观的，是通过建立者的主观意识创建的。</li><li>需要人类劳动来创造和调整。</li><li>无法计算单词相似度。</li></ul><h3 id="独热编码（one-hot）"><a href="#独热编码（one-hot）" class="headerlink" title="独热编码（one-hot）"></a>独热编码（one-hot）</h3><p>在传统的自然语言处理中，我们把词语看作离散的符号: hotel, conference, motel - a <strong>localist</strong> representation。单词可以通过独热向量（one-hot vectors，只有一个1，其余均为0的稀疏向量）。</p><center> $motel=[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]$</center><center>$hotel=[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]$</center> </br>向量的维度等于词库的词个数。虽然 one-hot 编码可以用作词的向量表示，但是弊端也是很明显的：<ul><li>当语料库也就是词库的单词个数过大时，one-hot 编码的词向量的维度也会很大。</li><li>由于one-hot 编码都是由0-1组成，并且只有表示该词的位置是１，其余位置都是０，所以过于稀疏。</li><li>并且用one-hot 编码的词之间都是正交的（两个词向量的内积为０），所以无法表示两个词的相关性。</li></ul><h3 id="通过上下文表示词"><a href="#通过上下文表示词" class="headerlink" title="通过上下文表示词"></a>通过上下文表示词</h3><blockquote><p>分布式语义：一个单词的意思是由经常出现在它附近的单词给出的。</p></blockquote><p>这个很容易理解，一个词实际所表达的意思往往取决于它所在的句子。有点物以类聚，人以群分的感觉。这个概念被称为现代统计$NLP$最成功的理念之一，所以才有了后来的$Word2Vec$词嵌入框架。</p><p>当一个单词$w$出现在文本中时，它的上下文$context$是出现在其附近的一组单词（在一个固定大小的窗口$Window$中）。在大量的语料库中，词$w$会出现在不同的语句中，所以也就有了许多不同的上下文$context$。我们可以通过这些$context$去得到该词的有效表示。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n-19-lec1-context.png" alt=""></p><h3 id="词向量（Word-Vector）"><a href="#词向量（Word-Vector）" class="headerlink" title="词向量（Word Vector）"></a>词向量（Word Vector）</h3><p>词向量也叫词的表示（word representations）或者词嵌入（word embeddings），它是一种分布式表示。上文说的独热和通过上下文表示都属于分布式表示，但是独热编码的方式是稀疏的高纬的，而通过上下文表示得到的向量是低纬度的稠密的（dense）。我们希望在相似的$context$下的$word vector$也较为相似。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec1_dens_vector.png" alt=""></p><p>词向量在NLP中非常重要，一个训练好的词向量模型，可以很好的表达出词与词之间的关系。使得可以很好的进行下游任务的处理，有助于提高模型的性能和准确率。</p><h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><h3 id="Word2vec-introduction"><a href="#Word2vec-introduction" class="headerlink" title="Word2vec introduction"></a>Word2vec introduction</h3><p>$Word2Vec(Mikolov et al. 2013)$是一个学习词向量的框架，通过模型将自然语言的单词映射到n维空间中，这个n就是词向量的维度。在该空间中，语义相近的词向量位置相对比较接近。</p><p>它的主要思路是：</p><ul><li>我们有大量的语料文本 (corpus means ‘body’ in Latin. 复数为corpora)。</li><li>固定词汇表中的每个单词都由一个向量表示。</li><li>文本中的每个位置$t$，其中有一个中心词$c$和上下文$context$单词$o$。</li><li>使用$c$和$o$的词向量的相似性来计算给定$c$的$o$的概率$P(o|c)$（反之亦然）。</li><li>不断调整词向量来最大化这个概率。</li></ul><p>下图为窗口大小$j=2$时的$P(w_t+j|w_t)$计算过程，center word分别为$into$和$banking$。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec1_w2v_ov.png" alt=""></p><p>当我们扫到下一个位置时，banking就成为center word。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n_19_lec1_w2v_ov1.png" alt=""></p><h3 id="Word2vec-objective-function"><a href="#Word2vec-objective-function" class="headerlink" title="Word2vec objective function"></a>Word2vec objective function</h3><p>对于每个位置$t=1,\dots,T$，其中$T$为一句话中单词的个数。在大小为$m$的固定窗口$Window$内预测上下文单词，给定中心词$w_j$。</p><center>$Likelihood = L(\theta) = \prod_{t=1}^T \prod_{-m \leq j \leq m , j\neq 0} P(w_{t+j}|w_t;\theta)$</center></br>其中，$\theta$是所有需要优化的变量。<p>目标函数$J(\theta)$也叫代价函数或者损失函数。上述公式中求乘的方式最后得到一个非常小的值，因为每个概率$P$都是小于１大于０的小数，通过不断相乘（我们知道小于１的小数乘以一个小于１的小数会比这两个小数值更小）最后得到一个非常小的小数。所以我们通常会转为求对数，也就是在上述公式的两边加上$Log$，其中右边就可以转化为对数求和的形式。同时根据凸优化理论，我们将求最大化转为求最小化，变形后的目标函数为（平均）负对数似然：</p><center>$J(\theta) = -{1\over T}LogL(\theta) = -{1\over T}\sum^T_{t=1} \sum_{-m\leq j\leq m,j\neq 0}LogP(w_{t+j}|w_t;\theta)$</center></br>那么问题来了，我们如何计算$P(w_{t+j}|w_t)$呢？答案是使用$Softmax$函数来计算概率。<h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h3><p>在这里对于每个单词$w$使用两个向量：</p><ul><li>$v_w$当$w$是中心词的时候。</li><li>$u_w$当$w$是上下文词的时候。</li></ul><p>然后对于一个中心词$c$和一个上下文词$o$的概率$P$：</p><center> $P(o|c) = {exp(u_o^T v_c)\over \sum_{w\in V}exp(u_w^T v_c)}$</center></br>其中，$T$表示的是向量$u_o$的转置，而不是上文所代表的单词数量T。<center> $u^T v = u\cdot v = \sum_{i=1}^n u_i v_i$</center> </br>公式中向量$u_o$和向量$v_c$进行了点乘来计算词向量之间的相似度，向量之间相似度越高点乘的结果越大。模型的训练正是为了使得具有相似上下文的单词，具有相似的向量。<blockquote><p>两个向量内积的几何含义是什么</p><p>定义：两个向量a与b的内积为 a·b = |a||b|cos∠(a, b)，特别地，0·a =a·0 = 0；若a，b是非零向量，则a与b正交的充要条件是a·b = 0。</p></blockquote><p>分子中加上$exp$指数，一是为了防止内积为负数，二是为了使得内积大的概率值更大，内积小的概率值更小。</p><p>分母中对整个词汇进行归一化以给出概率分布，其中$V$是整个词汇表中单词的个数。</p><p>上述的内容就是一个$softmax$函数的应用例子。</p><center>$softmax(x_i) = {exp(x_i) \over \sum_{j=1}^n exp(x_j)} = p_i$</center></br>$softmax$函数将一个值$x_i$映射成对应的概率值$p_i$。<ul><li><strong>max</strong> ：因为放大了最大的概率。</li><li><strong>soft</strong> ：因为仍然为较小的$x_i$赋予了一定概率。</li><li>$softmax$通常用于深度学习中。</li></ul><h3 id="Training-a-model-by-optimizing-parameters"><a href="#Training-a-model-by-optimizing-parameters" class="headerlink" title="Training a model by optimizing parameters"></a>Training a model by optimizing parameters</h3><p>有了目标函数，我们就可以通过梯度下降法来优化参数$\theta$，在这里$\theta$就是我们的词向量，也就是模型中所有的参数。比如，我们有$V$个单词，每个单词取$d$维度。那么$\theta$可以表示为：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_cs224n-19-lec1-theta.png" alt=""></p><p>要记住的是这里每个单词都有两个向量作为中心词时的$v_w$和作为上下文词时的$u_w$。在训练时首先要随机初始化两个向量，通过梯度下降法不断更新向量，最后取平均值来表示该词的词向量。</p><h2 id="Word2vec-objective-function-gradients"><a href="#Word2vec-objective-function-gradients" class="headerlink" title="Word2vec objective function gradients"></a>Word2vec objective function gradients</h2><h3 id="储备知识"><a href="#储备知识" class="headerlink" title="储备知识"></a>储备知识</h3><p>现在我们来求解目标函数的梯度，这需要用到一些高等数学和线性代数的知识。</p><ul><li>求偏导数</li><li>对数的导数求法</li><li>指数的导数求法</li><li>链式求导法则</li></ul><h3 id="Calculating-all-gradients"><a href="#Calculating-all-gradients" class="headerlink" title="Calculating all gradients"></a>Calculating all gradients</h3><p>根据求导法则偏导数可以移进求和中：</p><center>${\partial \over \partial x }\sum_i y_i = \sum_i { {\partial \over \partial x} y_i}$ </center></br>所以我们对$J(\theta)$求偏导可以只关注累加内部的$P$的求导，最后将前面的两个累加填上去就可以了。<p>先求中心词$v_c$的偏导：</p><center> ${\partial \over \partial v_c} log P(o|c)={\partial \over \partial v_c}log{exp(u^T_o v_c)\over {\sum_{w\in V} exp(u^T_w v_c)}}$</center><center> $ = {\partial \over \partial v_c} {(log exp(u^T_o v_c) - log\sum_{w\in V}exp(u^T_w v_c))} $</center><center> $ = { {\partial \over \partial v_c}(u^T_o v_c - log\sum_{w\in V}exp(u^T_w v_c))}$</center><center> $ = {u_o - {\sum_{w\in V}exp(u^T_w v_c)u_w \over \sum_{w\in V}exp(u^T_w v_c)}}$</center><center> $ = {u_o - \sum_{w\in V}{exp(u^T_w v_c)\over \sum_{w\in V}exp(u^T_w) v_c} u_w}$</center><center> $ = {u_o - \sum_{w\in V}P(w|c)u_w}$</center></br>再求上下文词$u_o$的偏导：<center>${\partial \over \partial u_o} log P(o|c)={\partial \over \partial u_o}log{exp(u^T_o v_c)\over {\sum_{w\in V} exp(u^T_w v_c)}}$</center><center>$ = {\partial \over \partial u_o} {(log exp(u^T_o v_c) - log\sum_{w\in V}exp(u^T_w v_c))} $</center><center>$ = { {\partial \over \partial u_o}(u^T_o v_c - log\sum_{w\in V}exp(u^T_w v_c))}$</center><center>$ = {v_c - {log\sum_{w\in V}{\partial \over \partial u_o} exp(u^T_w v_c) \over \sum_{w\in V}exp(u^T_w v_c)}}$</center><center>$ = {v_c - {exp(u^T_o v_c) v_c\over \sum_{w\in V}exp(u^T_w v_c)}}$</center><center>$ = {v_c - {exp(u^T_o v_c)\over \sum_{w\in V}exp(u^T_w v_c)}v_c}$</center><center>$ = {v_c - P(o|c)v_c}$</center><center>$ = {(1 - P(o|c))v_c}$</center> </br>这样我们就得到了某一时刻的中心词和上下文词的梯度，这样通过下面的公式去更新梯度也就是对应的词向量：<center>$\theta ^{new}_j = \theta ^{old}_j - \alpha {\partial\over \partial \theta _j^{old}}J(\theta)$</center></br>## 总结<p>这里的$word2vec$算法又被叫做Skip-Gram model，还有另一种$word2vec$算法是Continuous Bag of Words，简称$CBOW$，它们的原理区别是Skip-Gram是求context word相对于center word的条件概率，也就是知道通过中心词求上下文词。而$CBOW$是求center相对于context word的条件概率，也就是通过上下文词求中心词。其他方面基本类似。</p><p>加快训练的$trick$有负采样（Negative sampling）和层次$Softmax$。</p><h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><p><a href="https://hiyoungai.com/posts/fcba888f.html">论文阅读《Efficient Estimation of Word Representations in Vector Space》</a><br><a href="https://hiyoungai.com/posts/28911bbb.html">论文阅读《Distributed Representations of Words and Phrases and their Compositionality》</a></p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> CS224n </tag>
            
            <tag> Word2vec </tag>
            
            <tag> 词向量 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习 - 线性回归</title>
      <link href="posts/19883263.html"/>
      <url>posts/19883263.html</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>今天要说一下机器学习中大多数书籍第一个讲的（有的可能是KNN）模型-线性回归。说起线性回归，首先要介绍一下机器学习中的两个常见的问题：回归任务和分类任务。那什么是回归任务和分类任务呢？简单的来说，在监督学习中（也就是有标签的数据中），标签值为连续值时是回归任务，标志值是离散值时是分类任务。而线性回归模型就是处理回归任务的最基础的模型。</p><h2 id="形式"><a href="#形式" class="headerlink" title="形式"></a>形式</h2><p>在只有一个变量的情况下，线性回归可以用方程：$y = ax+b$表示。而如果有多个变量，也就是n元线性回归的形式如下：</p><center> $h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \cdots$ <br><br> </center ><center> $h_\theta(x) = \sum^n_{i=0}{\theta_ix_i} = {\theta^Tx}$<br><br> </center >在这里我们将截断$b$用$\theta_0$代替，同时数据集X也需要添加一列1用于与$\theta_0$相乘，表示$+b$。最后写成矩阵的形式就是$\theta$的转置乘以x。其中如果数据集有n个特征，则$\theta$就是$n+1$维的向量并非矩阵，其中包括截断$b$。<h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><p>线性回归的目的就是求解出合适的$\theta$，在一元的情况下拟合出一条直线（多元情况下是平面或者曲面），可以近似的代表各个数据样本的标签值。所以最好的直线要距离各个样本点都很接近，而如何求出这条直线就是本篇文章重点要将的内容。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_lr_compare.webp" alt="图1"></p><h2 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h2><p>求解线性回归模型的方法叫做最小二乘法，最小二乘法的核心就是保证所有数据偏差的平方和最小。它的具体形式是：</p><center> $J(\theta) = {1\over2}\sum^m_{i=1}(h_\theta(x^{(i)}) - y^{(i)})^2$ <br><br> </center >其中$h_\theta(x^{(i)})$代表每个样本通过我们模型的预测值，$y^{(i)}$代表每个样本标签的真实值，$m$为样本个数。因为模型预测值和真实值间存在误差$e$，可以写作：<center> $y^{(i)} = {\theta^Tx^{(i)} + \epsilon^{(i)}}$ <br><br> </center >根据中心极限定理，$e^{(i)}$是独立同分布的(IID)，服从均值为0，方差为某定值$σ$的平方的正太分布。具体推导过程如下：<p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_lr_zuixiaoercheng.webp" alt="图2"></p><h2 id="求解最小二乘法"><a href="#求解最小二乘法" class="headerlink" title="求解最小二乘法"></a>求解最小二乘法</h2><p>我们要求得就是当$\theta$取某个值时使$J(\theta)$最小，求解最小二乘法的方法一般有两种方法:矩阵式和梯度下降法。</p><h3 id="矩阵式求解"><a href="#矩阵式求解" class="headerlink" title="矩阵式求解"></a>矩阵式求解</h3><p>当我们的数据集含有m个样本，每个样本有n个特征时，数据x可以写成$m\cdot(n+1)$维的矩阵（$+1$是添加一列1，用于与截断$b$相乘），$\theta$则为$n+1$维的列向量（$+1$是截断b），y为m维的列向量代表每m个样本结果的预测值。则矩阵式的推导如下所示：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_lr_zuixiaoercheng_qiujie_juzhen.webp" alt="图3"></p><p>因为$X^TX$为方阵，如果$X^TX$是可逆的，则参数$\theta$得解析式可以写成：</p><center> $\theta = (X^TX)^{-1}X^Ty$ <br><br> </center >如果$X$的特征数n不是很大，通常情况下$X^TX$是可以求逆的，但是如果n非常大，$X^TX$不可逆，则用梯度下降法求解参数$\theta$的值。<h3 id="梯度下降法求解（GD）"><a href="#梯度下降法求解（GD）" class="headerlink" title="梯度下降法求解（GD）"></a>梯度下降法求解（GD）</h3><p>在一元函数中叫做求导，在多元函数中就叫做求梯度。梯度下降是一个最优化算法，通俗的来讲也就是沿着梯度下降的方向来求出一个函数的极小值。比如一元函数中，加速度减少的方向，总会找到一个点使速度达到最小。通常情况下，数据不可能完全符合我们的要求，所以很难用矩阵去求解，所以机器学习就应该用学习的方法，因此我们采用梯度下降，不断迭代，沿着梯度下降的方向来移动，求出极小值。梯度下降法包括批量梯度下降法和随机梯度下降法（SGD）以及二者的结合mini批量下降法（通常与SGD认为是同一种，常用于深度学习中）。</p><p>梯度下降法的一般过程如下：</p><ol><li>初始化$\theta$（随机）</li><li>求$J(\theta)$对$\theta$的偏导：</li></ol><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_lr_zuixiaoercheng_gd.webp" alt="图4"></p><ol start="3"><li><p>更新$\theta$</p><center> $\theta = \theta - \alpha \cdot {\partial J(\theta)\over{\partial \theta}}$ <br><br> </center ></li></ol><p>其中$\alpha$为学习率，调节学习率这个超参数也是建模中的一个重要内容。因为$J(\theta)$是凸函数，所以GD求出的最优解是全局最优解。</p><p>批量梯度下降法是求出整个数据集的梯度，再去更新$\theta$ ，所以每次迭代都是在求全局最优解。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_lr_gd_1.webp" alt="图5"></p><p>而随机梯度下降法是求一个样本的梯度后就去跟新$\theta$，所以每次迭代都是求局部最优解，但是总是朝着全局最优解前进，最后总会到达全局最优解。</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_lr_gd_2.webp" alt="图6"></p><h2 id="其他回归模型"><a href="#其他回归模型" class="headerlink" title="其他回归模型"></a>其他回归模型</h2><p>在机器学习中，有时为了防止模型太复杂容易过拟合，通常会在模型上加入正则项，抑制模型复杂度，防止过拟合。在线性回归中有两种常用的正则，一个是$L1$正则，一个是$L2$正则，加入$L1$正则的称为$Lasso$回归，加入$L2$正则的为$Ridge$回归也叫岭回归。</p><h3 id="Lasso回归"><a href="#Lasso回归" class="headerlink" title="Lasso回归"></a>Lasso回归</h3><center> $J({\vec\theta}) = {1\over2}\sum^m_{i=1}(h_{\vec\theta}(x^{(i)}) - y^{(i)}) + \lambda\sum^n_{j=1}{|\theta_j|}$ <br><br> </center>-- -- --<h3 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h3><center> $J({\vec\theta}) = {1\over2}\sum^m_{i=1}(h_{\vec\theta}(x^{(i)}) - y^{(i)}) + \lambda\sum^n_{j=1}{\theta_j^2}$ <br><br> </center>-- -- --<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>下图是个人实现代码结果与真实值对比图：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_lr_compare_result.webp" alt="图7"></p><p>详细代码可参考<a href="https://github.com/hiyoung123/ML">GitHub</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 线性回归 </tag>
            
            <tag> 岭回归 </tag>
            
            <tag> Lasso回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习 - Logistic回归</title>
      <link href="posts/c237bc03.html"/>
      <url>posts/c237bc03.html</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Logistic回归是机器学习中最常用最经典的分类方法之一，有的人称为逻辑回归或逻辑斯蒂回归。虽然它称为回归模型，但是却处理的是分类问题，这主要是因为它的本质是一个线性模型加上一个映射函数sigmoid，将线性模型得到的连续结果映射到离散型上。它常用于二分类问题，在多分类问题的推广叫做softmax。 </p><h2 id="Logisitc回归"><a href="#Logisitc回归" class="headerlink" title="Logisitc回归"></a>Logisitc回归</h2><p>由于Logistic回归是将线性模型的输出$ \theta x+b$经过$f(z)$数处理后，映射到离散值上形成分类问题，所以我们可以假设分类值$y={0，1}$，所以Logistic回归模型可以写成：$h(x)=f(θx+b) $，也就是当$ \theta x+b$的值大于0时$h(x)=+1$，当$θx+b$的值小于0时$h(x)=-1$。但是这样的$f(z)$函数称为单位阶跃函数，但是它的数学性质不好，不连续也不方便求导，所以我们使用它的替代函数sigmoid函数也叫s型函数，我们用$g(x)$表示。这样线性模型的输出经过sigmoid的映射就变成了求出样本属于哪一类别的概率，即$θx+b&gt;0$的话，那么样本属于分类1的概率大一点，如果$θx+b&lt;0$的话就是样本属于1的概率小属于类别0的概率大一些。图1是单位阶跃函数（红线）与sigmoid函数（黑线）。 </p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_ml_logic_sigmoid.webp" alt="图1"></p><p>sigmoid的函数表达式为： </p> <center> $y={1\over1+e^{-z}}$ <br><br> </center >其中z在Logistic回归中就是$θx+b$。那么为什么要用sigmoid函数呢？ <h2 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h2><p>从概率的角度看Logistic回归，如果将样本分为正类的概率看成$h(x)$，那么分为负类的概率就是$1-h(x)$，则Logistic回归模型的概率表达式符合$0-1$分布： </p><center> $P(y=1|x;θ) = h_θ(x)$ <br><br> </center ><center> $P(y=0|x;θ) = 1 - h_θ(x)$ <br><br> </center >对上式结合就是Logistic回归的概率分布函数，也就是从概率角度的目标函数： <center> $P(y|x;θ) = (h_θ(x))^y(1 - h_θ(x))^{1-y}$  <br><br> </center >我们对该式进行变换，可以得到指数族分布，最后可以得出函数$h(x)$就是sigmoid函数。以下是推导过程： <p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_logist_sigmoid_process.webp" alt="图2"></p><p>其中图2中的p是图4中的$h(x)$，而图2的z是线性模型的输出$θx+b$。这样从指数族分布就可以推出sigmoid函数。换一个思路，我们将一个事件发生的概率$p$与其不发生的概率$1-p$的比值叫做几率，对其取对数后称为对数几率（logit）：</p><center> $log{p\over{1-p}}$ <br><br> </center >令它等于线性函数θx+b，最后也可以推出$p$就是sigmoid函数，也就是图2的后半段，这样说明了sigmoid函数的值是概率值。另外，如果我们不让对数几率函数等于线性函数，让他等于其他的函数呢？这也是可以的，只不过是sigmoid函数中$z$的表达方式改变而已。 <h2 id="求解Logistic回归模型参数"><a href="#求解Logistic回归模型参数" class="headerlink" title="求解Logistic回归模型参数"></a>求解Logistic回归模型参数</h2><p> 我们重新整理一下Logistic回归的目标函数，他的最终形式为： </p><center> $h_θ(x) = g(θ^Tx) = {1\over{1 + e^{-θ^Tx}}}$ <br><br> </center >因为这是一个概率问题，所以我们可以使用极大似然估计的方式求解Logistic回归的参数$θ$。以下是求导过程： <p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_loggist_process.webp" alt="图3"></p><p> 其中$g()$函数是sigmoid函数，它的导数为： </p><center> $g\prime(x) = ({1\over{1 + e^{-x}}})\prime= {e^{-x}\over(1 + e^{-x})^2}$ <br><br> </center ><center> $= {1\over{1 + e^{-x}}}\cdot{e^{-x}\over{1 + e^{-x}}} = {1\over{1 + e^{-x}}}\cdot(1 - {1\over{1 + e^{-x}}})$ <br><br> </center ><center> $= {g(x)\cdot(1 - g(x))}$ <br><br> </center >这样图3得到的结果就是关于$θ$的梯度，我们通过梯度提升算法（因为目标函数是最大似然估计，求极大值所以用梯度上升，如果想用梯度下降，可以对似然函数取负就是求极小值）更新$θ$，最后就求出Logistic回归模型的参数$θ$，这与线性回归方法相同（有没有发现他们的更新梯度的目标函数也相同）。 <center> $\theta_j:= \theta_j +  \alpha (y^{(i)} - h_\theta(x^{(i)}))x_j^{(i)}$ <br><br> </center >以上就是Logistic回归模型的建立与参数估计过程，下面我们要说一下他在多分类问题中的推广-----softmax回归。 <h2 id="Softmax函数"><a href="#Softmax函数" class="headerlink" title="Softmax函数"></a>Softmax函数</h2><p>Softmax与Logistic回归的主要区别就是，Logistic处理二分类问题，只有一组权重参数$θ$。而softmax处理多分类问题，如果有k个类别，那么Softmax就有k组权值参数。每组权值对应一种分类，通过k组权值求解出样本数据对应每个类别的概率，最后取概率最大的类别作为该数据的分类结果。它的概率函数为： </p><center> $p(c=k|x;\theta) = {exp(\theta^T_kx)\over{\sum^k_{I=1}exp(\theta^T_ix)}},k = 1,2,3\cdots$ <br><br> </center >Softmax经常用于神经网络的最后一层，用于对神经网络已经处理好的特征进行分类。<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>个人实现了一个二分类的逻辑回归，并与sklearn中的logistic回归做对比：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_logist_compare_result.webp" alt="图4"></p><p>数据只使用了鸢尾花数据的0/1两个类别，由于本代码实现的比较简单，只能处理类别为0/1的数据，有兴趣的朋友可以自己做补充，本代码只做参考。 </p><p>详细代码可参考<a href="https://github.com/hiyoung123/ML">Github</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 逻辑回归 </tag>
            
            <tag> Logistic </tag>
            
            <tag> Sigmoid </tag>
            
            <tag> Softmax </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>压缩数据</title>
      <link href="posts/4b093d11.html"/>
      <url>posts/4b093d11.html</url>
      
        <content type="html"><![CDATA[<p>压缩数据的原因主要有两点：节省保存信息所需的空间和节省传输信息所需的时间。我们将学习的算法之所以能够节省空间，是因为大多数数据文件都有很大的冗余。</p><p>我们将会讨论广泛应用的一种初级算法和两种高级算法。这些算法的压缩效果可能不同，取决于输入的特征。文本数据一般能节省 20% ~ 50% 的空间，某些情况下能够达到 50% ~ 90%。</p><blockquote><p>本文提到的性能（对于数据压缩），性能指代的是算法的压缩率，也会考虑压缩用时。</p></blockquote><h2 id="基础模型"><a href="#基础模型" class="headerlink" title="基础模型"></a>基础模型</h2><p>数据压缩的基础模型主要由两部分组成，两者都是一个能读写比特流的黑盒子：</p><ul><li>压缩盒：能够将一个比特流B转化为压缩后的版本C(B)。</li><li>展开盒：能够将C(B)转化回B。</li></ul><p>如果使用|B|表示比特流中比特的数量的话，我们感兴趣的是将|C(B)|/|B|最小化，这个值被称为压缩率。</p><p>待添加图片</p><p>这种模型叫做无损压缩模型－保证不丢失任何信息，即压缩和展开之后的比特流必须和原始的比特流完全相同。许多种类型的文件都会使用无损压缩，    如果数值数据或者可执行代码。对于某些类型的文件，如图像视频和音乐，有损压缩也是能接受的。此时解释器产生的输出只是与原输入的文件近似。有损压缩算法的评价标准不仅是压缩率，还包括主管的质量感受。</p><h2 id="压缩的局限"><a href="#压缩的局限" class="headerlink" title="压缩的局限"></a>压缩的局限</h2><h3 id="通用数据压缩"><a href="#通用数据压缩" class="headerlink" title="通用数据压缩"></a>通用数据压缩</h3><p>通用性的数据压缩算法是指一个能够缩小任意比特流的算法。但是这样的算法是不存在的：</p><ul><li><p>反证法</p><p>假设存在通用压缩算法，那么说明可以用它压缩它自己的输出，从而得到一个更短的比特流，循环直到比特流的长度为0，显然是错误的。</p></li><li><p>统计法</p><p>后续讲解</p></li></ul><p>根据统计法可以得出，对于任意数据压缩算法，将长度1000位的随机比特流压缩为一半的概率最多为$1/2^{500}$</p><h3 id="不可判定性"><a href="#不可判定性" class="headerlink" title="不可判定性"></a>不可判定性</h3><blockquote><p>压缩一个文件最好的办法就是找出创造这些数据的程序。</p></blockquote><p>可以证明最优数据压缩（找到能够产生给定字符串的最短程序）是一个不可能判定的问题：我们不但不可能找到能够压缩任意比特流的算法，也不可能找到最佳的压缩算法。</p><p>这些局限性所带来的实际影响要求无损压缩算法必须尽量利用被压缩的数据流中的已知结构：</p><ul><li>小规模的字母表</li><li>较长的连续相同的位或字符</li><li>频繁使用的字符</li><li>较长的连续重复的位或字符</li></ul><h3 id="游程编码"><a href="#游程编码" class="headerlink" title="游程编码"></a>游程编码</h3><h3 id="霍夫曼压缩"><a href="#霍夫曼压缩" class="headerlink" title="霍夫曼压缩"></a>霍夫曼压缩</h3><h3 id="LZW压缩算法"><a href="#LZW压缩算法" class="headerlink" title="LZW压缩算法"></a>LZW压缩算法</h3>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 压缩数据 </tag>
            
            <tag> 游程编码 </tag>
            
            <tag> 霍夫曼编码 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Machine Learning Yearning》阅读笔记</title>
      <link href="posts/7f3faaf.html"/>
      <url>posts/7f3faaf.html</url>
      
        <content type="html"><![CDATA[<h2 id="机器学习为什么需要策略"><a href="#机器学习为什么需要策略" class="headerlink" title="机器学习为什么需要策略"></a>机器学习为什么需要策略</h2><h2 id="如何使用这本书帮助你的团队"><a href="#如何使用这本书帮助你的团队" class="headerlink" title="如何使用这本书帮助你的团队"></a>如何使用这本书帮助你的团队</h2><h2 id="先决条件和符号"><a href="#先决条件和符号" class="headerlink" title="先决条件和符号"></a>先决条件和符号</h2><h2 id="规模驱动机器学习进程"><a href="#规模驱动机器学习进程" class="headerlink" title="规模驱动机器学习进程"></a>规模驱动机器学习进程</h2><h2 id="你的开发集和测试集"><a href="#你的开发集和测试集" class="headerlink" title="你的开发集和测试集"></a>你的开发集和测试集</h2><h2 id="开发集合和测试集合应该来自统一分布"><a href="#开发集合和测试集合应该来自统一分布" class="headerlink" title="开发集合和测试集合应该来自统一分布"></a>开发集合和测试集合应该来自统一分布</h2><h2 id="开发集合和测试集合应该有多大规模"><a href="#开发集合和测试集合应该有多大规模" class="headerlink" title="开发集合和测试集合应该有多大规模"></a>开发集合和测试集合应该有多大规模</h2><h2 id="建立一个单值评估指标去优化"><a href="#建立一个单值评估指标去优化" class="headerlink" title="建立一个单值评估指标去优化"></a>建立一个单值评估指标去优化</h2><h2 id="优化指标和满意指标"><a href="#优化指标和满意指标" class="headerlink" title="优化指标和满意指标"></a>优化指标和满意指标</h2><h2 id="通过开发集和度量指标快速迭代"><a href="#通过开发集和度量指标快速迭代" class="headerlink" title="通过开发集和度量指标快速迭代"></a>通过开发集和度量指标快速迭代</h2><h2 id="什么时候修改开发集，测试集和评价指标？"><a href="#什么时候修改开发集，测试集和评价指标？" class="headerlink" title="什么时候修改开发集，测试集和评价指标？"></a>什么时候修改开发集，测试集和评价指标？</h2><h2 id="小结：建立开发集和测试集"><a href="#小结：建立开发集和测试集" class="headerlink" title="小结：建立开发集和测试集"></a>小结：建立开发集和测试集</h2><p>快速构建并且迭代你的第一个系统</p><p>误差分析：根据开发集样本去验证想法</p><p>在误差分析时并行评估多个想法</p><p>清洗误标注的开发集和测试集样本</p><p>将大型的开发集拆分为两个子集，专注其一</p><p>Eyeball 和 Blackbox 开发集该设置多大？</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 吴恩达 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何选择职业 - 一个美国高级工程师的建议</title>
      <link href="posts/97876509.html"/>
      <url>posts/97876509.html</url>
      
        <content type="html"><![CDATA[<p>今天看到一篇<a href="https://github.com/ruanyf/weekly/blob/master/docs/issue-82.md">博客</a>，是讲述了一个美国高级工程师如何选择职业的，感觉很有道理，于是摘抄过来。<a href="https://erikbern.com/2019/09/12/misc-unsolicited-career-advice.html">原文地址</a></p><blockquote><p>停滞发展、或者缓慢发展的公司，完全是一个零和游戏。</p><p>如果你想晋升，必须等别人把位置空出来。你得到的，一定是其他人失去的。相比之下，快速发展的公司有源源不断的新人加入，最终每个人都会得到晋升！</p><p>最重要的是，业务的发展比人员增长快，所以你会被“往上拉”，拉到更高层的岗位。</p></blockquote><p>作者原来是一个外行，但是通过努力，进入了一家机器学习公司，现在发展得很好。他从自己的经历，得出一个观点： <strong>就业要选择发展最快的行业</strong>。</p><p>作者以自己为例，他并没有机器学习背景，但是公司发展得太快，他需要组建团队，自然就成了团队管理者，环境把他“拉”到了更高的位置上。位阶高了，就有了更多的机会和资源。</p><p>这让我想起自己呆过的一所大学，每年学校就那么几个名额，可以晋升教授和副教授，大家挤破头，如果今年评不上，就要至少再等一年。有的老师熬到头发白了，才评上副高职称。这就是发展缓慢的结果。去了这种地方，真是消耗生命。</p><p>那篇文章还提到了另外一点，我也很赞同。他说，就业的目的是为自己积累两种资本：人力资本（增长能力）和金融资本（多赚钱）。对于年轻人来说，人力资本更重要。 <strong>就业时，年轻人的关注重点应该是，快速增长自己的人力资本。</strong> 因为长期来看，在你的一生中，人力资本会比金融资本带来更大的回报。</p><p>最快速形成人力资本的方法，就是去聪明人多的地方，从比你更聪明的人身上学习。跟高手在一起工作，你会成长得非常快。大公司虽然高手很多，但是你接触不到也没用。 <strong>在一个快速发展的行业里面，加入一群聪明人组成的小团队，可能是事业成功的最佳方式。</strong></p>]]></content>
      
      
      <categories>
          
          <category> 杂谈 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 杂谈 </tag>
            
            <tag> 职业规划 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP系列 - 基于词典的中文分词</title>
      <link href="posts/9eeee454.html"/>
      <url>posts/9eeee454.html</url>
      
        <content type="html"><![CDATA[<h2 id="中文分词概述"><a href="#中文分词概述" class="headerlink" title="中文分词概述"></a>中文分词概述</h2><p>词是最小的能够独立活动的有意义的语言成分，一般分词是自然语言处理的第一项核心技术。英文中每个句子都将词用空格或标点符号分隔开来，而在中文中很难对词的边界进行界定，难以将词划分出来。在汉语中，虽然是以字为最小单位，但是一篇文章的语义表达却仍然是以词来划分的。因此处理中文文本时，需要进行分词处理，将句子转为词的表示，这就是中文分词。</p><h2 id="中文分词的三个难题"><a href="#中文分词的三个难题" class="headerlink" title="中文分词的三个难题"></a>中文分词的三个难题</h2><p>分词规则，消除歧义和未登录词识别：</p><ul><li>构建完美的分词规则便可以将所有的句子正确的划分，但是这根本无法实现，语言是长期发展自然而然形成的，而且语言规则庞大复杂，很难做出完美的分词规则。</li><li>在中文句子中，很多词是由歧义性的，在一句话也可能有多种分词方法。比如：”结婚/的/和尚/未结婚/的“，“结婚/的/和/尚未/结婚/的”，人分辨这样的句子都是问题，更何况是机器。</li><li>此外对于未登陆词，很难对其进行正确的划分。</li></ul><h2 id="目前主流分词方法"><a href="#目前主流分词方法" class="headerlink" title="目前主流分词方法"></a>目前主流分词方法</h2><p>基于规则，基于统计以及二者混合。本篇主要介绍一下基于规则词典进行分词。</p><h2 id="基于规则的分词"><a href="#基于规则的分词" class="headerlink" title="基于规则的分词"></a>基于规则的分词</h2><p>主要是人工建立词库也叫做词典，通过词典匹配的方式对句子进行划分。其实现简单高效，但是对未登陆词很难进行处理。主要有正向最大匹配法，逆向最大匹配法以及双向最大匹配法。</p><h3 id="正向最大匹配法FMM"><a href="#正向最大匹配法FMM" class="headerlink" title="正向最大匹配法FMM"></a>正向最大匹配法FMM</h3><p><code>FMM</code>的步骤是：</p><ol><li>从左向右取待分汉语句的m个字作为匹配字段，m为词典中最长词的长度。</li><li>查找词典进行匹配。</li><li>若匹配成功，则将该字段作为一个词切分出去。</li><li>若匹配不成功，则将该字段最后一个字去掉，剩下的字作为新匹配字段，进行再次匹配。</li><li>重复上述过程，直到切分所有词为止。</li></ol><p>代码实现：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">cut</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>    result <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    index <span class="token operator">=</span> <span class="token number">0</span>    text_size <span class="token operator">=</span> len<span class="token punctuation">(</span>text<span class="token punctuation">)</span>    <span class="token keyword">while</span> text_size <span class="token operator">></span> index<span class="token punctuation">:</span>        <span class="token keyword">for</span> size <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>window_size<span class="token operator">+</span>index<span class="token punctuation">,</span>index<span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            piece <span class="token operator">=</span> text<span class="token punctuation">[</span>index<span class="token punctuation">:</span>size<span class="token punctuation">]</span>            <span class="token keyword">if</span> piece <span class="token keyword">in</span> self<span class="token punctuation">.</span>word_dict<span class="token punctuation">:</span>　<span class="token comment" spellcheck="true">#查看是否存在于词典中</span>                index <span class="token operator">=</span> size <span class="token operator">-</span> <span class="token number">1</span>                <span class="token keyword">break</span>        index <span class="token operator">=</span> index <span class="token operator">+</span> <span class="token number">1</span>        result<span class="token punctuation">.</span>append<span class="token punctuation">(</span>piece<span class="token punctuation">)</span>    <span class="token keyword">return</span> result<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>分词效果：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_nlp_fenci_fmm.png" alt="FMM分词结果"></p><h3 id="逆向最大匹配法RMM"><a href="#逆向最大匹配法RMM" class="headerlink" title="逆向最大匹配法RMM"></a>逆向最大匹配法RMM</h3><p><code>RMM</code>的基本原理与<code>FMM</code>基本相同，不同的是分词的方向与<code>FMM</code>相反。<code>RMM</code>是从待分词句子的末端开始，也就是从右向左开始匹配扫描，每次取末端m个字作为匹配字段，匹配失败，则去掉匹配字段前面的一个字，继续匹配。</p><p>代码实现：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">cut</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>    result <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    index <span class="token operator">=</span> len<span class="token punctuation">(</span>text<span class="token punctuation">)</span>    window_size <span class="token operator">=</span> min<span class="token punctuation">(</span>index<span class="token punctuation">,</span>self<span class="token punctuation">.</span>window_size<span class="token punctuation">)</span>    <span class="token keyword">while</span> index <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> size <span class="token keyword">in</span> range<span class="token punctuation">(</span>index<span class="token operator">-</span>window_size<span class="token punctuation">,</span>index<span class="token punctuation">)</span><span class="token punctuation">:</span>            piece <span class="token operator">=</span> text<span class="token punctuation">[</span>size<span class="token punctuation">:</span>index<span class="token punctuation">]</span>            <span class="token keyword">if</span> piece <span class="token keyword">in</span> self<span class="token punctuation">.</span>word_dict<span class="token punctuation">:</span>　<span class="token comment" spellcheck="true">#查看是否存在于词典中</span>                index <span class="token operator">=</span> size <span class="token operator">+</span> <span class="token number">1</span>                <span class="token keyword">break</span>        index <span class="token operator">=</span> index <span class="token operator">-</span> <span class="token number">1</span>        result<span class="token punctuation">.</span>append<span class="token punctuation">(</span>piece<span class="token punctuation">)</span>    result<span class="token punctuation">.</span>reverse<span class="token punctuation">(</span><span class="token punctuation">)</span>　<span class="token comment" spellcheck="true">#因为是从后向前分词，所以需要将结果逆序</span>    <span class="token keyword">return</span> result<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>分词效果：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_nlp_fenci_rmm.png" alt="RMM分词结果"></p><h3 id="双向最大匹配法Bi-MM"><a href="#双向最大匹配法Bi-MM" class="headerlink" title="双向最大匹配法Bi-MM"></a>双向最大匹配法Bi-MM</h3><p><code>Bi-MM</code>是将正向最大匹配法得到的分词结果和逆向最大匹配法得到的结果进行比较，然后按照最大匹配原则，选取词数切分最少的作为结果。据<code>SunM.S.</code>和<code>Benjamin K.T.(1995)</code>的研究表明，中文中<code>90.0%</code>左右的句子，正向最大匹配法和逆向最大匹配法完全重合且正确，只有大概<code>9.0%</code>的句子两种切分方法得到的结果不一样，但其中必有一个是正确的（歧义检测成功），只有不到<code>1.0%</code>的句子，使用正向最大匹配法和逆向最大匹配法的切分虽然重合但是错的，或者两种方法切分不同但结果都不对（歧义检测失败）。</p><p>双向最大匹配的规则是：</p><ol><li>如果正反向分词结果词数不同，则取分词数量少的那个。</li><li>如果分词结果词数相同：<ul><li>分词结果相同，没有歧义，返回任意一个。</li><li>分词结果不同，返回其中单字数量较少的那个。</li></ul></li></ol><p>上述例子中词数相同，但结果不同，逆向最大匹配法的分词结果单字个数是<code>1</code>，所以返回的是逆向最大匹配法的结果。</p><p>代码实现：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">cut</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>    res_fmm <span class="token operator">=</span> self<span class="token punctuation">.</span>FMM<span class="token punctuation">.</span>cut<span class="token punctuation">(</span>text<span class="token punctuation">)</span>    res_rmm <span class="token operator">=</span> self<span class="token punctuation">.</span>RMM<span class="token punctuation">.</span>cut<span class="token punctuation">(</span>text<span class="token punctuation">)</span>    <span class="token keyword">if</span> len<span class="token punctuation">(</span>res_fmm<span class="token punctuation">)</span> <span class="token operator">==</span> len<span class="token punctuation">(</span>res_rmm<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> res_fmm <span class="token operator">==</span> res_rmm <span class="token punctuation">:</span>            <span class="token keyword">return</span> res_fmm        <span class="token keyword">else</span><span class="token punctuation">:</span>            f_word_count <span class="token operator">=</span> len<span class="token punctuation">(</span><span class="token punctuation">[</span>w <span class="token keyword">for</span> w <span class="token keyword">in</span> res_fmm <span class="token keyword">if</span> len<span class="token punctuation">(</span>w<span class="token punctuation">)</span><span class="token operator">==</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>            r_word_count <span class="token operator">=</span> len<span class="token punctuation">(</span><span class="token punctuation">[</span>w <span class="token keyword">for</span> w <span class="token keyword">in</span> res_rmm <span class="token keyword">if</span> len<span class="token punctuation">(</span>w<span class="token punctuation">)</span><span class="token operator">==</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>            <span class="token keyword">return</span> res_fmm <span class="token keyword">if</span> f_word_count <span class="token operator">&lt;</span> r_word_count <span class="token keyword">else</span> res_rmm    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> res_fmm <span class="token keyword">if</span> len<span class="token punctuation">(</span>res_fmm<span class="token punctuation">)</span> <span class="token operator">&lt;</span> len<span class="token punctuation">(</span>res_rmm<span class="token punctuation">)</span> <span class="token keyword">else</span> res_rmm<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>分词效果：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_nlp_fenci_bimm.png" alt="BIMM分词结果"></p><p>可能有人会问，如果单字的数量也相同怎么办？如果你明白了中文分词的原理和实际用处的话，那么这个问题的答案自然会知晓。中文分词目前仍然没有完全准确的结果，一句话可以分成不同的分词结果。如果单字数量也相同，按照正常的逻辑那么会继续比较双字词，但是这样却没有可比性，在中文中大多数都是双字词，所以即使双字词的数量相同，但是结果可能却有很多种可能。</p><p>我们比较单字词的数量，取数量少的那个结果，只是为了大概率更准确一些，因为中文字单字为词的情况比较少，大多数是双字或多字词。但是针对一些特殊的句子，这种判断方法不见得结果是最优的。虽然如此，但是基于规则的中文分词仍然是目前为止最简单高效的方法。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>基于规则的分词，一般较为简单高效，但是词典的维护很大的人力维护，同时对于未登录词也没有很好的解决办法。双向最大匹配结合了正反两种方法的结果，结果较为准确，在实用中文信息处理中使用广泛。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li>《Python自然语言处理实战-核心技术与算法》涂铭，刘祥，刘树春 著</li><li>《统计自然语言处理》 宗成庆 著</li><li>详细代码可参考<a href="https://github.com/hiyoung123/NLP">GitHub</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> nlp </tag>
            
            <tag> 中文分词 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Markdown基本语法</title>
      <link href="posts/5d36ff15.html"/>
      <url>posts/5d36ff15.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>Markdown 是一种轻量级标记语言，它允许人们使用易读易写的纯文本格式编写文档。</p><p>Markdown 语言在 2004 由约翰·格鲁伯（英语：John Gruber）创建。</p><p>Markdown 编写的文档可以导出 HTML 、Word、图像、PDF、Epub 等多种格式的文档。</p><p>Markdown 编写的文档后缀为 <strong>.md</strong>, <strong>.markdown</strong>。</p></blockquote><hr><h2 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h2><p>Markdown支持6种级别的标题，对应html标签 <code>h1</code> ~ <code>h6</code>，严格的Markdown语法<code>＃</code>和文本之间要有一个空格。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># h1　这是一级标题</span><span class="token comment" spellcheck="true">## h2　这是二级标题</span><span class="token comment" spellcheck="true">### h3　这是三级标题</span><span class="token comment" spellcheck="true">#### h4　这是四级标题</span><span class="token comment" spellcheck="true">##### h5　这是五级标题</span><span class="token comment" spellcheck="true">###### h6　这是六级标题</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>效果如下（实际演示会造成菜单混乱，所以此处使用截图）：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_markdown_list_1.png" alt=""></p><p>除此之外，Markdown还支持另外一种形式的标题展示形式，使用下划线进行文本大小的控制。但是这种形式仅有两种表现形式：即一级标题和二级标题。</p><pre class="line-numbers language-bash"><code class="language-bash">这是一级标题<span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span>这是二级标题----------<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>效果如下：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_markdown_list_2.png" alt=""></p><hr><h2 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h2><p>使用三个或者三个以上的<code>-</code>或者<code>*</code>都可以。</p><pre class="line-numbers language-bash"><code class="language-bash">-------********<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>效果如下：</p><hr><hr><hr><hr><p>可以看出效果都是一样的。</p><hr><h2 id="字体"><a href="#字体" class="headerlink" title="字体"></a>字体</h2><ul><li><p>粗体</p><p>要加粗的文字左右分别用两个<code>*</code>号或下划线<code>_</code>包起来。</p></li><li><p>斜体</p><p>要加斜的文字左右分别用一个<code>*</code>号或下划线<code>_</code>包起来。</p></li><li><p>斜粗体</p><p>要加粗加斜的文字左右分别用三个<code>*</code>号或下划线<code>_</code>包起来。</p></li><li><p>删除线</p><p>要加删除线的文字左右分别用两个波浪线<code>~~</code>包起来。</p></li><li><p>下划线</p><p>要加下划线可以通过 HTML 的<code>&lt;u&gt;</code>标签来实现。</p></li><li><p>高亮</p><p>文字高亮功能能使行内部分文字高亮，使用一对反引号。</p></li></ul><pre class="line-numbers language-bash"><code class="language-bash">**这是加粗的文字**__这是加粗的文字__*这是倾斜的文字*_这是倾斜的文字_***这是斜体加粗的文字***___这是斜体加粗的文字___~~这是加删除线的文字~~<span class="token operator">&lt;</span>u<span class="token operator">></span>这是加下划线的文字<span class="token operator">&lt;</span>/u<span class="token operator">></span>这是要<span class="token variable"><span class="token variable">`</span>高亮<span class="token variable">`</span></span>的文字<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>效果如下：</p><p><strong>这是加粗的文字</strong><br><strong>这是加粗的文字</strong><br><em>这是倾斜的文字</em><br><em>这是倾斜的文字</em><br><strong><em>这是斜体加粗的文字</em></strong><br><strong><em>这是斜体加粗的文字</em></strong><br><del>这是加删除线的文字</del><br><u>这是加下划线的文字</u></p><p>这是要<code>高亮</code>的文字</p><hr><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p>在引用的文字前加<code>&gt;</code>即可，同样严格语法需要中间加一个空格。引用也可以嵌套，如加两个<code>&gt;&gt;</code>三个<code>&gt;&gt;&gt;</code>n个…</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">></span>最外层嵌套<span class="token operator">>></span>第一层嵌套<span class="token operator">>></span><span class="token operator">>></span><span class="token operator">>></span><span class="token operator">>></span><span class="token operator">>></span>最内层嵌套<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>效果如下：</p><blockquote><p>最外层嵌套</p><blockquote><p>第一层嵌套</p><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><p>最内层嵌套</p></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><hr><h2 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h2><p>Markdown 支持有序列表和无序列表。无序列表使用星号<code>*</code>、加号<code>+</code>或是减号<code>-</code>作为列表标记：</p><pre class="line-numbers language-bash"><code class="language-bash">* 第一项* 第二项* 第三项+ 第一项+ 第二项+ 第三项- 第一项- 第二项- 第三项<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>效果如下：</p><ul><li>第一项</li><li>第二项</li><li>第三项</li></ul><ul><li>第一项</li><li>第二项</li><li>第三项</li></ul><ul><li>第一项</li><li>第二项</li><li>第三项</li></ul><p>有序列表使用数字并加上 <code>.</code> 号来表示，如：</p><pre class="line-numbers language-bash"><code class="language-bash">1. 第一项2. 第二项3. 第三项<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>效果如下：</p><ol><li>第一项</li><li>第二项</li><li>第三项</li></ol><p>列表可以嵌套使用，只需在子列表中的选项添加四个空格即可：</p><pre class="line-numbers language-bash"><code class="language-bash">1. 第一项：    - 第一项嵌套的第一个元素    - 第一项嵌套的第二个元素2. 第二项：    - 第二项嵌套的第一个元素    - 第二项嵌套的第二个元素<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>效果如下：</p><ol><li>第一项：<ul><li>第一项嵌套的第一个元素</li><li>第一项嵌套的第二个元素</li></ul></li><li>第二项：<ul><li>第二项嵌套的第一个元素</li><li>第二项嵌套的第二个元素</li></ul></li></ol><hr><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>如果是段落上的一个函数或片段的代码可以用反引号把它包起来，例如：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token variable"><span class="token variable">`</span>print<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token variable">`</span></span>函数<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>效果如下：</p><p><code>print()</code>函数</p><p>代码块需要使用４个空格或者一个制表符（Tab键）：</p><pre class="line-numbers language-bash"><code class="language-bash">    fun <span class="token punctuation">(</span>x: Int, y: Int<span class="token punctuation">)</span>: Int <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;</span>      <span class="token keyword">return</span> x + y    <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>​    效果如下</p><pre class="line-numbers language-kotlin"><code class="language-kotlin"><span class="token keyword">fun</span> <span class="token punctuation">(</span>x<span class="token operator">:</span> Int<span class="token punctuation">,</span> y<span class="token operator">:</span> Int<span class="token punctuation">)</span><span class="token operator">:</span> Int &amp;#<span class="token number">123</span><span class="token punctuation">;</span>  <span class="token keyword">return</span> x <span class="token operator">+</span> y&amp;#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>或者使用’’’code’’’把代码包裹起来，也可以指定代码语言，这样可以进行代码高亮：</p><pre><code>​```javascript$(document).ready(function () &#123;    alert(&#39;RUNOOB&#39;);&#125;);​```</code></pre><p>效果如下：</p><pre class="line-numbers language-javascript"><code class="language-javascript"><span class="token function">$</span><span class="token punctuation">(</span>document<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">ready</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>    <span class="token function">alert</span><span class="token punctuation">(</span><span class="token string">'RUNOOB'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><hr><h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h2><h3 id="基本链接的使用方法"><a href="#基本链接的使用方法" class="headerlink" title="基本链接的使用方法"></a>基本链接的使用方法</h3><pre><code>[链接名称](链接地址 &quot;描述&quot;)　其中描述为鼠标放到url的显示文字，可加可不加。或者&lt;链接地址&gt;</code></pre><pre class="line-numbers language-bash"><code class="language-bash">这是一个链接 <span class="token punctuation">[</span>hiyoung blog<span class="token punctuation">]</span><span class="token punctuation">(</span>https://hiyoungai.com　<span class="token string">"我的博客"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>效果如下：</p><p>这是一个链接 <a href="https://hiyoungai.com" title="我的博客">hiyoung blog</a></p><p>直接使用链接地址：</p><pre><code>(https://hiyoungai.com)</code></pre><p>效果如下：</p><p><a href="https://hiyoungai.com">https://hiyoungai.com</a></p><h3 id="高级链接的使用方法"><a href="#高级链接的使用方法" class="headerlink" title="高级链接的使用方法"></a>高级链接的使用方法</h3><pre><code>链接使用变量代替，文档末尾定义变量且带有链接地址。这个链接使用１作为链接变量[Google][1]这个链接使用url作为链接变量[baidu][url][1]:https://www.google.com[url]:https://www.baidu.com</code></pre><p>效果如下：</p><p>这个链接使用１作为链接变量<a href="https://www.google.com">Google</a><br>这个链接使用url作为链接变量<a href="https://www.baidu.com">baidu</a></p><h3 id="锚点链接"><a href="#锚点链接" class="headerlink" title="锚点链接"></a>锚点链接</h3><p>每一个标题都是一个锚点，和HTML的锚点<code>#</code>类似：</p><pre><code>[回到顶部](#Markdown基本语法)</code></pre><p>效果如下：</p><p><a href="#Markdown基本语法">回到顶部</a></p><hr><h2 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h2><p>基本语法：</p><pre><code>![alt 属性文本](图片地址)![alt 属性文本](图片地址 &quot;可选标题title&quot;)图片alt就是显示在图片下面的文字，相当于对图片内容的解释。图片title是图片的标题，当鼠标移到图片上时显示的内容。title可加可不加也可以使用高级链接的方式，此处不再演示。例子：![Write](https://cdn.jsdelivr.net/gh/hiyoung123/cdn/img/img_markdown.jpeg &quot;写作&quot;)</code></pre><p>效果如下：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_markdown.jpeg" alt="Write 属性文本" title="写作"></p><hr><h2 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h2><p>Markdown 制作表格使用 <code>|</code> 来分隔不同的单元格，使用 <code>-</code> 来分隔表头和其他行。</p><h3 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h3><pre><code>|  表头   | 表头  ||  ----  | ----  || 单元格  | 单元格 || 单元格  | 单元格 |</code></pre><p>效果如下：</p><table><thead><tr><th>表头</th><th>表头</th></tr></thead><tbody><tr><td>单元格</td><td>单元格</td></tr><tr><td>单元格</td><td>单元格</td></tr></tbody></table><h3 id="对齐方式"><a href="#对齐方式" class="headerlink" title="对齐方式"></a>对齐方式</h3><p>我们可以设置表格的对齐方式：</p><ul><li><code>-:</code> 设置内容和标题栏居右对齐。</li><li><code>:-</code> 设置内容和标题栏居左对齐。</li><li><code>:-:</code> 设置内容和标题栏居中对齐。</li></ul><p>效果如下：</p><pre><code>| 左对齐 | 右对齐 | 居中对齐 || :-----| ----: | :----: || 单元格 | 单元格 | 单元格 || 单元格 | 单元格 | 单元格 |</code></pre><p>效果如下：</p><table><thead><tr><th align="left">左对齐</th><th align="right">右对齐</th><th align="center">居中对齐</th></tr></thead><tbody><tr><td align="left">单元格</td><td align="right">单元格</td><td align="center">单元格</td></tr><tr><td align="left">单元格</td><td align="right">单元格</td><td align="center">单元格</td></tr></tbody></table><p>而且表格中也可以混用其他语法：如粗体斜体，插入图片等。</p><hr><h2 id="高级技巧"><a href="#高级技巧" class="headerlink" title="高级技巧"></a>高级技巧</h2><h3 id="支持html元素"><a href="#支持html元素" class="headerlink" title="支持html元素"></a>支持html元素</h3><p>不在 Markdown 涵盖范围之内的标签，都可以直接在文档里面用 HTML 撰写。目前支持的 HTML 元素有：<code>&lt;kbd&gt; &lt;b&gt; &lt;i&gt; &lt;em&gt; &lt;sup&gt; &lt;sub&gt; &lt;br&gt;</code>等 ，如：</p><pre><code>使用 &lt;kbd&gt;Ctrl&lt;/kbd&gt;+&lt;kbd&gt;Alt&lt;/kbd&gt;+&lt;kbd&gt;Del&lt;/kbd&gt; 重启电脑</code></pre><p>效果如下：</p><p>使用 <kbd>Ctrl</kbd>+<kbd>Alt</kbd>+<kbd>Del</kbd> 重启电脑</p><h3 id="转义字符"><a href="#转义字符" class="headerlink" title="转义字符"></a>转义字符</h3><p> Markdown 使用了很多特殊符号来表示特定的意义，如果需要显示特定的符号则需要使用转义字符，Markdow使用反斜杠转义特殊字符：</p><pre><code>**文本加粗**\*\* 正常显示星号 \*\*</code></pre><p> 效果如下：</p><p><strong>文本加粗</strong><br>** 正常显示星号 **</p><p> Markdown 支持以下这些符号前面加上反斜杠来帮助插入普通的符号：</p><pre><code>\   反斜线`   反引号*   星号_   下划线&#123;&#125;  花括号[]  方括号()  小括号#   井字号+   加号-   减号.   英文句点!   感叹号</code></pre><h3 id="数学公式"><a href="#数学公式" class="headerlink" title="数学公式"></a>数学公式</h3><p>当你需要在编辑器中插入数学公式时，可以使用两个美元符 <code>$$</code> 包裹 <code>TeX</code> 或 <code>LaTeX</code> 格式的数学公式来实现。提交后，问答和文章页会根据需要加载 <code>Mathjax</code> 对数学公式进行渲染。如：</p><pre><code>$$\mathbf&#123;V&#125;_1 \times \mathbf&#123;V&#125;_2 =  \begin&#123;vmatrix&#125;\mathbf&#123;i&#125; &amp; \mathbf&#123;j&#125; &amp; \mathbf&#123;k&#125; \\\frac&#123;\partial X&#125;&#123;\partial u&#125; &amp;  \frac&#123;\partial Y&#125;&#123;\partial u&#125; &amp; 0 \\\frac&#123;\partial X&#125;&#123;\partial v&#125; &amp;  \frac&#123;\partial Y&#125;&#123;\partial v&#125; &amp; 0 \\\end&#123;vmatrix&#125;$$</code></pre><p>效果如下：<br>$$<br>\mathbf{V}_1 \times \mathbf{V}_2 =  \begin{vmatrix}<br>\mathbf{i} &amp; \mathbf{j} &amp; \mathbf{k} \<br>\frac{\partial X}{\partial u} &amp;  \frac{\partial Y}{\partial u} &amp; 0 \<br>\frac{\partial X}{\partial v} &amp;  \frac{\partial Y}{\partial v} &amp; 0 \<br>\end{vmatrix}<br>$$</p><h3 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h3><p>流程图需要平台支持，而我使用的hexo，需要安装如下三个插件：</p><pre><code>npm install --save hexo-filter-flowchartnpm install --save hexo-filter-mermaid-diagramsnpm install --save hexo-filter-sequence</code></pre><p>同时，对于<code>Matery</code>主题的博客还需要配置一下_config.xml和修改footer.ejs。</p><p>在主题的_config.yml中添加如下代码：</p><pre><code># Mermaid tagmermaid:  enable: true  # Available themes: default | dark | forest | neutral  theme: forest  cdn: https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js  #cdn: //cdnjs.cloudflare.com/ajax/libs/mermaid/8.0.0/mermaid.min.js</code></pre><p>在footer.ejs的结尾处添加：</p><pre><code>&lt;div class=&quot;progress-bar&quot;&gt;&lt;/div&gt;&lt;% if (theme.mermaid.enable) &#123; %&gt;  &lt;script src=&#39;&lt;%= theme.mermaid.cdn %&gt;&#39;&gt;&lt;/script&gt;  &lt;script&gt;    if (window.mermaid) &#123;      mermaid.initialize(&#123;theme: &#39;forest&#39;&#125;);    &#125;  &lt;/script&gt;&lt;% &#125; %&gt;</code></pre><p>如果不使用<code>mermaid</code>的话那么不需要上述配置。</p><h4 id="横向流程图"><a href="#横向流程图" class="headerlink" title="横向流程图"></a>横向流程图</h4><pre><code>​&lt;pre class=&quot;mermaid&quot;&gt;graph LRA[方形] --&gt;B(圆角)    B --&gt; C&#123;条件a&#125;    C --&gt;|a=1| D[结果1]    C --&gt;|a=2| E[结果2]    F[横向流程图]​&lt;/pre&gt;</code></pre><pre class="mermaid">graph LRA[方形] -->B(圆角)    B --> C{条件a}    C -->|a=1| D[结果1]    C -->|a=2| E[结果2]    F[横向流程图]</pre><h4 id="纵向流程图"><a href="#纵向流程图" class="headerlink" title="纵向流程图"></a>纵向流程图</h4><pre><code>​&lt;pre class=&quot;mermaid&quot;&gt;graph TDA[方形] --&gt; B(圆角)    B --&gt; C&#123;条件a&#125;    C --&gt; |a=1| D[结果1]    C --&gt; |a=2| E[结果2]    F[竖向流程图]​&lt;/pre&gt;</code></pre><pre class="mermaid">graph TDA[方形] --> B(圆角)    B --> C{条件a}    C --> |a=1| D[结果1]    C --> |a=2| E[结果2]    F[竖向流程图]</pre><h4 id="标准流程图"><a href="#标准流程图" class="headerlink" title="标准流程图"></a>标准流程图</h4><pre><code>​&lt;div id=&quot;flowchart-0&quot; class=&quot;flow-chart&quot;&gt;&lt;/div&gt;</code></pre><div id="flowchart-1" class="flow-chart"></div><h4 id="标准流程图（横向）"><a href="#标准流程图（横向）" class="headerlink" title="标准流程图（横向）"></a>标准流程图（横向）</h4><pre><code>​&lt;div id=&quot;flowchart-2&quot; class=&quot;flow-chart&quot;&gt;&lt;/div&gt;</code></pre><div id="flowchart-3" class="flow-chart"></div><h4 id="UML时序图"><a href="#UML时序图" class="headerlink" title="UML时序图"></a>UML时序图</h4><pre><code>​&lt;div id=&quot;sequence-0&quot;&gt;&lt;/div&gt;</code></pre><div id="sequence-1"></div><h4 id="UML时序图（复杂样例）"><a href="#UML时序图（复杂样例）" class="headerlink" title="UML时序图（复杂样例）"></a>UML时序图（复杂样例）</h4><pre><code>​&lt;div id=&quot;sequence-2&quot;&gt;&lt;/div&gt;</code></pre><div id="sequence-3"></div><h4 id="UML标准时序图"><a href="#UML标准时序图" class="headerlink" title="UML标准时序图"></a>UML标准时序图</h4><pre><code>​&lt;pre class=&quot;mermaid&quot;&gt;%% 时序图例子,-&gt; 直线，--&gt;虚线，-&gt;&gt;实线箭头  sequenceDiagram    participant 张三    participant 李四    张三-&gt;王五: 王五你好吗？    loop 健康检查        王五-&gt;王五: 与疾病战斗    end    Note right of 王五: 合理 食物 &lt;br/&gt;看医生...    李四--&gt;&gt;张三: 很好!    王五-&gt;李四: 你怎么样?    李四--&gt;王五: 很好!​&lt;/pre&gt;</code></pre><pre class="mermaid">%% 时序图例子,-> 直线，-->虚线，->>实线箭头  sequenceDiagram    participant 张三    participant 李四    张三->王五: 王五你好吗？    loop 健康检查        王五->王五: 与疾病战斗    end    Note right of 王五: 合理 食物 <br/>看医生...    李四-->>张三: 很好!    王五->李四: 你怎么样?    李四-->王五: 很好!</pre><h4 id="甘特图"><a href="#甘特图" class="headerlink" title="甘特图"></a>甘特图</h4><pre><code>​&lt;pre class=&quot;mermaid&quot;&gt;%% 语法示例        gantt        dateFormat  YYYY-MM-DD        title 软件开发甘特图        section 设计        需求                      :done,    des1, 2014-01-06,2014-01-08        原型                      :active,  des2, 2014-01-09, 3d        UI设计                     :         des3, after des2, 5d    未来任务                     :         des4, after des3, 5d        section 开发        学习准备理解需求                      :crit, done, 2014-01-06,24h        设计框架                             :crit, done, after des2, 2d        开发                                 :crit, active, 3d        未来任务                              :crit, 5d        耍                                   :2d        section 测试        功能测试                              :active, a1, after des3, 3d        压力测试                               :after a1  , 20h        测试报告                               : 48h​&lt;/pre&gt;</code></pre><pre class="mermaid">%% 语法示例        gantt        dateFormat  YYYY-MM-DD        title 软件开发甘特图        section 设计        需求                      :done,    des1, 2014-01-06,2014-01-08        原型                      :active,  des2, 2014-01-09, 3d        UI设计                     :         des3, after des2, 5d    未来任务                     :         des4, after des3, 5d        section 开发        学习准备理解需求                      :crit, done, 2014-01-06,24h        设计框架                             :crit, done, after des2, 2d        开发                                 :crit, active, 3d        未来任务                              :crit, 5d        耍                                   :2d        section 测试        功能测试                              :active, a1, after des3, 3d        压力测试                               :after a1  , 20h        测试报告                               : 48h</pre><h3 id="Emoj表情"><a href="#Emoj表情" class="headerlink" title="Emoj表情"></a>Emoj表情</h3><p><code>Github</code>的<code>Markdown</code>语法支持添加emoji表情，输入不同的符号码（两个冒号包着的字符）可以显示出不同的表情（本网站没有添加该插件，需要支持Github的markdwon才可以正常显示）：</p><pre><code>:bluesh:</code></pre><p>效果如下：</p><p>😀</p><p> 具体每一个表情的符号码，可以查询<code>Github</code>的官方网页<a href="http://www.emoji-cheat-sheet.com/">http://www.emoji-cheat-sheet.com</a>。 </p><h3 id="插入视频"><a href="#插入视频" class="headerlink" title="插入视频"></a>插入视频</h3><pre><code>&lt;video id=&quot;video&quot; controls=&quot;&quot; preload=&quot;none&quot; poster=&quot;缩略图&quot;&gt;      &lt;source id=&quot;视频url&quot; type=&quot;video/mp4&quot;&gt;      &lt;/video&gt;例子：&lt;video id=&quot;video&quot; controls=&quot;&quot; preload=&quot;none&quot; poster=&quot;&quot;&gt;      &lt;source id=&quot;mp4&quot; src=&quot;https://www.typora.io/img/beta.mp4&quot;&gt;      &lt;/video&gt;</code></pre><p>效果可在<a href="#Typora">Typora</a>看到。</p><h3 id="插入Github-Star"><a href="#插入Github-Star" class="headerlink" title="插入Github Star"></a>插入Github Star</h3><pre><code>  &lt;iframe                         style=&quot;margin-left: 2px; margin-bottom:-5px;&quot;                         frameborder=&quot;0&quot; scrolling=&quot;0&quot; width=&quot;100px&quot; height=&quot;20px&quot;                         src=&quot;https://ghbtns.com/github-btn.html?user=hiyoung123&amp;repo=hiyoung123.github.io&amp;type=star&amp;count=true&quot; &gt;                     &lt;/iframe&gt;</code></pre><p>效果如下：</p>  <iframe                         style="margin-left: 2px; margin-bottom:-5px;"                         frameborder="0" scrolling="0" width="100px" height="20px"                         src="https://ghbtns.com/github-btn.html?user=hiyoung123&repo=hiyoung123.github.io&type=star&count=true" >                     </iframe>-- -- --<h2 id="工具介绍"><a href="#工具介绍" class="headerlink" title="工具介绍"></a>工具介绍</h2><h3 id="Typora"><a href="#Typora" class="headerlink" title="Typora"></a>Typora</h3><p>特点：简洁，快速，可以实现所见即所得。看下面这个视频你就会知道他的<code>Simple, yet Powerful</code></p><video id="video" width="420" height="320" autoplay muted="muted" preload="preload" loop="loop" poster="https://cdn.jsdelivr.net/gh/hiyoung123/cdn/img/loading.gif">      <source id="mp4" src="https://cdn.jsdelivr.net/gh/hiyoung123/CDN/video/video_markdown_typora.mp4" type="video/mp4">      </video><p><a href="https://www.typora.io/">Typora官方下载</a> </p><h3 id="Atom"><a href="#Atom" class="headerlink" title="Atom"></a>Atom</h3><p>特点：插件丰富（毕竟是Github推出的），并且可以用作其他语言的编辑器。其实也可以做到一边编辑一边看结果，只不过是需要多开一个窗口，😄！</p><p><a href="https://github.com/atom/atom">Atom下载地址</a></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>作者也是刚开始接触这两个软件，所以不是特别熟悉，等使用一段时间，对比之后再来详细的写一下。<br>流程图插件配置参考博客：<a href="https://blog.csdn.net/Olivia_Vang/article/details/92987859">https://blog.csdn.net/Olivia_Vang/article/details/92987859</a></p><script src="https://cdnjs.cloudflare.com/ajax/libs/raphael/2.2.7/raphael.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/flowchart/1.6.5/flowchart.min.js"></script><textarea id="flowchart-0-code" style="display: none">st=>start: 开始框op=>operation: 处理框cond=>condition: 判断框(是或否?)sub1=>subroutine: 子流程io=>inputoutput: 输入输出框e=>end: 结束框st->op->condcond(yes)->io->econd(no)->sub1(right)->op​</textarea><textarea id="flowchart-0-options" style="display: none">{"theme":"simple","scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("flowchart-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-0", options);</script><textarea id="flowchart-1-code" style="display: none">st=>start: 开始框op=>operation: 处理框cond=>condition: 判断框(是或否?)sub1=>subroutine: 子流程io=>inputoutput: 输入输出框e=>end: 结束框st->op->condcond(yes)->io->econd(no)->sub1(right)->op</textarea><textarea id="flowchart-1-options" style="display: none">{"theme":"simple","scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("flowchart-1-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-1-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-1", options);</script><textarea id="flowchart-2-code" style="display: none">st=>start: 开始框op=>operation: 处理框cond=>condition: 判断框(是或否?)sub1=>subroutine: 子流程io=>inputoutput: 输入输出框e=>end: 结束框st(right)->op(right)->condcond(yes)->io(bottom)->econd(no)->sub1(right)->op​</textarea><textarea id="flowchart-2-options" style="display: none">{"theme":"simple","scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("flowchart-2-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-2-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-2", options);</script><textarea id="flowchart-3-code" style="display: none">st=>start: 开始框op=>operation: 处理框cond=>condition: 判断框(是或否?)sub1=>subroutine: 子流程io=>inputoutput: 输入输出框e=>end: 结束框st(right)->op(right)->condcond(yes)->io(bottom)->econd(no)->sub1(right)->op</textarea><textarea id="flowchart-3-options" style="display: none">{"theme":"simple","scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("flowchart-3-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-3-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-3", options);</script><script src="https://cdnjs.cloudflare.com/ajax/libs/webfont/1.6.27/webfontloader.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/snap.svg/0.4.1/snap.svg-min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/underscore.js/1.8.3/underscore-min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/js-sequence-diagrams/1.0.6/sequence-diagram-min.js"></script><textarea id="sequence-0-code" style="display: none">对象A->对象B: 对象B你好吗?（请求）Note right of 对象B: 对象B的描述Note left of 对象A: 对象A的描述(提示)对象B-->对象A: 我很好(响应)对象A->对象B: 你真的好吗？​</textarea><textarea id="sequence-0-options" style="display: none">{"theme":"simple","scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("sequence-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-0", options);</script><textarea id="sequence-1-code" style="display: none">对象A->对象B: 对象B你好吗?（请求）Note right of 对象B: 对象B的描述Note left of 对象A: 对象A的描述(提示)对象B-->对象A: 我很好(响应)对象A->对象B: 你真的好吗？</textarea><textarea id="sequence-1-options" style="display: none">{"theme":"simple","scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("sequence-1-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-1-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-1", options);</script><textarea id="sequence-2-code" style="display: none">Title: 标题：复杂使用对象A->对象B: 对象B你好吗?（请求）Note right of 对象B: 对象B的描述Note left of 对象A: 对象A的描述(提示)对象B-->对象A: 我很好(响应)对象B->小三: 你好吗小三-->>对象A: 对象B找我了对象A->对象B: 你真的好吗？Note over 小三,对象B: 我们是朋友participant CNote right of C: 没人陪我玩​</textarea><textarea id="sequence-2-options" style="display: none">{"theme":"simple","scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("sequence-2-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-2-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-2", options);</script><textarea id="sequence-3-code" style="display: none">Title: 标题：复杂使用对象A->对象B: 对象B你好吗?（请求）Note right of 对象B: 对象B的描述Note left of 对象A: 对象A的描述(提示)对象B-->对象A: 我很好(响应)对象B->小三: 你好吗小三-->>对象A: 对象B找我了对象A->对象B: 你真的好吗？Note over 小三,对象B: 我们是朋友participant CNote right of C: 没人陪我玩</textarea><textarea id="sequence-3-options" style="display: none">{"theme":"simple","scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("sequence-3-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-3-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-3", options);</script>]]></content>
      
      
      <categories>
          
          <category> 便捷工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markdown </tag>
            
            <tag> Typora </tag>
            
            <tag> Atom </tag>
            
            <tag> 流程图 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>最全Hexo博客搭建教程以及优化</title>
      <link href="posts/4dbbde95.html"/>
      <url>posts/4dbbde95.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>使用Hexo+Github搭建一个免费的个人博客，本文略长，大佬请自行选择阅读。</p></blockquote><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>一边上班一边搭建博客，忙了大概有一周左右的时间，终于把博客都调好了。我使用的是<code>Hexo</code>框架，主题是闪烁之狐之狐的<a href="https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md">hexo-theme-matery</a>，本文介绍的也是该主题的配置，大家如果喜欢可以去下载使用。</p><p>本文除了介绍了<code>matery</code>主题的一些基础配置之外，也介绍了一些我个人和在其他大佬处看到的功能定制。只要你懂得操作软件，懂得键盘打字，那么就可以通过本教程搭建一个完全<code>免费</code>的个人博客。如果你是技术大佬，那么更可以通过修改源码去定制更好的功能。本文也记录了一些我搭建过程中遇到的坑，希望可以帮你在搭建过程中少走一些弯路，同时如果你也遇到一些本文没有记载的<em>bug</em>，也请你给我留言，让我们一起学习解决，多谢。</p><h2 id="第一部分：准备"><a href="#第一部分：准备" class="headerlink" title="第一部分：准备"></a>第一部分：准备</h2><h3 id="1-Hexo介绍"><a href="#1-Hexo介绍" class="headerlink" title="1.Hexo介绍"></a>1.Hexo介绍</h3><p><a href="https://hexo.io/zh-cn/">Hexo</a>是一款快速、简洁且高效的基于<code>Node.js</code>的静态博客框架，四大特性：</p><ul><li>超快速度：<code>Node.js</code> 所带来的超快生成速度，让上百个页面在几秒内瞬间完成渲染。</li><li>支持Markdown：<code>Hexo</code> 支持 <code>GitHub Flavored Markdown</code> 的所有功能，甚至可以整合 <code>Octopress</code> 的大多数插件。</li><li>一键部署：只需一条指令即可部署到 <code>GitHub Pages</code>, <code>Heroku</code>或其他平台。</li><li>插件和可扩展性：强大的 <code>API</code> 带来无限的可能，与数种模板引擎<code>（EJS，Pug，Nunjucks）</code>和工具<code>（Babel，PostCSS，Less/Sass）</code>轻易集成。</li></ul><p>这使得很多非编程人员可以很轻松，很自由的定制博客。废话不多说，开始进入搭建环境把。</p><h3 id="2-安装Node环境"><a href="#2-安装Node环境" class="headerlink" title="2.安装Node环境"></a>2.安装Node环境</h3><h4 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h4><p>直接命令行输入：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> nodejs<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> <span class="token function">npm</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>或者到<a href="http://nodejs.cn/download/">官网</a>下载：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_hexo_download_nodejs_1.png" alt=""></p><p>下载完成后解压到指定文件夹，然后配置环境变量（目的是为了在终端可以任意位置使用它）：</p><p>首先打开<code>~/.bashrc</code>文件</p><pre class="line-numbers language-bash"><code class="language-bash">vim ~/.bashrc<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在文件的最下端填写如下代码</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">export</span> PATH<span class="token operator">=</span>$<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;PATH&amp;#125;:$HOME/node-v12.13.0-linux-x64/bin/</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>因为我下载的是<code>64</code>位<code>12.13.0</code>版本，并且放到了根目录<code>home</code>下，你可以根据自己的需求进行更改上面的路径。保存退出后，执行命令让修改生效。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">source</span> ~/.bashrc<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>然后在终端输入<code>npm -v</code>和<code>node -v</code>验证是否安装配置成功</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token variable">$npm</span> -v6.13.0<span class="token variable">$node</span> -vv12.13.0<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h4><p>下载稳定版或者最新版都可以<a href="http://nodejs.cn/download/">Node.js</a>，安装选项全部默认，一路点击<code>Next</code>。最后安装好之后，按<code>Win+R</code>打开命令提示符，输入<code>node -v</code>和<code>npm -v</code>，如果出现版本号，那么就安装成功了。</p><h4 id="npm加速"><a href="#npm加速" class="headerlink" title="npm加速"></a>npm加速</h4><p>一般国内通过<code>npm</code>下载东西会比较慢，所以需要添加阿里的源进行加速。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> config <span class="token keyword">set</span> registry https://registry.npm.taobao.org<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="3-安装Git"><a href="#3-安装Git" class="headerlink" title="3.安装Git"></a>3.安装Git</h3><p>为了把本地的网页文件上传到<code>Github</code>上面去，我们需要用到分布式版本控制工具 <code>git</code>。关于<code>git</code>和<code>Github</code>这里就不多介绍了。同样分为两个版本：</p><h4 id="Linux-1"><a href="#Linux-1" class="headerlink" title="Linux"></a>Linux</h4><p>在Linux平台比较方便，直接使用命令就可以安装：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> <span class="token function">git</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>安装完成后即可享用。</p><h4 id="Windows-1"><a href="#Windows-1" class="headerlink" title="Windows"></a>Windows</h4><p>需要去官网下载<a href="https://git-scm.com/download/win">Git</a>，下载完成后按照向导安装即可。</p><blockquote><p>注意：在安装的最后一步添加路径时选择 Use Git from the Windows Command Prompt 。这是把Git添加到了环境变量中，以便可以在cmd中使用。而本人推荐使用下载附带的git bash进行操作，比较方便。</p></blockquote><p>对于git的讲解和使用，大家可以自行到网上查找。<code>Hexo</code>搭建的过程中，已经封装好一个git命令，可以直接使用<code>hexo</code>的命令将生成的静态网站代码同步到<code>github</code>的仓库里。但是如果想要自己同步源码的话，那么就需要掌握一下git命令了。在这里我只列举一下常用的命令：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">git</span> init <span class="token comment" spellcheck="true">#初始化一个git库，生成.git文件夹，里面保存的是该git库的记录和配置</span><span class="token function">git</span> remote add origin 远程仓库地址 <span class="token comment" spellcheck="true">#将本地仓库和远程仓库链接起来</span><span class="token function">git</span> pull <span class="token comment" spellcheck="true">#同步代码</span><span class="token function">git</span> status <span class="token comment" spellcheck="true">#检查本地仓库修改状态</span><span class="token function">git</span> add 文件名 或者 <span class="token function">git</span> add <span class="token keyword">.</span>  <span class="token comment" spellcheck="true">#将本地修改的文件加入缓存</span><span class="token function">git</span> commit 文件名 -m <span class="token string">"描述"</span> 或者 <span class="token function">git</span> commit <span class="token keyword">.</span> -m <span class="token string">"描述"</span>  <span class="token comment" spellcheck="true">#提交缓存，并描述该提交</span><span class="token function">git</span> push -u origin code <span class="token comment" spellcheck="true"># 将本地的提交推送到远程仓库.-u是代表输入账号密码，如果你已经配置了git的公钥，那么可直接push.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-注册Github"><a href="#4-注册Github" class="headerlink" title="4.注册Github"></a>4.注册Github</h3><p><code>Git</code>安装完成之后就可以去<a href="https://github.com/">Github</a>上注册账号并创建仓库， 用来存放我们的网站了。</p><blockquote><p> Github是基于 Git 做版本控制的代码托管平台，同时也是全球最大的代（同）码（性）托（交）管（友）网站。 </p></blockquote><p>创建完账户之后新建一个项目仓库<code>New repository</code>，如下所示 </p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_hexo_github_new_rep.png" alt=""></p><p>接着输入仓库名，后面一定要加<code>.github.io</code>后缀，README初始化也要勾上。 如下图配置（因为我的已经存在相同的仓库，所以报错）</p><blockquote><p>要创建一个和你用户名相同的仓库，后面加.github.io，只有这样将来要部署到GitHub page的时候，才会被识别，也就是<a href="http://xxxx.github.io，其中xxx就是你注册`GitHub`的用户名">http://xxxx.github.io，其中xxx就是你注册`GitHub`的用户名</a> </p></blockquote><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_hexo_github_creat_rep.png" alt=""></p><p>然后项目就建成了，点击<code>Settings</code>，向下拉到最后有个<code>GitHub Pages</code>，点击<code>Choose a theme</code>选择一个主题。然后等一会儿，再回到<code>GitHub Pages</code>，点击新出来的链接，就会进入到<code>github page</code>的界面。看到这个界面就说明<code>Github</code>的<code>page</code>已经可以使用了，接下来我们进入<code>Hexo</code>的搭建。</p><h2 id="第二部分：搭建"><a href="#第二部分：搭建" class="headerlink" title="第二部分：搭建"></a>第二部分：搭建</h2><h3 id="1-安装Hexo"><a href="#1-安装Hexo" class="headerlink" title="1.安装Hexo"></a>1.安装Hexo</h3><p>首先创建一个文件夹，名字自取如<code>YoungBlog</code>，用来存放自己的博客文件，然后<code>cd</code>到这个文件夹下（或者在这个文件夹下直接右键<code>git bash</code>打开）。在该目录下输入如下命令安装<code>Hexo</code>：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> -g hexo-cli<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>接下来初始化一下<code>hexo</code>,即初始化我们的网站，</p><pre class="line-numbers language-bash"><code class="language-bash">hexo init<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><blockquote><p>初始化要求必须是空的目录下进行。</p></blockquote><p>接着输入<code>npm install</code>安装必备的组件。</p><p>初始化完成后会在目下生成几个文件和文件夹，这些就是我们需要编写的网站源码了：</p><ul><li><code>node_modules:</code> 依赖包，npm安装的一些插件存放的文件夹。</li><li><code>public：</code>存放生成的页面，网站正式展示的内容。</li><li><code>scaffolds：</code>生成文章和页面的一些模板。</li><li><code>source：</code>用来存放你的文章和数据。</li><li><code>themes：</code>主题存放文件夹。</li><li><code>_config.yml:</code> 博客的配置文件，非主题的配置。</li><li><code>db.json</code>：博客的版本信息等。</li><li><code>package.json</code>和<code>package-lock.json</code>：依赖包和版本信息。</li></ul><p>这样本地的网站配置也弄好啦，输入<code>hexo g</code>生成静态网页，然后输入<code>hexo s</code>打开本地服务器，然后浏览器打开<a href="http://localhost:4000">http://localhost:4000</a>就可以看到我们的博客啦，效果如下：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_hexo_web_1.png" alt=""></p><p>这里介绍一下<code>Hexo</code>常用的几个命令：</p><pre class="line-numbers language-bash"><code class="language-bash">hexo clean <span class="token comment" spellcheck="true">#清除db和public文件下的内容，或可写成hexo cl</span>hexo g <span class="token comment" spellcheck="true">#根据源码生成静态文件</span>hexo s <span class="token comment" spellcheck="true">#开启本地的server，这样可在本地通过localhost:4000访问博客。或可写成hexo server</span>hexo d <span class="token comment" spellcheck="true">#部署网站的静态文件到配置好的托管网站，如Github或者Coding，配置在_config中的Deploy。</span><span class="token comment" spellcheck="true">#后续如果安装了一些插件，可能导致缩写无法使用，所以hexo d也可以写成hexo deploy。</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>看完展示后，可以按<code>ctrl+c</code>关闭本地服务器。</p><h3 id="2-部署到Github"><a href="#2-部署到Github" class="headerlink" title="2.部署到Github"></a>2.部署到Github</h3><p>首先要安装一个插件，用于<code>Hexo</code>部署代码的。</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> i hexo-deployer-git<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>安装完成之后，在<code>_config.yml</code>配置文件中加入如下代码，这样我们在使用<code>hexo d</code>的时候就可以直接部署到<code>Github</code>上了，如果你想部署到其他平台（支持<code>Git</code>），也可以添加到这里。</p><pre class="line-numbers language-bash"><code class="language-bash">deploy:  type: <span class="token function">git</span>  repository: https://github.com/hiyoung123/hiyoung123.github.io  branch: master<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>如果不了解git那么请先自行百度学习一下git的相关配置。</p></blockquote><p><code>Git</code>分为无密推送和需要输入账户密码推送。无密码推送就是需要在本地生成公钥，然后添加到代码托管平台如<code>Github</code>，这样在推送时候就不需要输入账户密码了。而反之的话，每次推送就会要求你输入账户密码。下面说一下无密推送的配置过程。</p><p>首先打开<code>Git bash</code>，输入如下内容：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">git</span> config --global user.name <span class="token string">"你的用户名"</span><span class="token function">git</span> config --global user.email <span class="token string">"你的邮箱"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>用户名和邮箱根据你注册<code>github</code>的信息自行修改。</p><p>然后生成密钥SSH key：</p><pre class="line-numbers language-bash"><code class="language-bash">ssh-keygen -t rsa -C <span class="token string">"你的邮箱"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这个时候它会告诉你已经生成了<code>.ssh</code>的文件夹。在你的电脑中找到这个文件夹。或者<code>git bash</code>中输入</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">cat</span> ~/.ssh/id_rsa.pub<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>打开<a href="http://github.com/">github</a>，在头像下面点击<code>settings</code>，再点击<code>SSH and GPG keys</code>，新建一个<code>SSH</code>，名字随便取一个都可以，把你的<code>id_rsa.pub</code>里面的信息复制进去。</p><p>这样你的电脑就跟<code>Github</code>建立起的安全联系，以后推送代码就不需要输入密码了。</p><blockquote><p>注意：这里使用hexo d推送代码，推送的是编译完成的静态文件，也就是上面说的public文件夹下的代码，而不是网站的源代码。</p></blockquote><h3 id="3-写文章、发布文章"><a href="#3-写文章、发布文章" class="headerlink" title="3.写文章、发布文章"></a>3.写文章、发布文章</h3><p>输入<code>hexo new post &quot;article title&quot;</code>，新建一篇文章。</p><p>然后打开<code>\source\_posts</code>的目录，可以发现下面多了一个文件夹和一个<code>.md</code>文件，一个用来存放你的图片等数据，另一个就是你的文章文件啦。</p><p>编写完markdown文件后，根目录下输入<code>hexo g</code>生成静态网页，然后输入<code>hexo s</code>可以本地预览效果，最后输入<code>hexo d</code>上传到<code>github</code>上。这时打开你的<code>github.io</code>主页就能看到发布的文章啦。</p><h3 id="4-绑定个人域名"><a href="#4-绑定个人域名" class="headerlink" title="4.绑定个人域名"></a>4.绑定个人域名</h3><p>现在默认的域名还是<code>xxx.github.io</code>，是不是很没有牌面？想不想也像我一样弄一个专属域名呢，首先你得购买一个域名，xx云都能买，看你个人喜好了。</p><p>以我的阿里云为例，如下图所示，添加两条解析记录：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_hexo_add_domin.png" alt=""></p><p>我添加的是A记录，也就是需要添加<code>IP</code>地址的，你部署到<code>Github</code>的<code>IP</code>可以通过<code>ping xxx.github.io</code>获得。当然也可以添加<code>CNAME</code>记录，记录值填写<code>xxx.github.io</code>即可。</p><p>解析域名完成后，需要在<code>Github</code>上加入你的域名。打开你的<code>github</code>博客项目，点击<code>settings</code>，拉到下面<code>Custom domain</code>处，填上你自己的域名，保存完成后如下图：</p><p><img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/hiyoung123/images/img/img_hexo_add_domin_github.png" alt=""></p><blockquote><p>注意：如果下面的Enforce HTTPS 没有点击的话请勾选上。这个作用是使你的网络请求以更安全的HTTPS方式请求。</p></blockquote><p>这时候你的项目根目录应该会出现一个名为<code>CNAME</code>的文件了，如果没有的话，打开你本地博客<code>/source</code>目录，新建<code>CNAME</code>文件，注意没有后缀。然后在里面写上你的域名，保存。因为每次推送代码的时候，都会把<code>Github</code>自动生成的<code>CNAME</code>文件删除掉，导致每次推送后域名和<code>Github</code>就失去了联系，我们在<code>source/</code>下自己创建一个<code>CNAME</code>文件，这样就可以永久保存了。</p><h3 id="5-备份博客源文件"><a href="#5-备份博客源文件" class="headerlink" title="5.备份博客源文件"></a>5.备份博客源文件</h3><p>这次我们提交到<code>Github</code>上的是博客的源代码，这样我们就可以在不同电脑上进行操作了。</p><p>首先在<code>github</code>博客仓库下新建一个分支<code>code</code>，然后<code>git clone</code>到本地，把<code>.git</code>文件夹拿出来，放在博客根目录下（也可以博客根目录下执行<code>git init</code> , 然后 <code>git remote add origin 远端仓库地址的方式</code>）。然后<code>git checkout code</code>切换到<code>code</code>分支，然后<code>git add .</code>，然后<code>git commit -m &quot;xxx&quot;</code>，最后<code>git push origin code</code>提交就行了。</p><h2 id="第三部分：定制"><a href="#第三部分：定制" class="headerlink" title="第三部分：定制"></a>第三部分：定制</h2><p>这部分主要讲解一下主题的功能定制，除了基本的功能定制外，还有我参考各个大佬们的功能，有些我虽然没有加在我的博客上，但是也列在了此处。所以先在此处感谢一下各位大佬的博客文章。</p><p><a href="https://github.com/blinkfox/hexo-theme-matery">闪烁之狐的原版定制</a> | <a href="https://godweiyang.com/2018/04/13/hexo-blog/">Godweiyang</a> | <a href="https://sunhwee.com/posts/6e8839eb.html">洪卫</a> | <a href="https://blog.sky03.cn/2019/42790.html">Sky03</a></p><h3 id="1-更换主题"><a href="#1-更换主题" class="headerlink" title="1.更换主题"></a>1.更换主题</h3><p>下载主题，解压到博客目录下的<code>themes</code>目录，修改根目录下的 <code>_config.yml</code> 的 <code>theme</code> 的值：<code>theme: hexo-theme-matery</code></p><h3 id="2-设置文章模板"><a href="#2-设置文章模板" class="headerlink" title="2.设置文章模板"></a>2.设置文章模板</h3><p><code>Hexo</code>的页面是包括一个<code>md</code>文件和<code>ejs</code>文件结合而成的，<code>md</code>文件中的内容是页面配置，基本信息，和显示的内容。而<code>ejs</code>文件就是<code>js</code>逻辑代码了。</p><p>我们在<code>scaffolds/post.md</code>中设置文章的默认模板，这样以后创建文章的时候，这些信息就默认添加上了，不同文章你也可以修改这些信息。</p><pre class="line-numbers language-bash"><code class="language-bash">---title: <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;&amp;#123; title &amp;#125;&amp;#125;</span>date: <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;&amp;#123; date &amp;#125;&amp;#125;</span>top: <span class="token boolean">false</span>cover: <span class="token boolean">false</span>password:toc: <span class="token boolean">true</span>mathjax: <span class="token boolean">true</span>summary:tags:categories:---<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-添加404页面"><a href="#3-添加404页面" class="headerlink" title="3.添加404页面"></a>3.添加404页面</h3><p>原来的主题没有404页面，所以我们自己添加一个。首先在<code>/source/</code>目录下新建一个<code>404.md</code>，内容如下：</p><pre class="line-numbers language-bash"><code class="language-bash">---title: 404date: 2019-07-19 16:41:10type: <span class="token string">"404"</span>layout: <span class="token string">"404"</span>description: <span class="token string">"你来到了没有知识的荒原 :("</span>---<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>然后在<code>/themes/matery/layout/</code>目录下新建一个<code>404.ejs</code>文件，内容如下：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>style type<span class="token operator">=</span><span class="token string">"text/css"</span><span class="token operator">></span>    /* don<span class="token string">'t remove. */    .about-cover &amp;#123;        height: 75vh;    &amp;#125;&lt;/style>&lt;div class="bg-cover pd-header about-cover">    &lt;div class="container">        &lt;div class="row">            &lt;div class="col s10 offset-s1 m8 offset-m2 l8 offset-l2">                &lt;div class="brand">                    &lt;div class="title center-align">                        404                    &lt;/div>                    &lt;div class="description center-align">                        &lt;%= page.description %>                    &lt;/div>                &lt;/div>            &lt;/div>        &lt;/div>    &lt;/div>&lt;/div>&lt;script>    // 每天切换 banner 图.  Switch banner image every day.    $('</span>.bg-cover<span class="token string">').css('</span>background-image<span class="token string">', '</span>url<span class="token punctuation">(</span>/medias/banner/<span class="token string">' + new Date().getDay() + '</span>.jpg<span class="token punctuation">)</span>'<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token operator">&lt;</span>/script<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-添加二级菜单"><a href="#4-添加二级菜单" class="headerlink" title="4.添加二级菜单"></a>4.添加二级菜单</h3><p>因为我使用的是最新版的主题代码，所以二级菜单可以直接在主题的配置文件<code>_config.yml</code>中配置，而不需要自己添加代码。如果你是老版本的主题，那么你可以参考上述两位大佬的博客进行添加代码。</p><pre class="line-numbers language-bash"><code class="language-bash">  <span class="token comment" spellcheck="true"># 二级菜单写法如下</span>  Medias:    icon: fas fa-list    children:      - name: Books        url: /books        icon: fas fa-book      - name: Musics        url: /musics        icon: fas fa-music      - name: Movies        url: /movies        icon: fas fa-film      - name: Galleries        url: /galleries        icon: fas fa-image<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这样我们就可以在导航栏中看见媒体的图标以及二级图标了，不过由于我们没有创建对应的页面，所以无法看见内容。这里只举例说一下<code>musics</code>页面的创建。</p><p>先使用命令创建<code>musics</code>对应的<code>md</code>文件</p><pre class="line-numbers language-bash"><code class="language-bash">hexo new page <span class="token string">"musics"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这样就在<code>source</code>目录下生成一个<code>musics</code>目录了，里面包含一个<code>index.md</code>就是<code>musics</code>页面的配置文件了。我们填入对应<code>layout</code>的<code>ejs</code>文件</p><pre class="line-numbers language-bash"><code class="language-bash">---title: musicsdate: 2019-11-14 23:41:25type: <span class="token string">"musics"</span>layout: <span class="token string">"musics"</span>---<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>然后在主题的<code>layout</code>目录下创建<code>ejs</code>文件，并写入如下内容：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>%- partial<span class="token punctuation">(</span><span class="token string">'_partial/bg-cover'</span><span class="token punctuation">)</span> %<span class="token operator">></span><span class="token operator">&lt;</span>main class<span class="token operator">=</span><span class="token string">"content"</span><span class="token operator">></span>    <span class="token operator">&lt;</span>div id<span class="token operator">=</span><span class="token string">"contact"</span> class<span class="token operator">=</span><span class="token string">"container chip-container"</span><span class="token operator">></span>                <span class="token operator">&lt;</span>div class<span class="token operator">=</span><span class="token string">"card"</span><span class="token operator">></span>                    <span class="token operator">&lt;</span>div class<span class="token operator">=</span><span class="token string">"card-content"</span> style<span class="token operator">=</span><span class="token string">"text-align: center"</span><span class="token operator">></span>                            <span class="token operator">&lt;</span>h3 style<span class="token operator">=</span><span class="token string">"margin: 5px 0 5px 5px;"</span><span class="token operator">></span>如果你有好的内容推荐，欢迎在下面留言！<span class="token operator">&lt;</span>/h3<span class="token operator">></span>                        <span class="token operator">&lt;</span>/div<span class="token operator">></span>                <span class="token operator">&lt;</span>/div<span class="token operator">></span>                <span class="token operator">&lt;</span>div class<span class="token operator">=</span><span class="token string">"card"</span><span class="token operator">></span>                    <span class="token operator">&lt;</span>% <span class="token keyword">if</span> <span class="token punctuation">(</span>theme.gitalk <span class="token operator">&amp;&amp;</span> theme.gitalk.enable<span class="token punctuation">)</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; %></span>                        <span class="token operator">&lt;</span>%- partial<span class="token punctuation">(</span><span class="token string">'_partial/gitalk'</span><span class="token punctuation">)</span> %<span class="token operator">></span>                        <span class="token operator">&lt;</span>% <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#125; %></span>                        <span class="token operator">&lt;</span>% <span class="token keyword">if</span> <span class="token punctuation">(</span>theme.gitment.enable<span class="token punctuation">)</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; %></span>                        <span class="token operator">&lt;</span>%- partial<span class="token punctuation">(</span><span class="token string">'_partial/gitment'</span><span class="token punctuation">)</span> %<span class="token operator">></span>                        <span class="token operator">&lt;</span>% <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#125; %></span>                        <span class="token operator">&lt;</span>% <span class="token keyword">if</span> <span class="token punctuation">(</span>theme.disqus.enable<span class="token punctuation">)</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; %></span>                        <span class="token operator">&lt;</span>%- partial<span class="token punctuation">(</span><span class="token string">'_partial/disqus'</span><span class="token punctuation">)</span> %<span class="token operator">></span>                        <span class="token operator">&lt;</span>% <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#125; %></span>                        <span class="token operator">&lt;</span>% <span class="token keyword">if</span> <span class="token punctuation">(</span>theme.livere <span class="token operator">&amp;&amp;</span> theme.livere.enable<span class="token punctuation">)</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; %></span>                        <span class="token operator">&lt;</span>%- partial<span class="token punctuation">(</span><span class="token string">'_partial/livere'</span><span class="token punctuation">)</span> %<span class="token operator">></span>                        <span class="token operator">&lt;</span>% <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#125; %></span>                        <span class="token operator">&lt;</span>% <span class="token keyword">if</span> <span class="token punctuation">(</span>theme.valine <span class="token operator">&amp;&amp;</span> theme.valine.enable<span class="token punctuation">)</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; %></span>                        <span class="token operator">&lt;</span>%- partial<span class="token punctuation">(</span><span class="token string">'_partial/valine'</span><span class="token punctuation">)</span> %<span class="token operator">></span>                        <span class="token operator">&lt;</span>% <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#125; %></span>                <span class="token operator">&lt;</span>/div<span class="token operator">></span>        <span class="token operator">&lt;</span>/div<span class="token operator">></span><span class="token operator">&lt;</span>/main<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这样一个页面就创建好了，具体内容可自行修改，其他页面也是如此创建的。</p><blockquote><p>这里有一个bug，就是二级菜单不显示中文，解决方法请见Debug部分。</p></blockquote><h3 id="5-图片添加水印"><a href="#5-图片添加水印" class="headerlink" title="5.图片添加水印"></a>5.图片添加水印</h3><p>为了防止别人抄袭你文章，可以把所有的图片都加上水印，方法很简单。首先在博客根目录下新建一个<code>watermark.py</code>，代码如下：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># -*- coding: utf-8 -*-</span><span class="token keyword">import</span> sys<span class="token keyword">import</span> glob<span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image<span class="token keyword">from</span> PIL <span class="token keyword">import</span> ImageDraw<span class="token keyword">from</span> PIL <span class="token keyword">import</span> ImageFont<span class="token keyword">def</span> <span class="token function">watermark</span><span class="token punctuation">(</span>post_name<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> post_name <span class="token operator">==</span> <span class="token string">'all'</span><span class="token punctuation">:</span>        post_name <span class="token operator">=</span> <span class="token string">'*'</span>    dir_name <span class="token operator">=</span> <span class="token string">'source/_posts/'</span> <span class="token operator">+</span> post_name <span class="token operator">+</span> <span class="token string">'/*'</span>    <span class="token keyword">for</span> files <span class="token keyword">in</span> glob<span class="token punctuation">.</span>glob<span class="token punctuation">(</span>dir_name<span class="token punctuation">)</span><span class="token punctuation">:</span>        im <span class="token operator">=</span> Image<span class="token punctuation">.</span>open<span class="token punctuation">(</span>files<span class="token punctuation">)</span>        <span class="token keyword">if</span> len<span class="token punctuation">(</span>im<span class="token punctuation">.</span>getbands<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">3</span><span class="token punctuation">:</span>            im <span class="token operator">=</span> im<span class="token punctuation">.</span>convert<span class="token punctuation">(</span><span class="token string">'RGB'</span><span class="token punctuation">)</span>            <span class="token keyword">print</span><span class="token punctuation">(</span>files<span class="token punctuation">)</span>        font <span class="token operator">=</span> ImageFont<span class="token punctuation">.</span>truetype<span class="token punctuation">(</span><span class="token string">'STSONG.TTF'</span><span class="token punctuation">,</span> max<span class="token punctuation">(</span><span class="token number">30</span><span class="token punctuation">,</span> int<span class="token punctuation">(</span>im<span class="token punctuation">.</span>size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">/</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        draw <span class="token operator">=</span> ImageDraw<span class="token punctuation">.</span>Draw<span class="token punctuation">(</span>im<span class="token punctuation">)</span>        draw<span class="token punctuation">.</span>text<span class="token punctuation">(</span><span class="token punctuation">(</span>im<span class="token punctuation">.</span>size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">,</span> im<span class="token punctuation">.</span>size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                  u<span class="token string">'@hiyoung'</span><span class="token punctuation">,</span> fill<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> font<span class="token operator">=</span>font<span class="token punctuation">)</span>        im<span class="token punctuation">.</span>save<span class="token punctuation">(</span>files<span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> len<span class="token punctuation">(</span>sys<span class="token punctuation">.</span>argv<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span>        watermark<span class="token punctuation">(</span>sys<span class="token punctuation">.</span>argv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'[usage] &lt;input>'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>字体也放根目录下，自己找字体。然后每次写完一篇文章可以运行<code>python3 watermark.py postname</code>添加水印，如果第一次运行要给所有文章添加水印，可以运行<code>python3 watermark.py all</code>。</p><blockquote><p>这个代码的逻辑就是从文章目录下拿到图片，添加水印。这个前提是要文章的图片放在source/_posts/下，所以如果在文章中直接引用了其他地方的图片链接，那么这个脚本不会去给那个图片加水印了。</p></blockquote><h3 id="6-动态标签栏"><a href="#6-动态标签栏" class="headerlink" title="6.动态标签栏"></a>6.动态标签栏</h3><p>这个功能我没有添加，只是简单的一段代码，在<code>theme/matery/layout/layout.ejs</code>下添加如下代码：</p><pre class="line-numbers language-javascript"><code class="language-javascript"><span class="token operator">&lt;</span>script type<span class="token operator">=</span><span class="token string">"text/javascript"</span><span class="token operator">></span> <span class="token keyword">var</span> OriginTitile <span class="token operator">=</span> document<span class="token punctuation">.</span>title<span class="token punctuation">,</span> st<span class="token punctuation">;</span> document<span class="token punctuation">.</span><span class="token function">addEventListener</span><span class="token punctuation">(</span><span class="token string">"visibilitychange"</span><span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span> document<span class="token punctuation">.</span>hidden <span class="token operator">?</span> <span class="token punctuation">(</span>document<span class="token punctuation">.</span>title <span class="token operator">=</span> <span class="token string">"Σ(っ °Д °;)っ喔哟，崩溃啦！"</span><span class="token punctuation">,</span> <span class="token function">clearTimeout</span><span class="token punctuation">(</span>st<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">:</span> <span class="token punctuation">(</span>document<span class="token punctuation">.</span>title <span class="token operator">=</span> <span class="token string">"φ(゜▽゜*)♪咦，又好了！"</span><span class="token punctuation">,</span> st <span class="token operator">=</span> <span class="token function">setTimeout</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span> document<span class="token punctuation">.</span>title <span class="token operator">=</span> OriginTitile <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span> <span class="token number">3e3</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span><span class="token operator">/</span>script<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="7-添加豆瓣插件"><a href="#7-添加豆瓣插件" class="headerlink" title="7.添加豆瓣插件"></a>7.添加豆瓣插件</h3><p>我的二级菜单书单和电影都是通过豆瓣插件来添加内容的。</p><p>首先安装插件：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> hexo-douban --save<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>将下面的配置写入博客的 <code>_config.yml</code> 文件里：</p><pre class="line-numbers language-bash"><code class="language-bash">douban:  user: <span class="token comment" spellcheck="true">#填写你的豆瓣id，打开豆瓣，登入账户，然后在右上角点击 ”个人主页“，url的后面就是id。</span>  builtin: <span class="token boolean">true</span>  book:    title: <span class="token string">'我的无味书屋！'</span>    quote: <span class="token string">'沉醉于知识的hiyoung.'</span>  movie:    title: <span class="token string">'电影推荐'</span>    quote: <span class="token string">'沉醉于电影的hiyoung.'</span>  <span class="token comment" spellcheck="true">#game:　不想要的内容可注释掉</span>  <span class="token comment" spellcheck="true">#  title: 'This is my game title'</span>  <span class="token comment" spellcheck="true">#  quote: 'This is my game quote'</span>  timeout: 10000<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>然后创建对应的页面，在页面的<code>ejs</code>文件中添加如下代码：</p><pre class="line-numbers language-javascript"><code class="language-javascript"><span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> <span class="token function">partial</span><span class="token punctuation">(</span><span class="token string">'_partial/bg-cover'</span><span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">></span><span class="token operator">&lt;</span>style<span class="token operator">></span>    <span class="token punctuation">.</span>hexo<span class="token operator">-</span>douban<span class="token operator">-</span>picture img <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>        width<span class="token punctuation">:</span> <span class="token number">100</span><span class="token operator">%</span><span class="token punctuation">;</span>        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span class="token operator">&lt;</span><span class="token operator">/</span>style<span class="token operator">></span><span class="token operator">&lt;</span>main <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"content"</span><span class="token operator">></span>    <span class="token operator">&lt;</span>div id<span class="token operator">=</span><span class="token string">"contact"</span> <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"container chip-container"</span><span class="token operator">></span>            <span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"card"</span><span class="token operator">></span>                    <span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"card-content"</span> style<span class="token operator">=</span><span class="token string">"padding: 30px"</span><span class="token operator">></span>                            <span class="token operator">&lt;</span>h1 style<span class="token operator">=</span><span class="token string">"margin: 10px 0 10px 0px;"</span><span class="token operator">></span>                                        <span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"tag-title center-align"</span><span class="token operator">></span>                                        <span class="token operator">&lt;</span>i <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"fas fa-book"</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>i<span class="token operator">></span><span class="token operator">&amp;</span>nbsp<span class="token punctuation">;</span><span class="token operator">&amp;</span>nbsp<span class="token punctuation">;</span><span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> page<span class="token punctuation">.</span>title <span class="token operator">%</span><span class="token operator">></span>                    <span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span>                                <span class="token operator">&lt;</span><span class="token operator">/</span>h1<span class="token operator">></span>                                <span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> page<span class="token punctuation">.</span>content <span class="token operator">%</span><span class="token operator">></span>                        <span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span>                <span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span>                <span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"card"</span><span class="token operator">></span>                    <span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"card-content"</span> style<span class="token operator">=</span><span class="token string">"text-align: center"</span><span class="token operator">></span>                            <span class="token operator">&lt;</span>h3 style<span class="token operator">=</span><span class="token string">"margin: 5px 0 5px 5px;"</span><span class="token operator">></span>如果你有好的内容推荐，欢迎在下面留言！<span class="token operator">&lt;</span><span class="token operator">/</span>h3<span class="token operator">></span>                        <span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span>                <span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span>                <span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"card"</span><span class="token operator">></span>                    <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>theme<span class="token punctuation">.</span>gitalk <span class="token operator">&amp;&amp;</span> theme<span class="token punctuation">.</span>gitalk<span class="token punctuation">.</span>enable<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token operator">%</span><span class="token operator">></span>                        <span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> <span class="token function">partial</span><span class="token punctuation">(</span><span class="token string">'_partial/gitalk'</span><span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">></span>                        <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token operator">%</span><span class="token operator">></span>                        <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>theme<span class="token punctuation">.</span>gitment<span class="token punctuation">.</span>enable<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token operator">%</span><span class="token operator">></span>                        <span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> <span class="token function">partial</span><span class="token punctuation">(</span><span class="token string">'_partial/gitment'</span><span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">></span>                        <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token operator">%</span><span class="token operator">></span>                        <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>theme<span class="token punctuation">.</span>disqus<span class="token punctuation">.</span>enable<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token operator">%</span><span class="token operator">></span>                        <span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> <span class="token function">partial</span><span class="token punctuation">(</span><span class="token string">'_partial/disqus'</span><span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">></span>                        <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token operator">%</span><span class="token operator">></span>                        <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>theme<span class="token punctuation">.</span>livere <span class="token operator">&amp;&amp;</span> theme<span class="token punctuation">.</span>livere<span class="token punctuation">.</span>enable<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token operator">%</span><span class="token operator">></span>                        <span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> <span class="token function">partial</span><span class="token punctuation">(</span><span class="token string">'_partial/livere'</span><span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">></span>                        <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token operator">%</span><span class="token operator">></span>                        <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>theme<span class="token punctuation">.</span>valine <span class="token operator">&amp;&amp;</span> theme<span class="token punctuation">.</span>valine<span class="token punctuation">.</span>enable<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token operator">%</span><span class="token operator">></span>                        <span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> <span class="token function">partial</span><span class="token punctuation">(</span><span class="token string">'_partial/valine'</span><span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">></span>                        <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token operator">%</span><span class="token operator">></span>                <span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span>        <span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>main<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在你的博客文件夹内找到这个文件夹 <code>/node_modules/hexo-douban/lib</code> ，这个文件夹内找到以下三个文件： <code>books-generator.js</code> 、<code>games-generator.js</code> 、<code>movies-generator.js</code></p><p>将每个文件内最下面的：</p><pre class="line-numbers language-javascript"><code class="language-javascript">layout<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'page'</span><span class="token punctuation">,</span> <span class="token string">'post'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>改为：</p><pre class="line-numbers language-javascript"><code class="language-javascript">layout<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'page'</span><span class="token punctuation">,</span> <span class="token string">'books'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><code>books</code>是对应的呈现内容的<code>ejs</code>文件名。这样我们的豆瓣内容就设置完成了，可以进行编译部署了。</p><blockquote><p>通常大家都喜欢用 <code>hexo d</code> 来作为 <code>hexo deploy</code> 命令的简化，但是当安装了 <code>hexo douban</code> 之后，就不能用 <code>hexo d</code> 了，因为 <code>hexo douban</code> 跟 <code>hexo deploy</code> 的前缀都是 <code>hexo d</code> ，你以后执行的 <code>hexo d</code> 将不再是 Hexo 页面的生成，而是豆瓣页面的生成。</p><p>这里也说一下这个插件的逻辑，该插件通过你设置的豆瓣id，去爬取豆瓣信息。将爬取到的信息返回给对应的layout，然后进行展示。</p></blockquote><h3 id="8-统一友链卡片样式"><a href="#8-统一友链卡片样式" class="headerlink" title="8.统一友链卡片样式"></a>8.统一友链卡片样式</h3><p>我不喜欢原版的友链显示，所以统一了颜色，打开<code>themes/matery/layout/friends.ejs</code>文件，找到如下代码并修改：</p><pre class="line-numbers language-javascript"><code class="language-javascript">                            <span class="token operator">&lt;</span><span class="token operator">!</span><span class="token operator">--</span>修改frends卡片，统一样式 <span class="token operator">--</span><span class="token operator">></span>                            <span class="token operator">&lt;</span><span class="token operator">!</span><span class="token operator">--</span><span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"card frind-card&lt;%- ((i % 10) +1) %>"</span><span class="token operator">></span> 修改前<span class="token operator">--</span><span class="token operator">></span>                            <span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"card frind-card1"</span><span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>它的样式颜色也在该文件中，各位可自行修改。</p><h3 id="9-添加交换友链卡片"><a href="#9-添加交换友链卡片" class="headerlink" title="9.添加交换友链卡片"></a>9.添加交换友链卡片</h3><p>在<code>/source/friends/index.md</code>文件中添加要交互的信息：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># 友链交换</span>想要交换友链的大佬，欢迎在留言板留言，留言格式：* **名称：**Hiyoung* **地址：**https://hiyoungai.com/* **简介：**宠辱不惊，看庭前花开花落；去留无意，望天空云卷云舒。* **头像：**https://cdn.jsdelivr.net/gh/hiyoung123/cdn/img/avatar.jpg<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>然后在<code>friends.ejs</code>文件中的如下位置添加代码：</p><pre class="line-numbers language-javascript"><code class="language-javascript">        <span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"card"</span><span class="token operator">></span>            <span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"card-content"</span><span class="token operator">></span>                <span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"card-content article-card-content"</span><span class="token operator">></span>                    <span class="token operator">&lt;</span>div id<span class="token operator">=</span><span class="token string">"articleContent"</span> data<span class="token operator">-</span>aos<span class="token operator">=</span><span class="token string">"fade-up"</span><span class="token operator">></span>                        <span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> page<span class="token punctuation">.</span>content <span class="token operator">%</span><span class="token operator">></span>                    <span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span>                <span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span>            <span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span>        <span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span>        <span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"card"</span><span class="token operator">></span>            <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>theme<span class="token punctuation">.</span>gitalk <span class="token operator">&amp;&amp;</span> theme<span class="token punctuation">.</span>gitalk<span class="token punctuation">.</span>enable<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token operator">%</span><span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="10-修改各菜单首图样式"><a href="#10-修改各菜单首图样式" class="headerlink" title="10.修改各菜单首图样式"></a>10.修改各菜单首图样式</h3><p>修改各个页面的首图为本页面标题，而不是统一的网站标题。</p><p>打开<code>layout/_partial/bg-cover-content.ejs</code>文件，找到如下代码：</p><pre class="line-numbers language-javascript"><code class="language-javascript">            <span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"title center-align"</span><span class="token operator">></span>                <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>config<span class="token punctuation">.</span>subtitle <span class="token operator">&amp;&amp;</span> config<span class="token punctuation">.</span>subtitle<span class="token punctuation">.</span>length <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token operator">%</span><span class="token operator">></span>                <span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">=</span> config<span class="token punctuation">.</span>subtitle <span class="token operator">%</span><span class="token operator">></span>                <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token keyword">else</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token operator">%</span><span class="token operator">></span>                subtitle                <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token operator">%</span><span class="token operator">></span>            <span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>修改为：</p><pre class="line-numbers language-javascript"><code class="language-javascript">            <span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"title center-align"</span><span class="token operator">></span>                <span class="token operator">&lt;</span><span class="token operator">!</span><span class="token operator">--</span> <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>config<span class="token punctuation">.</span>subtitle <span class="token operator">&amp;&amp;</span> config<span class="token punctuation">.</span>subtitle<span class="token punctuation">.</span>length <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token operator">%</span><span class="token operator">></span>                    <span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">=</span> config<span class="token punctuation">.</span>subtitle <span class="token operator">%</span><span class="token operator">></span>                <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token keyword">else</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token operator">%</span><span class="token operator">></span>                    subtitle                <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token operator">%</span><span class="token operator">></span> <span class="token operator">--</span><span class="token operator">></span>                <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">is_archive</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token operator">%</span><span class="token operator">></span>                    <span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">=</span> <span class="token function">__</span><span class="token punctuation">(</span><span class="token string">'archives'</span><span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">></span>                <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token keyword">else</span> <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">is_category</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token operator">%</span><span class="token operator">></span>                    <span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">=</span> <span class="token function">__</span><span class="token punctuation">(</span><span class="token string">'categories'</span><span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">></span>                <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token keyword">else</span> <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">is_tag</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token operator">%</span><span class="token operator">></span>                    <span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">=</span> <span class="token function">__</span><span class="token punctuation">(</span><span class="token string">'tag'</span><span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">></span>                <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token keyword">else</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>page<span class="token punctuation">.</span>title <span class="token operator">&amp;&amp;</span> page<span class="token punctuation">.</span>title<span class="token punctuation">.</span>length <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token operator">%</span><span class="token operator">></span>                    <span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">=</span> <span class="token function">__</span><span class="token punctuation">(</span>page<span class="token punctuation">.</span>title<span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">></span>                <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token keyword">else</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token operator">%</span><span class="token operator">></span>                    <span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">=</span> config<span class="token punctuation">.</span>subtitle<span class="token operator">%</span><span class="token operator">></span>                <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token operator">%</span><span class="token operator">></span>            <span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="11-在文章中添加网易云音乐"><a href="#11-在文章中添加网易云音乐" class="headerlink" title="11.在文章中添加网易云音乐"></a>11.在文章中添加网易云音乐</h3><p>首先打开网易云网页版，找到想听的歌曲，然后点击生成外链，复制<code>html</code>代码。粘贴到文章里就行了，为了美观，设置一下居中，具体代码如下：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>div align<span class="token operator">=</span><span class="token string">"middle"</span><span class="token operator">></span>这里粘贴刚刚复制的代码<span class="token operator">&lt;</span>/div<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="12-建站时间、卜算子计数、全站文字统计"><a href="#12-建站时间、卜算子计数、全站文字统计" class="headerlink" title="12.建站时间、卜算子计数、全站文字统计"></a>12.建站时间、卜算子计数、全站文字统计</h3><p>新版本中已经集成了该功能，可以直接在主题的配置文件<code>_config.yml</code>中进行配置：</p><p>首先需要安装插件：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> i --save hexo-wordcount<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>然后在<code>_config.yml</code>配置：</p><pre class="line-numbers language-bash"><code class="language-bash">wordCount:  enable: <span class="token boolean">false</span> <span class="token comment" spellcheck="true"># 将这个值设置为 true 即可.</span>  postWordCount: <span class="token boolean">true</span>  min2read: <span class="token boolean">true</span>  totalCount: <span class="token boolean">true</span> <span class="token comment" spellcheck="true">#需要添加这个字段，原版没有　全站文字统计配置</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>建站时间配置：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># Website start time.</span><span class="token comment" spellcheck="true"># 站点运行开始时间.</span>time:  enable: <span class="token boolean">true</span>  year: 2019 <span class="token comment" spellcheck="true"># 年份</span>  month: 11 <span class="token comment" spellcheck="true"># 月份</span>  date: 12 <span class="token comment" spellcheck="true"># 日期</span>  hour: 00 <span class="token comment" spellcheck="true"># 小时</span>  minute: 00 <span class="token comment" spellcheck="true"># 分钟</span>  second: 00 <span class="token comment" spellcheck="true"># 秒</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="13-关于页面添加简历"><a href="#13-关于页面添加简历" class="headerlink" title="13.关于页面添加简历"></a>13.关于页面添加简历</h3><p>修改<code>/themes/matery/layout/about.ejs</code>，找到<code>&lt;div class=&quot;card&quot;&gt;</code>标签，然后找到它对应的<code>&lt;/div&gt;</code>标签，接在后面新增一个card，语句如下：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>div class<span class="token operator">=</span><span class="token string">"card"</span><span class="token operator">></span> <span class="token operator">&lt;</span>div class<span class="token operator">=</span><span class="token string">"card-content"</span><span class="token operator">></span> <span class="token operator">&lt;</span>div class<span class="token operator">=</span><span class="token string">"card-content article-card-content"</span><span class="token operator">></span> <span class="token operator">&lt;</span>div class<span class="token operator">=</span><span class="token string">"title center-align"</span> data-aos<span class="token operator">=</span><span class="token string">"zoom-in-up"</span><span class="token operator">></span> <span class="token operator">&lt;</span>i class<span class="token operator">=</span><span class="token string">"fa fa-address-book"</span><span class="token operator">></span><span class="token operator">&lt;</span>/i<span class="token operator">></span><span class="token operator">&amp;</span>nbsp<span class="token punctuation">;</span><span class="token operator">&amp;</span>nbsp<span class="token punctuation">;</span><span class="token operator">&lt;</span>%- __<span class="token punctuation">(</span><span class="token string">'myCV'</span><span class="token punctuation">)</span> %<span class="token operator">></span> <span class="token operator">&lt;</span>/div<span class="token operator">></span> <span class="token operator">&lt;</span>div id<span class="token operator">=</span><span class="token string">"articleContent"</span> data-aos<span class="token operator">=</span><span class="token string">"fade-up"</span><span class="token operator">></span> <span class="token operator">&lt;</span>%- page.content %<span class="token operator">></span> <span class="token operator">&lt;</span>/div<span class="token operator">></span> <span class="token operator">&lt;</span>/div<span class="token operator">></span> <span class="token operator">&lt;</span>/div<span class="token operator">></span> <span class="token operator">&lt;</span>/div<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这样就会多出一张card，然后可以在<code>/source/about/index.md</code>下面写上你的简历了，当然这里的位置随你自己设置，你也可以把简历作为第一个card。</p><h3 id="14-添加评论插件"><a href="#14-添加评论插件" class="headerlink" title="14.添加评论插件"></a>14.添加评论插件</h3><p>主题已经自带了<code>gitalk</code>插件了，所以你只需要去<code>github</code>官网配置好就行了。</p><p>首先打开<a href="https://github.com/settings/applications/new">github</a>申请一个应用，要填四个东西：</p><pre class="line-numbers language-bash"><code class="language-bash">Application name //应用名称，随便填 Homepage URL //填自己的博客地址 Application description //应用描述，随便填 Authorization callback URL //填自己的博客地址<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>然后点击注册，会出现两个字符串<code>Client ID</code>和<code>Client Secret</code>，这个要复制出来。</p><p>然后去主题的配置文件<code>_config.yml</code>下修改<code>gitalk</code>那里：</p><pre class="line-numbers language-bash"><code class="language-bash">gitalk: enable: <span class="token boolean">true</span>   owner: 你的github用户名    repo: 你的github用户名.github.io   oauth:     clientId: 粘贴刚刚注册完显示的字符串     clientSecret: 粘贴刚刚注册完显示的字符串   admin: 你的github用户名<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>以后写文章的时候，只要在文章页面登陆过<code>github</code>，就会自动创建评论框，<strong>记得每次写完文章后打开博客文章页面一下</strong>。</p><h3 id="15-添加RSS插件"><a href="#15-添加RSS插件" class="headerlink" title="15.添加RSS插件"></a>15.添加RSS插件</h3><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> hexo-generator-feed --save<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在<code>Hexo</code>根目录下的 <code>_config.yml</code> 文件中，新增以下的配置项：</p><pre class="line-numbers language-bash"><code class="language-bash">feed:  type: atom  path: atom.xml  limit: 20  hub:  content:  content_limit: 140  content_limit_delim: <span class="token string">' '</span>  order_by: -date<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>执行 <code>hexo clean &amp;&amp; hexo g</code> 重新生成博客文件，然后在 <code>public</code> 文件夹中即可看到 <code>atom.xml</code> 文件，说明你已经安装成功了。</p><h3 id="16-添加搜索插件"><a href="#16-添加搜索插件" class="headerlink" title="16.添加搜索插件"></a>16.添加搜索插件</h3><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> hexo-generator-search --save<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在根目录下的 <code>_config.yml</code> 文件中，新增以下的配置项：</p><pre class="line-numbers language-bash"><code class="language-bash">search:  path: search.xml  field: post<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="17-添加代码高亮插件"><a href="#17-添加代码高亮插件" class="headerlink" title="17.添加代码高亮插件"></a>17.添加代码高亮插件</h3><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> i -S hexo-prism-plugin<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>然后，修改根目录下 <code>_config.yml</code> 文件中 <code>highlight.enable</code> 的值为 <code>false</code>，并新增 <code>prism</code> 插件相关的配置，主要配置如下：</p><pre class="line-numbers language-bash"><code class="language-bash">highlight:  enable: <span class="token boolean">false</span>prism_plugin:  mode: <span class="token string">'preprocess'</span>    <span class="token comment" spellcheck="true"># realtime/preprocess</span>  theme: <span class="token string">'tomorrow'</span>  line_number: <span class="token boolean">false</span>    <span class="token comment" spellcheck="true"># default false</span>  custom_css:<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="18-修改打赏功能"><a href="#18-修改打赏功能" class="headerlink" title="18.修改打赏功能"></a>18.修改打赏功能</h3><p>在主题文件的 <code>source/medias/reward</code> 文件中，你可以替换成你的的微信和支付宝的打赏二维码图片。</p><h3 id="19-修改页脚"><a href="#19-修改页脚" class="headerlink" title="19.修改页脚"></a>19.修改页脚</h3><p>页脚信息可能需要做定制化修改，而且它不便于做成配置信息，所以可能需要你自己去再修改和加工。修改的地方在主题文件的 <code>/layout/_partial/footer.ejs</code> 文件中，包括站点、使用的主题、访问量等。</p><h3 id="20-修改社交链接"><a href="#20-修改社交链接" class="headerlink" title="20.修改社交链接"></a>20.修改社交链接</h3><p>在主题的 <code>_config.yml</code> 文件中，默认支持 <code>QQ</code>、<code>GitHub</code> 和邮箱等的配置，你可以在主题文件的 <code>/layout/_partial/social-link.ejs</code> 文件中，新增、修改你需要的社交链接地址，增加链接可参考如下代码：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token operator">&lt;</span>% <span class="token keyword">if</span> <span class="token punctuation">(</span>theme.socialLink.github<span class="token punctuation">)</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; %></span>    <span class="token operator">&lt;</span>a href<span class="token operator">=</span><span class="token string">"&lt;%= theme.socialLink.github %>"</span> class<span class="token operator">=</span><span class="token string">"tooltipped"</span> target<span class="token operator">=</span><span class="token string">"_blank"</span> data-tooltip<span class="token operator">=</span><span class="token string">"访问我的GitHub"</span> data-position<span class="token operator">=</span><span class="token string">"top"</span> data-delay<span class="token operator">=</span><span class="token string">"50"</span><span class="token operator">></span>        <span class="token operator">&lt;</span>i class<span class="token operator">=</span><span class="token string">"fab fa-github"</span><span class="token operator">></span><span class="token operator">&lt;</span>/i<span class="token operator">></span>    <span class="token operator">&lt;</span>/a<span class="token operator">></span><span class="token operator">&lt;</span>% <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#125; %></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中，社交图标（如：<code>fa-github</code>）你可以在 <a href="https://fontawesome.com/icons">Font Awesome</a> 中搜索找到。以下是常用社交图标的标识，供你参考：</p><ul><li>Facebook: <code>fab fa-facebook</code></li><li>Twitter: <code>fab fa-twitter</code></li><li>Google-plus: <code>fab fa-google-plus</code></li><li>Linkedin: <code>fab fa-linkedin</code></li><li>Tumblr: <code>fab fa-tumblr</code></li><li>Medium: <code>fab fa-medium</code></li><li>Slack: <code>fab fa-slack</code></li><li>Sina Weibo: <code>fab fa-weibo</code></li><li>Wechat: <code>fab fa-weixin</code></li><li>QQ: <code>fab fa-qq</code></li><li>Zhihu: <code>fab fa-zhihu</code></li></ul><h3 id="21-添加聊天功能"><a href="#21-添加聊天功能" class="headerlink" title="21.添加聊天功能"></a>21.添加聊天功能</h3><p>前往 <a href="http://www.daovoice.io/">DaoVoice</a> 官网注册并且获取 <code>app_id</code>，并将 <code>app_id</code> 填入主题的 <code>_config.yml</code> 文件中。</p><p>前往 <a href="https://www.tidio.com/">Tidio</a> 官网注册并且获取 <code>Public Key</code>，并将 <code>Public Key</code> 填入主题的 <code>_config.yml</code> 文件中。</p><h2 id="第四部分：优化"><a href="#第四部分：优化" class="headerlink" title="第四部分：优化"></a>第四部分：优化</h2><h3 id="1-URL优化"><a href="#1-URL优化" class="headerlink" title="1.URL优化"></a>1.URL优化</h3><p>使用插件优化<code>url</code>，插件<code>hexo-abbrlink</code>实现了这个功能，它将原来的<code>URL</code>地址重新进行了进制转换和再编码。</p><p>安装<code>hexo-abbrlink</code>：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> hexo-abbrlink --save<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>配置博客根目录下的<code>_config.yml</code>文件。</p><h3 id="2-CDN优化"><a href="#2-CDN优化" class="headerlink" title="2.CDN优化"></a>2.CDN优化</h3><p>用法：</p><pre class="line-numbers language-bash"><code class="language-bash">https://cdn.jsdelivr.net/gh/你的用户名/你的仓库名@发布的版本号/文件路径<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>如本文的图片：</p><pre class="line-numbers language-bash"><code class="language-bash">https://cdn.jsdelivr.net/gh/hiyoung123/CDN/img/img_hexo_github_new_rep.png<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="3-压缩代码"><a href="#3-压缩代码" class="headerlink" title="3.压缩代码"></a>3.压缩代码</h3><p>首先安装插件：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> hexo-neat --save<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在根目录配置文件 <code>_config.yml</code> 末尾加入以下配置：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true">#hexo-neat 优化提速插件（去掉HTML、css、js的blank字符）</span>neat_enable: <span class="token boolean">true</span>neat_html:  enable: <span class="token boolean">true</span>  exclude:    - <span class="token string">'**/*.md'</span>neat_css:  enable: <span class="token boolean">true</span>  exclude:    - <span class="token string">'**/*.min.css'</span>neat_js:  enable: <span class="token boolean">true</span>  mangle: <span class="token boolean">true</span>  output:  compress:  exclude:    - <span class="token string">'**/*.min.js'</span>    - <span class="token string">'**/**/instantpage.js'</span>    - <span class="token string">'**/matery.js'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-双部署到Coding"><a href="#4-双部署到Coding" class="headerlink" title="4.双部署到Coding"></a>4.双部署到Coding</h3><p><code>Github</code> &amp; <code>Coding Pages</code> 双部署,对国内,国外用户进行分流访问,以提升网站的访问速度.<br><code>Github Pages</code> 的部署前面已经说了,这里就讲一讲 <code>Coding Pages</code> 如何部署.其实与 <code>Github Pages</code> 也类似,先到<code>coding</code>官网注册,创建一个与用户名同名的仓库,添加仓库地址到配置文件中,在根目录<code>_config.yml</code>对应地方添加如下:</p><pre class="line-numbers language-bash"><code class="language-bash">deploy:  - type: <span class="token function">git</span>    repo:      github: https://github.com/hiyoung123/hiyoung123.github.io      coding: https://e.coding.net/hiyoung123/hiyoung123.coding.me.git    branch: master<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="5-图片懒加载"><a href="#5-图片懒加载" class="headerlink" title="5.图片懒加载"></a>5.图片懒加载</h3><p>安装插件：</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> hexo-lazyload-image --save<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>然后在根目录配置文件末尾加入以下代码</p><pre class="line-numbers language-bash"><code class="language-bash">lazyload:  enable: <span class="token boolean">true</span>   onlypost: <span class="token boolean">false</span>  <span class="token comment" spellcheck="true"># 是否只对文章的图片做懒加载</span>  loadingImg: <span class="token comment" spellcheck="true"># eg ./images/loading.gif</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>但是一般情况下懒加载和gallery插件会发生冲突，结果可能就是点开图片，左翻右翻都是loading image。<code>matery</code>主题的解决方案是：修改 <code>/themes/matery/source/js</code> 中的 <code>matery.js</code>文件</p><p>在第108行加上：</p><pre class="line-numbers language-javascript"><code class="language-javascript"><span class="token function">$</span><span class="token punctuation">(</span>document<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">find</span><span class="token punctuation">(</span><span class="token string">'img[data-original]'</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">each</span><span class="token punctuation">(</span><span class="token keyword">function</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>    <span class="token function">$</span><span class="token punctuation">(</span><span class="token keyword">this</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">parent</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">attr</span><span class="token punctuation">(</span><span class="token string">"href"</span><span class="token punctuation">,</span> <span class="token function">$</span><span class="token punctuation">(</span><span class="token keyword">this</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">attr</span><span class="token punctuation">(</span><span class="token string">"data-original"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>做完这步之后，还有点小Bug，首页的logo点击会直接打开logo图，而不是跳到首页。</p><p>伪解决方案：打开 <code>/themes/matery/layout/_partial/header.ejs</code>文件，</p><p>在<code>img</code>和<code>span</code>的两个头加个<code>div</code>：</p><pre class="line-numbers language-javascript"><code class="language-javascript"><span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"brand-logo"</span><span class="token operator">></span>    <span class="token operator">&lt;</span>a href<span class="token operator">=</span><span class="token string">"&lt;%- url_for() %>"</span> <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"waves-effect waves-light"</span><span class="token operator">></span>        <span class="token operator">&lt;</span>div<span class="token operator">></span>            <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>theme<span class="token punctuation">.</span>logo <span class="token operator">!==</span> undefined <span class="token operator">&amp;&amp;</span> theme<span class="token punctuation">.</span>logo<span class="token punctuation">.</span>length <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token operator">%</span><span class="token operator">></span>            <span class="token operator">&lt;</span>img src<span class="token operator">=</span><span class="token string">"&lt;%= theme.logo %>"</span> <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"logo-img"</span> alt<span class="token operator">=</span><span class="token string">"LOGO"</span><span class="token operator">></span>            <span class="token operator">&lt;</span><span class="token operator">%</span> <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token operator">%</span><span class="token operator">></span>            <span class="token operator">&lt;</span>span <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"logo-span"</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> config<span class="token punctuation">.</span>title <span class="token operator">%</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>span<span class="token operator">></span>        <span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span>    <span class="token operator">&lt;</span><span class="token operator">/</span>a<span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其实第一次加载后本地都是有缓存的，如果每次都把loading显示出来就不那么好看。所以我们需要对插件进行魔改，让图片稍微提前加载，避开加载动画。</p><p>打开 <code>Hexo根目录</code>&gt;<code>node_modules</code> &gt; <code>hexo-lazyload-image</code> &gt; <code>lib</code> &gt; <code>simple-lazyload.js</code> 文件第9行修改为：</p><pre class="line-numbers language-javascript"><code class="language-javascript"><span class="token operator">&amp;&amp;</span> rect<span class="token punctuation">.</span>top <span class="token operator">&lt;=</span> <span class="token punctuation">(</span>window<span class="token punctuation">.</span>innerHeight <span class="token operator">+</span><span class="token number">240</span> <span class="token operator">||</span> document<span class="token punctuation">.</span>documentElement<span class="token punctuation">.</span>clientHeight <span class="token operator">+</span><span class="token number">240</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>作用：提前240个像素加载图片；当然这个值也可以根据自己情况修改。</p><h2 id="第五部分：Debug"><a href="#第五部分：Debug" class="headerlink" title="第五部分：Debug"></a>第五部分：Debug</h2><h3 id="1-解决部分菜单页面，标签栏不显示中文标题"><a href="#1-解决部分菜单页面，标签栏不显示中文标题" class="headerlink" title="1.解决部分菜单页面，标签栏不显示中文标题"></a>1.解决部分菜单页面，标签栏不显示中文标题</h3><p>首先需要去<code>/themes/matery/languages/</code>下，修改<code>default.yml</code>和<code>zh-CN.yml</code>添加对应的文字信息。</p><p>接着在<code>mobile-nav.ejs</code>和<code>navigation.ejs</code>中添加如下代码：</p><pre class="line-numbers language-javascript"><code class="language-javascript">        menuMap<span class="token punctuation">.</span><span class="token keyword">set</span><span class="token punctuation">(</span><span class="token string">"Medias"</span><span class="token punctuation">,</span> <span class="token string">"媒体"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        menuMap<span class="token punctuation">.</span><span class="token keyword">set</span><span class="token punctuation">(</span><span class="token string">"Books"</span><span class="token punctuation">,</span> <span class="token string">"书单"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        menuMap<span class="token punctuation">.</span><span class="token keyword">set</span><span class="token punctuation">(</span><span class="token string">"Musics"</span><span class="token punctuation">,</span> <span class="token string">"音乐"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        menuMap<span class="token punctuation">.</span><span class="token keyword">set</span><span class="token punctuation">(</span><span class="token string">"Videos"</span><span class="token punctuation">,</span> <span class="token string">"视频"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        menuMap<span class="token punctuation">.</span><span class="token keyword">set</span><span class="token punctuation">(</span><span class="token string">"Galleries"</span><span class="token punctuation">,</span> <span class="token string">"相册"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>找到下面的代码：</p><pre class="line-numbers language-javascript"><code class="language-javascript"><span class="token operator">&lt;</span>span<span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> childrenLink<span class="token punctuation">.</span>name <span class="token operator">%</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>span<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>修改为：</p><pre class="line-numbers language-javascript"><code class="language-javascript"><span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> <span class="token punctuation">(</span>config<span class="token punctuation">.</span>language <span class="token operator">===</span> <span class="token string">'zh-CN'</span> <span class="token operator">&amp;&amp;</span> menuMap<span class="token punctuation">.</span><span class="token function">has</span><span class="token punctuation">(</span>childrenLink<span class="token punctuation">.</span>name<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">?</span> menuMap<span class="token punctuation">.</span><span class="token keyword">get</span><span class="token punctuation">(</span>childrenLink<span class="token punctuation">.</span>name<span class="token punctuation">)</span> <span class="token punctuation">:</span> childrenLink<span class="token punctuation">.</span>name <span class="token operator">%</span><span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>并在<code>head.ejs</code>文件中修改：</p><pre class="line-numbers language-javascript"><code class="language-javascript">    <span class="token keyword">var</span> title <span class="token operator">=</span> page<span class="token punctuation">.</span>title<span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// tags, categories, about pages title</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>title <span class="token operator">==</span> <span class="token string">'tags'</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>        title <span class="token operator">=</span> <span class="token function">__</span><span class="token punctuation">(</span><span class="token string">'tags'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token keyword">else</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>title <span class="token operator">==</span> <span class="token string">'categories'</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>        title <span class="token operator">=</span> <span class="token function">__</span><span class="token punctuation">(</span><span class="token string">'categories'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token keyword">else</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>title <span class="token operator">==</span> <span class="token string">'about'</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>        title <span class="token operator">=</span> <span class="token function">__</span><span class="token punctuation">(</span><span class="token string">'about'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token keyword">else</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>title <span class="token operator">==</span> <span class="token string">'contact'</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>        title <span class="token operator">=</span> <span class="token function">__</span><span class="token punctuation">(</span><span class="token string">'contact'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token keyword">else</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>title <span class="token operator">==</span> <span class="token string">'friends'</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>        title <span class="token operator">=</span> <span class="token function">__</span><span class="token punctuation">(</span><span class="token string">'friends'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token keyword">else</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>title <span class="token operator">==</span> <span class="token string">'musics'</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>        title <span class="token operator">=</span> <span class="token function">__</span><span class="token punctuation">(</span><span class="token string">'musics'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token keyword">else</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>title <span class="token operator">==</span> <span class="token string">'galleries'</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>        title <span class="token operator">=</span> <span class="token function">__</span><span class="token punctuation">(</span><span class="token string">'galleries'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-部署在coding中，使用www-访问域名时，出现404．"><a href="#2-部署在coding中，使用www-访问域名时，出现404．" class="headerlink" title="2.部署在coding中，使用www.访问域名时，出现404．"></a>2.部署在coding中，使用www.访问域名时，出现404．</h3><p>需要在coding部署设置中，绑定一下<code>www</code>的域名，同时需要申请证书。</p><h3 id="3-在coding中认证失败"><a href="#3-在coding中认证失败" class="headerlink" title="3.在coding中认证失败"></a>3.在coding中认证失败</h3><p>如果申请失败的话，在域名解析处将境外的解析记录关掉，然后再去申请。申请成功后再打开境外的记录。</p><h3 id="4-使用neat插件压缩代码，导致鼠标点击特效消失"><a href="#4-使用neat插件压缩代码，导致鼠标点击特效消失" class="headerlink" title="4.使用neat插件压缩代码，导致鼠标点击特效消失"></a>4.使用neat插件压缩代码，导致鼠标点击特效消失</h3><p>在压缩代码插件配置中修改为如下代码：</p><pre class="line-numbers language-bash"><code class="language-bash">neat_js:  enable: <span class="token boolean">true</span>  mangle: <span class="token boolean">true</span>  output:  compress:  exclude:    - <span class="token string">'**/*.min.js'</span>    - <span class="token string">'**/**/instantpage.js'</span>    - <span class="token string">'**/matery.js'</span>    - <span class="token string">'**/clicklove.js'</span>  <span class="token comment" spellcheck="true">#防止影响点击特效</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>5.卜算子区分www和不带www的域名，导致访问数无法同步。</p><p>卜算子按照域名进行统计，带www和不带www的属于两个域名。可能需要重定向解决。</p><p>使用cloudflare貌似可以做到域名重定向。</p><h2 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h2><h3 id="hexo-升级"><a href="#hexo-升级" class="headerlink" title="hexo 升级"></a>hexo 升级</h3><ol><li><p>打开 package.json，修改 dependencies 项：</p><pre class="line-numbers language-xml"><code class="language-xml">  "hexo": "^5.0.0",<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>执行更新</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> update<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ol><h3 id="更换butterfly主题"><a href="#更换butterfly主题" class="headerlink" title="更换butterfly主题"></a>更换butterfly主题</h3><ol><li><p>clone 项目到 themes 目录</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">git</span> clone -b master https://github.com/jerryc127/hexo-theme-butterfly.git themes/hexo-theme-butterfly<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>安装对应插件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> hexo-renderer-pug hexo-renderer-stylus<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>修改_config.yml文件</p><pre><code> theme: hexo-theme-butterfly</code></pre></li><li><p>执行构建等命令</p><pre class="line-numbers language-bash"><code class="language-bash">hexo clean <span class="token operator">&amp;</span> hexo g <span class="token operator">&amp;</span> hexo s<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ol><h3 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h3><ol><li><p>prettyUrls is not a function</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">npm</span> cache clean --forcedelete node_modules folderdelete package-lock.json <span class="token function">file</span><span class="token function">npm</span> <span class="token function">install</span>hexo clean<span class="token punctuation">;</span> hexo g<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="鸣谢"><a href="#鸣谢" class="headerlink" title="鸣谢"></a>鸣谢</h2><p>再次感谢下面几位大佬的博客：</p><p><a href="https://github.com/blinkfox/hexo-theme-matery">闪烁之狐的原版定制</a> | <a href="https://godweiyang.com/2018/04/13/hexo-blog/">Godweiyang</a> | <a href="https://sunhwee.com/posts/6e8839eb.html">洪卫</a> | <a href="https://blog.sky03.cn/2019/42790.html">Sky03</a></p>]]></content>
      
      
      <categories>
          
          <category> 网站搭建与优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
